{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e62d23-ca7e-4c4d-8ea7-08cddb8f12ab",
   "metadata": {},
   "source": [
    "======FairFed============\n",
    "\n",
    "-> Init Models for clients\n",
    "\n",
    "-> Get statistics of the dataset\n",
    "\n",
    "-> start training\n",
    "\n",
    "      -> for each round-> assigning agg global model to all clients\n",
    "\n",
    "-> for each client\n",
    "\n",
    "        > Update its weights based on local and global fairness\n",
    "        \n",
    "        > store it\n",
    "-> SecAgg applied on the server side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae9b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import glob\n",
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from folktables import ACSDataSource, ACSEmployment,ACSIncome\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from scipy.stats import multivariate_normal\n",
    "import torch, random, copy, os\n",
    "from collections import OrderedDict\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bea641-8d82-477c-8c94-ce49034ebf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPR\n",
    "# Gender: \n",
    "# 1: Male, 0: Female\n",
    "# Fairness: Male- Female\n",
    "# eq-4: Global- TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23e284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e8550ad",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70374e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    # CUDA is not available\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badffcd-029a-47e7-9d4e-40b52d32b39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4eb7f6d-5757-494b-8d6a-1192058aee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 14 : input shape\n",
    "        # 9-> we have 9 columns in data \n",
    "        self.layer1 = nn.Linear(14, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(256, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e6ddc-37c5-4a04-ba53-7215bb513904",
   "metadata": {},
   "source": [
    "# assigning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b03167-c1a8-48ce-887b-80a9d50f7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssignModel:\n",
    "    def __init__(self, global_model_base_path, selected_clients,algo=\"FF\"):      \n",
    "        self.global_model_path = f\"{global_model_base_path}/global_model_{algo}.pt\"\n",
    "        self.selected_clients = selected_clients\n",
    "\n",
    "    def save_global_model(self, model):\n",
    "        torch.save(model.state_dict(), self.global_model_path)\n",
    "\n",
    "    def save_client_models(self, model):\n",
    "        for client_id in self.selected_clients:\n",
    "            client_model_path = f\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/client_{client_id}_model.pth\"\n",
    "            torch.save(model.state_dict(), client_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52352bb-381e-4af7-baf1-e5900e214c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24c1cb2-bf5f-4164-a4d5-9251d6358acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self):\n",
    "        self.client_id: int = None\n",
    "        self.valset: DataLoader = None\n",
    "        self.trainset: DataLoader = None\n",
    "        self.testset: DataLoader = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")         \n",
    "        self.model = DeepNet().to(self.device)\n",
    "        self.learning_rate=0.001\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "        self.optimizer: torch.optim.Optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.fainess_score=0\n",
    "        self.local_epochs=5\n",
    "        self.client_df = pd.DataFrame()\n",
    "        self.selected_clients=[0,1,2,3]\n",
    "\n",
    "        self.client_fairness=0\n",
    "        self.global_fairness=0\n",
    "\n",
    "        \n",
    "    def get_client_local_dataset(self):\n",
    "                \n",
    "        temp_path_data=\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/data_fairFed\"\n",
    "        \n",
    "        with open(temp_path_data+\"/clients_training.pkl\", \"rb\") as f:\n",
    "            self.trainset = pickle.load(f)\n",
    "            \n",
    "        with open(temp_path_data+\"/clients_validation.pkl\", \"rb\") as f:\n",
    "            self.valset = pickle.load(f)\n",
    "         \n",
    "        with open(temp_path_data+\"/clients_testing_wrong.pkl\", \"rb\") as f:\n",
    "            self.testset  = pickle.load(f)\n",
    "\n",
    "        self.trainset = self.trainset[self.client_id]\n",
    "        self.valset = self.valset[self.client_id]       \n",
    "        self.testset = self.testset[self.client_id]\n",
    "\n",
    "    def get_stats(self):\n",
    "        # 1 male 0 female\n",
    "        total_pr_Y1_A0 = 0\n",
    "        total_pr_Y1_A1 = 0\n",
    "\n",
    "        for client_id in self.selected_clients:\n",
    "            self.client_id=client_id\n",
    "            self.get_client_local_dataset()\n",
    "            for inputs, labels, A in self.trainset:\n",
    "                pr_Y1_A1 = torch.sum((labels == 1) & (A == 1)).item()\n",
    "                pr_Y1_A0 = torch.sum((labels == 1) & (A == 0)).item()\n",
    "          \n",
    "                total_pr_Y1_A0 += pr_Y1_A0\n",
    "                total_pr_Y1_A1 += pr_Y1_A1\n",
    "        temp_sum=total_pr_Y1_A0+total_pr_Y1_A1\n",
    "        return [total_pr_Y1_A1/temp_sum,total_pr_Y1_A0/temp_sum]\n",
    "        \n",
    "    def calculate_fairness(self,y_hat, A, Y,server_acc,client_acc,len_client_data):\n",
    "        # Calculate counts using torch.sum\n",
    "        y_hat, A, Y = y_hat.to(self.device), A.to(self.device), Y.to(self.device)\n",
    "\n",
    "        # 1: Male, 0: Female\n",
    "        predict_Y_male = torch.sum((y_hat[(A == 1) & (Y == 1)] == 1)).item()\n",
    "        predict_Y_female = torch.sum((y_hat[(A == 0) & (Y == 1)] == 1)).item()\n",
    "        \n",
    "        count_Y_male = torch.sum((Y[(A == 1) & (Y == 1)] == 1)).item()\n",
    "        count_Y_female = torch.sum((Y[(A == 0) & (Y == 1)] == 1)).item()\n",
    "    \n",
    "        # Calculate probabilities\n",
    "        \n",
    "        if count_Y_male > 0 and count_Y_female > 0:\n",
    "            prob_Y_male = predict_Y_male / count_Y_male\n",
    "            prob_Y_female = predict_Y_female / count_Y_female\n",
    "            fairness_score = prob_Y_male - prob_Y_female\n",
    "        else:\n",
    "            # Cal sever_acc from client using Eq.\n",
    "            total_len_data=sum(len_client_data)   \n",
    "            temp_server_acc=0\n",
    "            for i in range(len(self.selected_clients)):\n",
    "                temp_server_acc+=client_acc[i]*(len_client_data[i]/total_len_data)\n",
    "                \n",
    "                \n",
    "            avg_acc_client=sum(client_acc) /len(client_acc)\n",
    "            fairness_score= abs(avg_acc_client-server_acc)\n",
    "            # print(\"Else calculate_fairness fairness_score\",fairness_score)\n",
    "            \n",
    "        return  fairness_score\n",
    "\n",
    "    def calculate_ser_fairness(self,y_hat, A, Y,statistics,server_acc,client_acc,len_client_data):\n",
    "        # print(\"calculate_ser_fairness statistics\",statistics)\n",
    "        y_hat, A, Y = y_hat.to(self.device), A.to(self.device), Y.to(self.device)\n",
    "\n",
    "        # for individual clients!\n",
    "        #Male \n",
    "        predict_Y_male = torch.sum((y_hat[(A == 1) & (Y == 1)] == 1)).item() #get total\n",
    "        #Pr(A=0, Y=1)\n",
    "        count_Y_male = torch.sum((A == 1) & (Y == 1)).item() \n",
    "        #Pr(Ŷ=1|A=1, Y=1)  #Female      \n",
    "        predict_Y_female = torch.sum((y_hat[(A == 0) & (Y == 1)] == 1)).item()  #get total\n",
    "        #Pr(A=1, Y=1)\n",
    "        count_Y_female = torch.sum((A == 0) & (Y == 1)).item()\n",
    "        \n",
    "        if count_Y_male>0 and count_Y_female>0:\n",
    "            predict_Y_male=predict_Y_male/count_Y_male #Pr(Ŷ=1|A=0, Y=1)\n",
    "            predict_Y_female=predict_Y_female/count_Y_female   #Pr(Ŷ=1|A=1, Y=1)\n",
    "            temp_fairness_server=((predict_Y_male * count_Y_male)/statistics[0]) - ((predict_Y_female * count_Y_female)/statistics[1])\n",
    "        else:\n",
    "            # Cal sever_acc from client using Eq.\n",
    "            total_len_data=sum(len_client_data)   \n",
    "            temp_server_acc=0\n",
    "            for i in range(len(self.selected_clients)):\n",
    "                temp_server_acc+=client_acc[i]*(len_client_data[i]/total_len_data)\n",
    "                \n",
    "                \n",
    "            avg_acc_client=sum(client_acc) /len(client_acc)\n",
    "            temp_fairness_server= abs(avg_acc_client-server_acc)\n",
    "            # print(\"Else calculate_ser_fairness\",temp_fairness_server)\n",
    "\n",
    "        # print(\"calculate_ser_fairness temp_fairness_server\",self.client_id,temp_fairness_server)\n",
    "        return temp_fairness_server\n",
    "\n",
    "    def train_FA(self, client_id: int, model_path,learning_rate=0.001,num_epochs=5):\n",
    "        self.model.load_state_dict(torch.load(model_path,map_location=self.device))\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"epoch\",epoch)\n",
    "            for x, y,z in self.trainset:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                logits = self.model(x)\n",
    "                loss = self.criterion(logits, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "        models_directory = \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/\"\n",
    "        model_path_1 = os.path.join(models_directory, f\"client_{client_id}_model.pth\")          \n",
    "        torch.save(self.model.state_dict(), model_path_1)\n",
    "        print(f\"client_{client_id} is updated \\n\\n\\n\" )\n",
    "                \n",
    "        return list(self.model.state_dict().values()), len(self.trainset.dataset)\n",
    "       \n",
    "        \n",
    "    def train_FF(self, client_id: int, model_path,statistics,server_acc,clients_acc,len_client_data, num_epochs=5, learning_rate=0.001):\n",
    "        self.client_id = client_id\n",
    "        self.get_client_local_dataset()\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        \n",
    "        self.fainess_score=0\n",
    "      \n",
    "        self.model.load_state_dict(torch.load(model_path,map_location=self.device))\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Set the model to training mode\n",
    "\n",
    "            for inputs, labels, sens in self.trainset:\n",
    "                \n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs).to(self.device)\n",
    "                loss = self.criterion(outputs, labels.float())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Calculate and print the training accuracy for this epoch (optional)\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            size = 0\n",
    "            loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            predicted_labels = []\n",
    "            true_labels = []\n",
    "            final_fairness=[]\n",
    "            final_fairness_server=[]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                print(\"======Validation========\")\n",
    "                for inputs, labels,sens in self.valset:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(inputs)\n",
    "                    loss += self.criterion(outputs, labels)\n",
    "                    predicted = outputs > 0.5\n",
    "\n",
    "                    predicted_labels.extend(predicted.cpu().numpy())\n",
    "                    true_labels.extend(labels.cpu().numpy())                   \n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()                   \n",
    "\n",
    "            loss = loss / len(self.valset)        \n",
    "            acc = correct/total\n",
    " \n",
    "            fairNess_per_batch=self.calculate_fairness(torch.round(outputs).squeeze(), sens.squeeze(), labels.squeeze(),\n",
    "                                                       server_acc,\n",
    "                                                       clients_acc,\n",
    "                                                      len_client_data)\n",
    "            final_fairness.append(fairNess_per_batch)\n",
    "            self.fainess_score=np.mean(final_fairness)\n",
    "\n",
    "            fairNess_per_batch_server=self.calculate_ser_fairness(torch.round(outputs).squeeze(), sens.squeeze(), labels.squeeze(),\n",
    "                                                                  statistics,\n",
    "                                                                  server_acc,\n",
    "                                                                  clients_acc,\n",
    "                                                                 len_client_data)\n",
    "            final_fairness_server.append(fairNess_per_batch_server)            \n",
    "            ser_score=np.mean(final_fairness_server)\n",
    "                        \n",
    "            precision = precision_score(true_labels, predicted_labels,zero_division=0.0)            \n",
    "            recall = recall_score(true_labels, predicted_labels)\n",
    "            \n",
    "            # accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Val Accuracy: {acc:.5f}%\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Val Fairness: {self.fainess_score:.5f}%\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Val Recall: {recall:.5f}%\")\n",
    "\n",
    "\n",
    "        # Optionally, save the trained model parameters\n",
    "        #store model client model\n",
    "        models_directory = \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/\"\n",
    "        model_path_1 = os.path.join(models_directory, f\"client_{client_id}_model.pth\")          \n",
    "        torch.save(self.model.state_dict(), model_path_1)\n",
    "        print(f\"client_{client_id} is updated \\n\\n\\n\" )\n",
    "\n",
    "        \n",
    "        return list(self.model.state_dict().values()), len(self.trainset.dataset),self.model.state_dict(), self.fainess_score, ser_score\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def client_evaluate(self, val=False):\n",
    "        \n",
    "        size = 0\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "        len_of_data=[]\n",
    "        all_client_acc=[]\n",
    "        \n",
    "        models_directory = \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/\"\n",
    "                 \n",
    "        for client_id in self.selected_clients:\n",
    "            \n",
    "            model_path = os.path.join(models_directory, f\"client_{client_id}_model.pth\")            \n",
    "        \n",
    "            self.model.load_state_dict(torch.load(model_path,map_location=self.device))\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            for inputs, targets,sens in self.testset:\n",
    "                \n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                loss += self.criterion(outputs, targets)\n",
    "                predicted = outputs > 0.5\n",
    "                \n",
    "                predicted_labels.extend(predicted.cpu().numpy())\n",
    "                true_labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                len_of_data.append(len(self.testset))\n",
    "            \n",
    "            loss = loss / len(self.testset)\n",
    "            acc = correct / total\n",
    "            all_client_acc.append(acc)\n",
    "                        \n",
    "            precision = precision_score(true_labels, predicted_labels,zero_division=0.0)\n",
    "            \n",
    "            recall = recall_score(true_labels, predicted_labels)\n",
    "                            \n",
    "        return loss, acc, precision, recall,len_of_data, all_client_acc\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def server_evaluate(self,path,all_client_acc,len_of_data):\n",
    "\n",
    "        # print(\"Global Model testing Starts\")\n",
    "        # print(\"kindly check the Path. Select it based on FedAvg Model\")\n",
    "        # print(\"warning: Test data has already been used\")\n",
    "        temp_path_data=\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/\"\n",
    "\n",
    "        with open(temp_path_data+\"/testing_client.pkl\", \"rb\") as f:\n",
    "            testset  = pickle.load(f)\n",
    "            \n",
    "        \n",
    "        # path=\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/global_model_2.pt\"\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(path,map_location=self.device))\n",
    "        self.model.eval()\n",
    "        size = 0\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "        final_fairness=[]\n",
    "\n",
    "        self.testset=testset[1]\n",
    "        for inputs, targets,sens in self.testset:\n",
    "            \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            outputs = self.model(inputs)\n",
    "            loss += self.criterion(outputs, targets)\n",
    "            predicted = outputs > 0.5\n",
    "\n",
    "            \n",
    "#            print(\"predicted\",predicted)\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(targets.cpu().numpy())\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "        \n",
    "        loss = loss / len(self.testset)\n",
    "        ser_acc = correct / total\n",
    "\n",
    "        fairNess_per_batch=self.calculate_fairness(torch.round(outputs).squeeze(), sens.squeeze(), targets.squeeze(),\n",
    "                                                    ser_acc,\n",
    "                                                    all_client_acc,\n",
    "                                                    len_of_data)\n",
    "        final_fairness.append(fairNess_per_batch)\n",
    "        fairness_global=np.mean(final_fairness)\n",
    "\n",
    "        # print(\"loss: %f\\n\" % (loss))\n",
    "        \n",
    "        # print(f\"Global Testing Accuracy: {ser_acc:.5%}\")\n",
    "        # print(f\" Global Fairness: {fairness_global:.5f}%\")\n",
    "        precision = precision_score(true_labels, predicted_labels,zero_division=0.0)\n",
    "        \n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        \n",
    "        # print(f\"Global Precision: {precision:.5%}\")\n",
    "        # print(f\"Global Recall: {recall:.5%}\")\n",
    "        \n",
    "        return loss, ser_acc, precision, recall,fairness_global\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "199eb51c-d78b-44f1-86b0-e8b325ce899c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Serverbase:\n",
    "    def __init__ (self,model):\n",
    "        self.model = model\n",
    "        self.global_model=model\n",
    "        self.num_rounds=2\n",
    "        self.local_epoch=2\n",
    "        self.optimizer=2\n",
    "        self.lr=0.001\n",
    "        self.beta=1\n",
    "        self.batch_size=32\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "    \n",
    "        self.updated_params_cache = []\n",
    "        self.weights_cache = []\n",
    "        self.model_dict = []\n",
    "        \n",
    "        self.global_params_dict: OrderedDict[str : torch.Tensor] = None\n",
    "\n",
    "        self.backbone=DeepNet\n",
    "        _dummy_model = self.backbone()\n",
    "        self.global_params_dict = OrderedDict(_dummy_model.state_dict())\n",
    "\n",
    "        self.temp_dir=\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/\"\n",
    "\n",
    "        self.fair_global_t_step=0\n",
    "        self.fair_local_client=0   \n",
    "        \n",
    "        self.acc_clients=[]\n",
    "        self.clients_id=[]\n",
    "        self.precision_clients=[]\n",
    "        self.recall_clients=[]\n",
    "        \n",
    "        self.global_acc=0\n",
    "        self.fairness_diff=[]\n",
    "        self.clients_weights=[]\n",
    "        self.final_agg_weights_clients=[]\n",
    "        self.selected_clients=[0,1,2,3]\n",
    "        self.statistics=0\n",
    "\n",
    "    def global_fairness(self,weights, all_clients_fairness): \n",
    "        # all_clients_fairness is based on equation: 7\n",
    "        # upper part is done in client class\n",
    "        # self.statistics cal. pr(y=1,a=0)\n",
    "        \n",
    "        # print(\"global_fairness::\",all_clients_fairness)\n",
    "        # print(\"weights::\",weights)\n",
    "        \n",
    "        weight_sum = sum(weights)  \n",
    "        final_global_score=0\n",
    "        \n",
    "        for i in range(len(all_clients_fairness)):\n",
    "            temp=weights[i]/weight_sum            \n",
    "            final_global_score+=temp*all_clients_fairness[i]\n",
    "            \n",
    "        self.fair_global=final_global_score\n",
    "\n",
    "    \n",
    "        \n",
    "    def fedAlgo(self, num_rounds = 2, local_epochs = 2, learning_rate = 0.001, beta = 1, optimizer = 'adam',algo=\"FF\"):\n",
    "        \n",
    "        client = Client()\n",
    "        path=f\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/global_model_{algo}.pt\"\n",
    "        models_directory = \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models/\"\n",
    "        \n",
    "        #before we start the training, we need statistics\n",
    "        print(\"Getting data statistics....\")\n",
    "        self.statistics= client.get_stats()      \n",
    "        \n",
    "\n",
    "        for round_ in tqdm(range(num_rounds)):\n",
    "  \n",
    "            #each round we need empty lists\n",
    "            self.updated_params_cache = []\n",
    "            self.weights_cache = []\n",
    "            self.model_dict=[]\n",
    "            self.clients_weights=[]\n",
    "            self.final_agg_weights_clients=[]\n",
    "            self.fair_global=0\n",
    "            temp_ser_fairness=[]\n",
    "\n",
    "            print (\"round::\",round_)\n",
    "            \n",
    "             # clients and server acc for calculating Fairness whenever fairness_client is undefined\n",
    "        \n",
    "            loss_client, acc_client,precision, recall,len_of_data,all_client_acc = client.client_evaluate() \n",
    "            \n",
    "            loss_client, accuracy_server, pre_server, recall_ser,fairness_global = client.server_evaluate(path,all_client_acc,len_of_data)\n",
    "\n",
    "            \n",
    "            for client_id in self.selected_clients:\n",
    "                self.global_acc=0\n",
    "                \n",
    "                \n",
    "                # weight is length of dataset\n",
    "                \n",
    "                # Clone the model from server and assign it back to the clients\n",
    "                model_path=os.path.join(self.temp_dir, f\"global_model_{algo}.pt\") \n",
    "\n",
    "                if algo==\"FF\":\n",
    "                    \n",
    "                    updated_params, weight, model_dict_list,fairness_client,ser_fairness = client.train_FF(client_id, \n",
    "                                                                                                             model_path, \n",
    "                                                                                                             statistics=self.statistics,\n",
    "                                                                                                             server_acc=accuracy_server,\n",
    "                                                                                                             clients_acc=all_client_acc,\n",
    "                                                                                                             len_client_data=len_of_data,\n",
    "                                                                                                             num_epochs=local_epochs, \n",
    "                                                                                                             learning_rate=learning_rate)        \n",
    "                                \n",
    "                    print(\"updated_params FF\",type(updated_params[0]))\n",
    "                    self.updated_params_cache.append(updated_params)\n",
    "                    self.weights_cache.append(weight)\n",
    "                    self.model_dict.append(model_dict_list)\n",
    "                    temp_ser_fairness.append(ser_fairness)\n",
    "                    \n",
    "                    \n",
    "                    # self.global_acc=acc_server\n",
    "                    self.fair_local_client=fairness_client\n",
    "                \n",
    "                if algo==\"FA\":\n",
    "                    updated_params, weight, = client.train_FA(client_id, model_path,\n",
    "                                                              num_epochs=local_epochs,\n",
    "                                                              learning_rate=learning_rate)\n",
    "                                                                                            \n",
    "                                \n",
    "                    # store it for SecAgg\n",
    "                    self.updated_params_cache.append(updated_params)\n",
    "                    self.weights_cache.append(weight)\n",
    "                    \n",
    "\n",
    "            \n",
    "            self.global_fairness(self.weights_cache, temp_ser_fairness)\n",
    "\n",
    "            print(\"for round\", round_,\", global fairness, accuracy, and recall are: \",self.fair_global,\", \",\n",
    "                                                                                  accuracy_server,\", \",\n",
    "                                                                                  recall_ser,\"\\n\\n\\n\")\n",
    "            \n",
    "            if algo==\"FF\":\n",
    "                # agg all the weights on serverside\n",
    "                # Update the weights of each client\n",
    "                \n",
    "                self.aggregate_client_weight_FedFair_part_1(self.model_dict, self.weights_cache,self.acc_clients, self.global_acc,self.fair_local_client,\n",
    "                                                     self.fair_global)\n",
    "                self.client_weights_update_part_2(self.clients_weights)\n",
    "                self.aggregate_parameters_FedAvg_updated(self.final_agg_weights_clients)\n",
    "                \n",
    "            elif algo==\"FA\":\n",
    "                self.aggregate_parameters_FedAvg(self.updated_params_cache,self.weights_cache)\n",
    "            else:\n",
    "                print(\"--Choose Algo --\")\n",
    "                break\n",
    "        \n",
    "            \n",
    "            torch.save(self.global_model.state_dict(), os.path.join(self.temp_dir, f\"global_model_{algo}.pt\"),)\n",
    "\n",
    "            \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def aggregate_parameters_FedAvg(self, updated_params_cache, weights_cache):\n",
    "        weight_sum = sum(weights_cache)  \n",
    "        \n",
    "        weights = torch.tensor(weights_cache, device=self.device) / weight_sum\n",
    "        \n",
    "        aggregated_params = []\n",
    "\n",
    "        for params in zip(*updated_params_cache):\n",
    "            aggregated_params.append(\n",
    "                torch.sum(weights * torch.stack(params, dim=-1), dim=-1)\n",
    "            )\n",
    "        \n",
    "        aggregated_model_params = {}\n",
    "        global_state_dict_keys = list(self.global_model.state_dict().keys())\n",
    "        \n",
    "        for param_name, param_value in zip(global_state_dict_keys, aggregated_params):\n",
    "            aggregated_model_params[param_name] = param_value\n",
    "\n",
    "\n",
    "        # aggregated_model_params = {param_name: param_value for param_name, param_value in zip(self.global_model.state_dict().keys(), aggregated_params)}\n",
    "\n",
    "        self.global_model.load_state_dict(aggregated_model_params)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def aggregate_parameters_FedAvg_updated(self, model_dict_list):        \n",
    "        w_avg = copy.deepcopy(model_dict_list[0])\n",
    "        for k in w_avg.keys():\n",
    "            for i in range(1, len(model_dict_list)):\n",
    "                w_avg[k] += model_dict_list[i][k]\n",
    "            w_avg[k] = torch.div(w_avg[k], len(model_dict_list))\n",
    "    \n",
    "        self.global_model.load_state_dict(w_avg)\n",
    "\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def aggregate_client_weight_FedFair_part_1 (self, model_dict_list ,weights_cache, acc_clients, global_acc,fairness_client,fairness_global):\n",
    "        \n",
    "\n",
    "        Delta_t_C_i = abs(self.fair_global - self.fair_local_client) \n",
    "        \n",
    "        for client_id in range(len(self.selected_clients)):\n",
    "        \n",
    "            w_avg = copy.deepcopy(model_dict_list[client_id])\n",
    "            temp = copy.deepcopy(model_dict_list[client_id])\n",
    "            \n",
    "            for k in w_avg.keys():\n",
    "                for i in range(1, len(model_dict_list)):\n",
    "                    w_avg[k] += model_dict_list[i][k]\n",
    "                w_avg[k] = torch.div(w_avg[k], len(model_dict_list))\n",
    "                \n",
    "                w_avg[k]=temp[k]-(self.beta*(Delta_t_C_i-w_avg[k]))\n",
    "                \n",
    "            self.clients_weights.append(w_avg)\n",
    "\n",
    "            \n",
    "    def client_weights_update_part_2(self,agg_weights):\n",
    "        # print(\"Before agg_weights:: \", agg_weights[0]['layer1.weight'])     \n",
    "        \n",
    "        temp=copy.deepcopy(agg_weights)\n",
    "        sum_params = temp[0]\n",
    "        \n",
    "        for model in temp[1:]:\n",
    "            for name, param in model.items():\n",
    "                \n",
    "                if name in sum_params:\n",
    "                    sum_params[name] += param\n",
    "                    \n",
    "\n",
    "        # print(\"\\n sum_params\",sum_params)\n",
    "        # print(\"\\n Before agg_weights\",agg_weights[0])\n",
    "        # print(\"\\n sum_params keys:\", sum_params.keys())\n",
    "        # print(\"\\n model keys:\", model.keys())\n",
    "\n",
    "        for model in agg_weights:\n",
    "             for name in model.keys():\n",
    "                 print(\"name\",name,\"\\n\")\n",
    "                 print(\"\\n model\",model[name])\n",
    "                 print(\"\\n sum_params[name]\",sum_params[name])               \n",
    "                 print(\"\\n model[name]  /= sum_params[name]\",model[name]  / sum_params[name])\n",
    "                 \n",
    "                 model[name]  /= sum_params[name]\n",
    "             # print(\"type of mdoel\", model)\n",
    "             self.final_agg_weights_clients.append(model)\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf995c8-5238-44f1-aa4d-e927101b0a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb97600f-7474-47ae-bd82-89bcad80e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm is  FF\n",
      "Getting data statistics....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                   | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:: 0\n",
      "======Validation========\n",
      "Epoch 1/1, Val Accuracy: 0.77468%\n",
      "Epoch 1/1, Val Fairness: 0.01208%\n",
      "Epoch 1/1, Val Recall: 0.74620%\n",
      "client_0 is updated \n",
      "\n",
      "\n",
      "\n",
      "updated_params FF <class 'torch.Tensor'>\n",
      "======Validation========\n",
      "Epoch 1/1, Val Accuracy: 0.76579%\n",
      "Epoch 1/1, Val Fairness: 0.01208%\n",
      "Epoch 1/1, Val Recall: 0.65910%\n",
      "client_1 is updated \n",
      "\n",
      "\n",
      "\n",
      "updated_params FF <class 'torch.Tensor'>\n",
      "======Validation========\n",
      "Epoch 1/1, Val Accuracy: 0.79450%\n",
      "Epoch 1/1, Val Fairness: 0.01208%\n",
      "Epoch 1/1, Val Recall: 0.76379%\n",
      "client_2 is updated \n",
      "\n",
      "\n",
      "\n",
      "updated_params FF <class 'torch.Tensor'>\n",
      "======Validation========\n",
      "Epoch 1/1, Val Accuracy: 0.76839%\n",
      "Epoch 1/1, Val Fairness: 0.01208%\n",
      "Epoch 1/1, Val Recall: 0.69796%\n",
      "client_3 is updated \n",
      "\n",
      "\n",
      "\n",
      "updated_params FF <class 'torch.Tensor'>\n",
      "for round 0 , global fairness, accuracy, and recall are:  0.012078994821371702 ,  0.3668816454592993 ,  1.0 \n",
      "\n",
      "\n",
      "\n",
      "name layer1.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.2594, -0.3976,  0.1014,  ...,  0.1605,  0.3764,  0.2278],\n",
      "        [-0.0603, -0.2498,  0.4555,  ...,  0.0047,  0.0211, -0.5338],\n",
      "        [ 0.0142, -0.2424,  0.5220,  ...,  0.2355,  0.4628, -0.5232],\n",
      "        ...,\n",
      "        [-0.0353,  0.0278, -0.3114,  ..., -0.2814,  0.3428,  0.1711],\n",
      "        [-0.4060, -0.4858, -0.3467,  ...,  0.2642,  0.0740,  0.0224],\n",
      "        [ 0.3685,  0.1776,  0.1710,  ..., -0.0011, -0.2734, -0.0445]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 1.0376, -1.5904,  0.4056,  ...,  0.6420,  1.5056,  0.9111],\n",
      "        [-0.2412, -0.9991,  1.8220,  ...,  0.0190,  0.0842, -2.1352],\n",
      "        [ 0.0569, -0.9697,  2.0882,  ...,  0.9422,  1.8511, -2.0929],\n",
      "        ...,\n",
      "        [-0.1413,  0.1111, -1.2456,  ..., -1.1256,  1.3714,  0.6843],\n",
      "        [-1.6240, -1.9431, -1.3870,  ...,  1.0569,  0.2961,  0.0896],\n",
      "        [ 1.4741,  0.7104,  0.6841,  ..., -0.0044, -1.0934, -0.1782]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer1.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0533, -0.2976,  0.1683, -0.2796,  0.4783, -0.1859,  0.1970,  0.2235,\n",
      "         0.4476, -0.4746,  0.0582,  0.2373, -0.0943, -0.0593, -0.0383,  0.4056,\n",
      "         0.1094,  0.3961,  0.2945, -0.4114,  0.2467,  0.1748, -0.2457,  0.0433,\n",
      "        -0.2843, -0.4822, -0.0307,  0.0962,  0.3481,  0.2948, -0.1386, -0.2260,\n",
      "        -0.1408, -0.2667,  0.3383, -0.2663,  0.2134, -0.2520, -0.3442, -0.5159,\n",
      "        -0.3523,  0.0703,  0.1675, -0.4977,  0.0274, -0.5294, -0.4071, -0.2197,\n",
      "         0.1088,  0.3603, -0.0918,  0.0754,  0.4334,  0.3057,  0.3766, -0.1408,\n",
      "        -0.3021,  0.3022, -0.2236,  0.3569,  0.4050, -0.2624,  0.4538,  0.1725,\n",
      "         0.0586,  0.2513, -0.1856,  0.3054, -0.2262, -0.1141, -0.0513,  0.2058,\n",
      "        -0.3756,  0.2403, -0.2751, -0.3446,  0.2654, -0.0293, -0.1686,  0.3292,\n",
      "         0.4432, -0.4684, -0.2763, -0.3647,  0.4375, -0.1154,  0.1397,  0.4050,\n",
      "         0.3931, -0.1647, -0.1753, -0.3663, -0.4981, -0.1992,  0.2200,  0.1146,\n",
      "        -0.3251,  0.0533,  0.0160,  0.2042, -0.2496,  0.1141, -0.4078, -0.1424,\n",
      "         0.1085, -0.4270, -0.3692, -0.2484, -0.4747, -0.1498, -0.5340, -0.0314,\n",
      "        -0.1802, -0.0993, -0.4582,  0.2455, -0.2263, -0.0550, -0.1619, -0.1929,\n",
      "         0.0914,  0.1237, -0.0710,  0.4401, -0.2588,  0.4174,  0.3714,  0.2574,\n",
      "        -0.3506, -0.4329,  0.1630,  0.4667, -0.0287,  0.3322, -0.1821, -0.3801,\n",
      "        -0.2611,  0.4145, -0.0007, -0.3786, -0.3146, -0.0159,  0.0102,  0.2508,\n",
      "        -0.3237, -0.3670,  0.1127, -0.5219, -0.2609,  0.3361,  0.2909,  0.2630,\n",
      "         0.3016, -0.3917, -0.4872, -0.4266,  0.2443, -0.3212, -0.0918, -0.0804,\n",
      "        -0.2903, -0.5236, -0.1680,  0.3135,  0.0755, -0.4906,  0.2998, -0.4246,\n",
      "         0.0407, -0.1521,  0.2767, -0.3670, -0.3565,  0.2147,  0.2308,  0.3634,\n",
      "         0.0052,  0.0903,  0.3477,  0.1473,  0.0694, -0.0430, -0.2624, -0.0924,\n",
      "         0.3793,  0.0731, -0.5296,  0.2131,  0.3267,  0.2794, -0.3457,  0.3043,\n",
      "         0.0422,  0.0724, -0.3112,  0.0859, -0.2733, -0.2427,  0.1653, -0.1537,\n",
      "         0.0476, -0.2495, -0.4828, -0.1244, -0.1088,  0.0046, -0.0012,  0.4617,\n",
      "         0.2375, -0.3458,  0.1529, -0.3748, -0.0276, -0.2087,  0.1731, -0.3386,\n",
      "         0.1302, -0.1685,  0.3728,  0.0889, -0.3937,  0.2867,  0.3718, -0.3299,\n",
      "        -0.3796, -0.4960,  0.0468, -0.4306, -0.3099, -0.3600,  0.1805,  0.4462,\n",
      "         0.0164, -0.4883, -0.0203,  0.3129, -0.5222,  0.0441,  0.0137,  0.0102,\n",
      "        -0.1568, -0.5443, -0.0731, -0.1525,  0.2775, -0.1180,  0.4777, -0.3341,\n",
      "        -0.3536,  0.2311, -0.1433, -0.3046,  0.1661,  0.2469, -0.3509,  0.3217,\n",
      "        -0.2336,  0.4728,  0.3464, -0.2160, -0.3805, -0.4916,  0.0239,  0.0339,\n",
      "         0.3407,  0.2819, -0.0255, -0.4977,  0.3217,  0.2836,  0.3084,  0.3222,\n",
      "        -0.0258,  0.2586,  0.0075, -0.4786, -0.2350,  0.3078,  0.0185,  0.1093,\n",
      "         0.0401, -0.0789,  0.1429, -0.3439,  0.2971, -0.4580, -0.0578, -0.5280,\n",
      "         0.2342,  0.3148, -0.4135, -0.2279, -0.4449, -0.3578,  0.3702,  0.1336,\n",
      "        -0.5069, -0.3149, -0.5344,  0.4053, -0.1074,  0.3561, -0.1516,  0.0733,\n",
      "         0.2813,  0.2799,  0.2204,  0.2000, -0.3559, -0.1788, -0.1005,  0.1587,\n",
      "        -0.3056,  0.0480, -0.1572,  0.0988, -0.3203,  0.3752,  0.1441, -0.1858,\n",
      "         0.0496, -0.2569, -0.0618,  0.0264, -0.4264, -0.1943, -0.1492,  0.3311,\n",
      "         0.1900,  0.4722,  0.1155,  0.3057, -0.3209,  0.4003, -0.0986,  0.1345,\n",
      "         0.3535, -0.1337, -0.0899,  0.3471,  0.4302, -0.3008, -0.0366, -0.2108,\n",
      "        -0.1828,  0.4011, -0.5035,  0.2733,  0.0909, -0.0391,  0.3933, -0.0676,\n",
      "        -0.3030, -0.1426, -0.0465,  0.0018, -0.0830,  0.0181,  0.0739, -0.1898,\n",
      "        -0.3978,  0.1440,  0.2752, -0.1834,  0.1594,  0.0292, -0.3882, -0.3835,\n",
      "         0.3140,  0.1215, -0.2803, -0.2791, -0.1879,  0.4263,  0.2653, -0.4888,\n",
      "        -0.3841, -0.5022,  0.2064, -0.1732,  0.2672, -0.4353,  0.1857, -0.4265,\n",
      "        -0.2006,  0.2788, -0.3028,  0.4202,  0.2692,  0.4319,  0.3084, -0.4182,\n",
      "         0.2253, -0.3089,  0.0870,  0.0059, -0.3054, -0.0176, -0.4108, -0.0083,\n",
      "        -0.3409,  0.4043, -0.1234,  0.1365,  0.1504, -0.0527,  0.4243,  0.0080,\n",
      "         0.1225,  0.3391,  0.1453, -0.0215, -0.3462,  0.1187, -0.4027,  0.0875,\n",
      "         0.4066, -0.0149,  0.3713, -0.1068,  0.1035,  0.3800,  0.2460,  0.3619,\n",
      "        -0.4111, -0.1826,  0.2778, -0.3398,  0.3421,  0.4399, -0.3789,  0.3259,\n",
      "        -0.1909, -0.2622,  0.4355,  0.1426,  0.1120,  0.3445,  0.0265, -0.0767,\n",
      "        -0.1538, -0.4370,  0.3049,  0.2692, -0.0558, -0.0103, -0.2042,  0.3164,\n",
      "         0.3008, -0.0461,  0.1757,  0.2354, -0.4257, -0.3459,  0.0008,  0.0081,\n",
      "         0.3492,  0.2701, -0.0588,  0.3723,  0.2743,  0.3463, -0.2644, -0.2942,\n",
      "        -0.3763, -0.1899, -0.1459, -0.5280,  0.3025,  0.0344, -0.4106,  0.2667,\n",
      "        -0.0136, -0.0609, -0.2359,  0.0552,  0.2563,  0.1276,  0.0555, -0.3754,\n",
      "        -0.2732,  0.1979, -0.4710, -0.2199, -0.3255, -0.0013, -0.0319, -0.4547,\n",
      "         0.1165, -0.0975,  0.1514,  0.0644, -0.2438,  0.0285, -0.0318,  0.1264,\n",
      "        -0.4959, -0.1553, -0.0676, -0.1071, -0.2301, -0.4652, -0.3956,  0.1989,\n",
      "         0.3353,  0.3988, -0.2981, -0.4626,  0.4522, -0.2912,  0.1587,  0.1767],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.2131, -1.1906,  0.6731, -1.1182,  1.9130, -0.7437,  0.7880,  0.8939,\n",
      "         1.7904, -1.8985,  0.2328,  0.9491, -0.3773, -0.2372, -0.1531,  1.6225,\n",
      "         0.4378,  1.5845,  1.1779, -1.6454,  0.9866,  0.6994, -0.9830,  0.1732,\n",
      "        -1.1372, -1.9290, -0.1227,  0.3849,  1.3923,  1.1791, -0.5545, -0.9040,\n",
      "        -0.5633, -1.0668,  1.3533, -1.0651,  0.8536, -1.0079, -1.3769, -2.0637,\n",
      "        -1.4092,  0.2810,  0.6701, -1.9910,  0.1098, -2.1177, -1.6282, -0.8786,\n",
      "         0.4351,  1.4413, -0.3673,  0.3018,  1.7335,  1.2228,  1.5062, -0.5631,\n",
      "        -1.2085,  1.2088, -0.8945,  1.4276,  1.6199, -1.0497,  1.8152,  0.6902,\n",
      "         0.2343,  1.0054, -0.7423,  1.2216, -0.9049, -0.4566, -0.2051,  0.8231,\n",
      "        -1.5025,  0.9612, -1.1005, -1.3785,  1.0616, -0.1173, -0.6746,  1.3166,\n",
      "         1.7728, -1.8736, -1.1052, -1.4588,  1.7499, -0.4615,  0.5589,  1.6201,\n",
      "         1.5723, -0.6588, -0.7014, -1.4652, -1.9924, -0.7969,  0.8800,  0.4585,\n",
      "        -1.3004,  0.2133,  0.0640,  0.8167, -0.9985,  0.4563, -1.6313, -0.5697,\n",
      "         0.4339, -1.7081, -1.4767, -0.9935, -1.8990, -0.5992, -2.1360, -0.1256,\n",
      "        -0.7208, -0.3974, -1.8328,  0.9821, -0.9051, -0.2199, -0.6476, -0.7714,\n",
      "         0.3655,  0.4949, -0.2839,  1.7602, -1.0352,  1.6697,  1.4857,  1.0295,\n",
      "        -1.4025, -1.7317,  0.6519,  1.8667, -0.1150,  1.3287, -0.7285, -1.5205,\n",
      "        -1.0445,  1.6579, -0.0028, -1.5144, -1.2584, -0.0636,  0.0409,  1.0034,\n",
      "        -1.2948, -1.4682,  0.4508, -2.0877, -1.0438,  1.3445,  1.1635,  1.0521,\n",
      "         1.2066, -1.5669, -1.9486, -1.7063,  0.9772, -1.2846, -0.3671, -0.3215,\n",
      "        -1.1613, -2.0944, -0.6722,  1.2541,  0.3020, -1.9626,  1.1994, -1.6985,\n",
      "         0.1627, -0.6083,  1.1068, -1.4679, -1.4260,  0.8589,  0.9231,  1.4537,\n",
      "         0.0207,  0.3611,  1.3906,  0.5891,  0.2776, -0.1719, -1.0498, -0.3695,\n",
      "         1.5173,  0.2924, -2.1183,  0.8526,  1.3069,  1.1175, -1.3828,  1.2173,\n",
      "         0.1689,  0.2897, -1.2448,  0.3436, -1.0933, -0.9708,  0.6612, -0.6149,\n",
      "         0.1904, -0.9979, -1.9311, -0.4977, -0.4352,  0.0183, -0.0047,  1.8466,\n",
      "         0.9499, -1.3830,  0.6117, -1.4993, -0.1103, -0.8349,  0.6923, -1.3546,\n",
      "         0.5210, -0.6739,  1.4912,  0.3554, -1.5749,  1.1469,  1.4872, -1.3197,\n",
      "        -1.5184, -1.9840,  0.1870, -1.7225, -1.2398, -1.4401,  0.7221,  1.7849,\n",
      "         0.0654, -1.9533, -0.0813,  1.2516, -2.0887,  0.1764,  0.0549,  0.0409,\n",
      "        -0.6273, -2.1770, -0.2924, -0.6098,  1.1099, -0.4720,  1.9108, -1.3365,\n",
      "        -1.4145,  0.9243, -0.5732, -1.2183,  0.6646,  0.9877, -1.4038,  1.2869,\n",
      "        -0.9345,  1.8911,  1.3854, -0.8641, -1.5221, -1.9665,  0.0955,  0.1354,\n",
      "         1.3629,  1.1275, -0.1020, -1.9906,  1.2867,  1.1343,  1.2337,  1.2887,\n",
      "        -0.1033,  1.0344,  0.0301, -1.9145, -0.9401,  1.2311,  0.0739,  0.4373,\n",
      "         0.1606, -0.3156,  0.5717, -1.3757,  1.1886, -1.8321, -0.2312, -2.1120,\n",
      "         0.9370,  1.2593, -1.6539, -0.9116, -1.7795, -1.4310,  1.4807,  0.5343,\n",
      "        -2.0275, -1.2595, -2.1376,  1.6212, -0.4295,  1.4244, -0.6065,  0.2934,\n",
      "         1.1253,  1.1195,  0.8815,  0.7998, -1.4238, -0.7153, -0.4020,  0.6349,\n",
      "        -1.2225,  0.1919, -0.6289,  0.3952, -1.2812,  1.5006,  0.5765, -0.7431,\n",
      "         0.1983, -1.0275, -0.2470,  0.1057, -1.7058, -0.7773, -0.5967,  1.3246,\n",
      "         0.7600,  1.8889,  0.4619,  1.2226, -1.2838,  1.6011, -0.3944,  0.5381,\n",
      "         1.4140, -0.5347, -0.3597,  1.3885,  1.7208, -1.2032, -0.1463, -0.8433,\n",
      "        -0.7312,  1.6046, -2.0138,  1.0933,  0.3635, -0.1562,  1.5733, -0.2705,\n",
      "        -1.2121, -0.5703, -0.1859,  0.0073, -0.3320,  0.0725,  0.2955, -0.7593,\n",
      "        -1.5914,  0.5759,  1.1008, -0.7336,  0.6374,  0.1168, -1.5529, -1.5339,\n",
      "         1.2558,  0.4860, -1.1212, -1.1166, -0.7516,  1.7053,  1.0611, -1.9554,\n",
      "        -1.5363, -2.0087,  0.8257, -0.6929,  1.0686, -1.7412,  0.7426, -1.7062,\n",
      "        -0.8026,  1.1151, -1.2112,  1.6808,  1.0769,  1.7275,  1.2334, -1.6729,\n",
      "         0.9011, -1.2357,  0.3480,  0.0237, -1.2215, -0.0702, -1.6433, -0.0330,\n",
      "        -1.3638,  1.6171, -0.4937,  0.5458,  0.6017, -0.2107,  1.6971,  0.0319,\n",
      "         0.4902,  1.3563,  0.5812, -0.0860, -1.3850,  0.4749, -1.6107,  0.3498,\n",
      "         1.6265, -0.0594,  1.4850, -0.4271,  0.4141,  1.5199,  0.9841,  1.4477,\n",
      "        -1.6446, -0.7305,  1.1113, -1.3593,  1.3684,  1.7597, -1.5155,  1.3034,\n",
      "        -0.7636, -1.0487,  1.7421,  0.5703,  0.4478,  1.3780,  0.1061, -0.3068,\n",
      "        -0.6152, -1.7480,  1.2195,  1.0768, -0.2231, -0.0413, -0.8166,  1.2656,\n",
      "         1.2032, -0.1842,  0.7027,  0.9416, -1.7029, -1.3836,  0.0033,  0.0324,\n",
      "         1.3969,  1.0806, -0.2351,  1.4891,  1.0973,  1.3851, -1.0574, -1.1769,\n",
      "        -1.5053, -0.7595, -0.5838, -2.1122,  1.2101,  0.1377, -1.6425,  1.0670,\n",
      "        -0.0542, -0.2436, -0.9437,  0.2207,  1.0251,  0.5105,  0.2218, -1.5015,\n",
      "        -1.0929,  0.7918, -1.8841, -0.8797, -1.3020, -0.0050, -0.1278, -1.8188,\n",
      "         0.4661, -0.3899,  0.6054,  0.2576, -0.9753,  0.1138, -0.1273,  0.5054,\n",
      "        -1.9837, -0.6214, -0.2703, -0.4285, -0.9202, -1.8610, -1.5826,  0.7955,\n",
      "         1.3411,  1.5950, -1.1925, -1.8503,  1.8087, -1.1648,  0.6348,  0.7067],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
      "       device='cuda:0')\n",
      "name layer2.weight \n",
      "\n",
      "\n",
      " model tensor([[-0.0634, -0.0549,  0.0018,  ...,  0.0458, -0.0822, -0.0203],\n",
      "        [ 0.0536,  0.0359,  0.0015,  ..., -0.0115, -0.0132,  0.0060],\n",
      "        [-0.0662, -0.0451,  0.0247,  ...,  0.0570,  0.0432, -0.0235],\n",
      "        ...,\n",
      "        [ 0.0251, -0.0267, -0.0191,  ..., -0.0429, -0.0456,  0.0241],\n",
      "        [ 0.1043, -0.0145,  0.0097,  ..., -0.0637, -0.0198,  0.0129],\n",
      "        [ 0.0114,  0.0589,  0.0012,  ...,  0.0182,  0.0218, -0.0614]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[-0.2534, -0.2197,  0.0070,  ...,  0.1831, -0.3286, -0.0811],\n",
      "        [ 0.2145,  0.1437,  0.0058,  ..., -0.0459, -0.0527,  0.0238],\n",
      "        [-0.2646, -0.1803,  0.0989,  ...,  0.2280,  0.1728, -0.0941],\n",
      "        ...,\n",
      "        [ 0.1004, -0.1066, -0.0764,  ..., -0.1718, -0.1823,  0.0963],\n",
      "        [ 0.4172, -0.0581,  0.0387,  ..., -0.2550, -0.0793,  0.0516],\n",
      "        [ 0.0455,  0.2356,  0.0048,  ...,  0.0728,  0.0871, -0.2454]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer2.bias \n",
      "\n",
      "\n",
      " model tensor([ 6.6205e-03, -3.2377e-02, -4.5958e-02,  6.1730e-02,  5.1142e-02,\n",
      "        -1.7462e-02, -5.4124e-02, -1.1232e-02,  5.8617e-02, -6.6676e-02,\n",
      "         3.8840e-02, -4.0419e-02,  9.2222e-03, -6.6494e-02, -4.6496e-02,\n",
      "        -1.2460e-02, -6.5301e-04, -6.5983e-02,  2.2284e-02, -2.4886e-02,\n",
      "         4.2531e-02,  8.4915e-02,  5.9802e-02, -3.4652e-03, -2.8837e-02,\n",
      "         6.0173e-02, -6.3772e-02, -8.8647e-02, -3.2353e-02,  1.2800e-01,\n",
      "         7.2008e-02,  8.0851e-02, -4.0117e-02,  2.5901e-02,  3.5606e-02,\n",
      "        -7.7808e-02,  3.5788e-03, -2.1756e-02, -1.2306e-01,  4.8123e-02,\n",
      "         1.1054e-02, -7.2002e-02,  7.8845e-02,  7.1751e-03,  3.9806e-02,\n",
      "         2.8828e-02, -3.6909e-02,  5.8479e-02, -7.0202e-02,  5.6132e-02,\n",
      "        -3.1911e-02, -1.9451e-02,  1.2588e-02, -4.9605e-02, -2.6415e-02,\n",
      "         5.0884e-02,  4.3441e-02, -1.1917e-02, -3.6576e-02,  5.1420e-02,\n",
      "        -8.2422e-02,  7.8674e-02, -3.3395e-02, -1.1349e-02,  3.8636e-03,\n",
      "        -3.9502e-03,  4.0672e-02, -2.6127e-02,  2.6751e-02, -2.2839e-02,\n",
      "        -3.9352e-02,  2.5426e-02,  3.9454e-02, -4.3014e-02, -6.4135e-02,\n",
      "         2.4835e-02, -1.9392e-02,  4.7930e-02,  5.5934e-02,  2.8320e-02,\n",
      "        -8.6030e-02,  1.4140e-02,  2.4989e-02,  4.3506e-02, -4.9501e-02,\n",
      "         3.8060e-02, -2.3130e-02, -5.5456e-02,  7.4137e-02,  6.9622e-02,\n",
      "        -5.7255e-02, -1.2605e-02, -3.3073e-02, -5.0543e-02, -2.6859e-02,\n",
      "         2.7225e-02, -3.9992e-02, -1.5378e-01,  4.2800e-02, -1.3810e-04,\n",
      "         4.8996e-02, -7.1443e-02, -1.6780e-02, -1.1945e-02, -7.4407e-02,\n",
      "        -3.6141e-02, -1.2170e-02, -5.0433e-02, -7.2944e-02, -1.8268e-02,\n",
      "        -8.9378e-02,  7.2016e-03,  5.4560e-02, -5.7141e-02,  5.8410e-03,\n",
      "         7.8669e-02,  4.9474e-02, -2.1693e-02, -8.4660e-02,  5.1386e-02,\n",
      "         8.8519e-03,  4.5689e-02, -2.3721e-02,  4.9863e-02,  3.7764e-03,\n",
      "         4.1239e-02,  1.1756e-01,  2.4989e-02, -1.7260e-02, -7.1427e-02,\n",
      "        -7.9698e-02, -7.5401e-02,  7.6057e-02,  4.5944e-02,  1.5109e-02,\n",
      "        -4.0553e-02,  5.7381e-02,  2.7081e-02, -1.4482e-02, -7.1393e-02,\n",
      "        -1.1495e-02, -2.8827e-02, -5.9305e-02, -9.3016e-02, -8.8831e-02,\n",
      "         5.9434e-02, -4.1125e-03, -9.1109e-02, -9.7782e-02,  6.5756e-02,\n",
      "        -6.5883e-02, -6.4734e-02,  2.9286e-05, -7.8842e-02,  2.7385e-02,\n",
      "        -3.7701e-02,  3.7715e-02,  9.7803e-02, -5.0331e-02, -6.7174e-02,\n",
      "        -1.5403e-02, -1.3481e-02,  6.7517e-02,  6.1399e-02, -8.1103e-02,\n",
      "        -1.1418e-01,  7.5766e-02,  4.6393e-03,  6.1258e-02, -1.7248e-02,\n",
      "         6.2396e-02, -3.1449e-02,  2.7584e-02,  3.5604e-02, -7.7220e-02,\n",
      "        -6.0437e-02, -3.1787e-02,  4.3068e-02,  6.6608e-02,  5.1794e-02,\n",
      "         6.3146e-02, -8.5386e-02, -7.8354e-02,  4.2019e-02, -7.6459e-02,\n",
      "         6.8323e-02,  9.4791e-02, -6.3084e-02, -1.5255e-03,  1.0377e-02,\n",
      "         6.8170e-02, -5.7751e-02,  1.0723e-02,  7.7447e-02, -9.2818e-03,\n",
      "        -9.2200e-02, -5.3574e-02, -5.7750e-02, -1.9773e-02, -8.3680e-02,\n",
      "        -6.3854e-03, -3.3826e-02, -5.6497e-02, -2.6097e-02, -5.3209e-02,\n",
      "         1.5681e-03, -1.8333e-02,  5.8289e-02, -8.5990e-02,  6.7770e-02,\n",
      "        -1.0025e-01,  2.6034e-02,  3.0845e-02, -4.5089e-02, -2.0955e-03,\n",
      "         1.6112e-02,  5.3509e-02, -3.8658e-03,  9.3778e-02, -6.5697e-02,\n",
      "        -2.8214e-02,  3.9072e-02, -2.7483e-02,  7.7915e-02, -1.8103e-02,\n",
      "        -2.3917e-02, -5.3242e-03,  6.8477e-02,  7.3193e-02,  4.7106e-02,\n",
      "        -6.8053e-02,  5.2855e-02,  7.4291e-02,  2.1017e-02, -6.6047e-02,\n",
      "        -1.9512e-02,  5.4492e-02,  1.8015e-02,  4.1885e-02,  1.7024e-02,\n",
      "        -8.1572e-02,  9.6217e-03, -1.0126e-01, -5.6150e-02, -8.1864e-02,\n",
      "        -5.3142e-02,  4.3732e-02, -4.9532e-03, -5.4868e-02, -8.3943e-02,\n",
      "         3.6248e-02, -2.0443e-02,  4.5764e-02,  2.3292e-02,  9.1441e-02,\n",
      "        -4.6304e-02], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([ 2.6482e-02, -1.2951e-01, -1.8383e-01,  2.4692e-01,  2.0457e-01,\n",
      "        -6.9849e-02, -2.1649e-01, -4.4930e-02,  2.3447e-01, -2.6670e-01,\n",
      "         1.5536e-01, -1.6168e-01,  3.6889e-02, -2.6598e-01, -1.8598e-01,\n",
      "        -4.9841e-02, -2.6121e-03, -2.6393e-01,  8.9138e-02, -9.9545e-02,\n",
      "         1.7012e-01,  3.3966e-01,  2.3921e-01, -1.3861e-02, -1.1535e-01,\n",
      "         2.4069e-01, -2.5509e-01, -3.5459e-01, -1.2941e-01,  5.1200e-01,\n",
      "         2.8803e-01,  3.2340e-01, -1.6047e-01,  1.0360e-01,  1.4243e-01,\n",
      "        -3.1123e-01,  1.4315e-02, -8.7023e-02, -4.9225e-01,  1.9249e-01,\n",
      "         4.4216e-02, -2.8801e-01,  3.1538e-01,  2.8700e-02,  1.5922e-01,\n",
      "         1.1531e-01, -1.4764e-01,  2.3392e-01, -2.8081e-01,  2.2453e-01,\n",
      "        -1.2764e-01, -7.7804e-02,  5.0351e-02, -1.9842e-01, -1.0566e-01,\n",
      "         2.0354e-01,  1.7377e-01, -4.7669e-02, -1.4630e-01,  2.0568e-01,\n",
      "        -3.2969e-01,  3.1470e-01, -1.3358e-01, -4.5395e-02,  1.5454e-02,\n",
      "        -1.5801e-02,  1.6269e-01, -1.0451e-01,  1.0700e-01, -9.1358e-02,\n",
      "        -1.5741e-01,  1.0170e-01,  1.5782e-01, -1.7206e-01, -2.5654e-01,\n",
      "         9.9338e-02, -7.7569e-02,  1.9172e-01,  2.2374e-01,  1.1328e-01,\n",
      "        -3.4412e-01,  5.6559e-02,  9.9956e-02,  1.7403e-01, -1.9800e-01,\n",
      "         1.5224e-01, -9.2521e-02, -2.2183e-01,  2.9655e-01,  2.7849e-01,\n",
      "        -2.2902e-01, -5.0421e-02, -1.3229e-01, -2.0217e-01, -1.0744e-01,\n",
      "         1.0890e-01, -1.5997e-01, -6.1513e-01,  1.7120e-01, -5.5240e-04,\n",
      "         1.9598e-01, -2.8577e-01, -6.7118e-02, -4.7782e-02, -2.9763e-01,\n",
      "        -1.4456e-01, -4.8680e-02, -2.0173e-01, -2.9177e-01, -7.3074e-02,\n",
      "        -3.5751e-01,  2.8807e-02,  2.1824e-01, -2.2856e-01,  2.3364e-02,\n",
      "         3.1468e-01,  1.9790e-01, -8.6772e-02, -3.3864e-01,  2.0554e-01,\n",
      "         3.5408e-02,  1.8276e-01, -9.4883e-02,  1.9945e-01,  1.5106e-02,\n",
      "         1.6495e-01,  4.7023e-01,  9.9957e-02, -6.9039e-02, -2.8571e-01,\n",
      "        -3.1879e-01, -3.0160e-01,  3.0423e-01,  1.8378e-01,  6.0437e-02,\n",
      "        -1.6221e-01,  2.2952e-01,  1.0832e-01, -5.7927e-02, -2.8557e-01,\n",
      "        -4.5980e-02, -1.1531e-01, -2.3722e-01, -3.7206e-01, -3.5532e-01,\n",
      "         2.3774e-01, -1.6450e-02, -3.6444e-01, -3.9113e-01,  2.6302e-01,\n",
      "        -2.6353e-01, -2.5894e-01,  1.1714e-04, -3.1537e-01,  1.0954e-01,\n",
      "        -1.5080e-01,  1.5086e-01,  3.9121e-01, -2.0133e-01, -2.6870e-01,\n",
      "        -6.1614e-02, -5.3923e-02,  2.7007e-01,  2.4559e-01, -3.2441e-01,\n",
      "        -4.5673e-01,  3.0306e-01,  1.8557e-02,  2.4503e-01, -6.8993e-02,\n",
      "         2.4959e-01, -1.2579e-01,  1.1033e-01,  1.4242e-01, -3.0888e-01,\n",
      "        -2.4175e-01, -1.2715e-01,  1.7227e-01,  2.6643e-01,  2.0718e-01,\n",
      "         2.5258e-01, -3.4154e-01, -3.1342e-01,  1.6808e-01, -3.0583e-01,\n",
      "         2.7329e-01,  3.7916e-01, -2.5233e-01, -6.1019e-03,  4.1509e-02,\n",
      "         2.7268e-01, -2.3100e-01,  4.2892e-02,  3.0979e-01, -3.7127e-02,\n",
      "        -3.6880e-01, -2.1429e-01, -2.3100e-01, -7.9094e-02, -3.3472e-01,\n",
      "        -2.5542e-02, -1.3530e-01, -2.2599e-01, -1.0439e-01, -2.1284e-01,\n",
      "         6.2723e-03, -7.3330e-02,  2.3316e-01, -3.4396e-01,  2.7108e-01,\n",
      "        -4.0098e-01,  1.0413e-01,  1.2338e-01, -1.8036e-01, -8.3819e-03,\n",
      "         6.4446e-02,  2.1404e-01, -1.5463e-02,  3.7511e-01, -2.6279e-01,\n",
      "        -1.1286e-01,  1.5629e-01, -1.0993e-01,  3.1166e-01, -7.2410e-02,\n",
      "        -9.5668e-02, -2.1297e-02,  2.7391e-01,  2.9277e-01,  1.8842e-01,\n",
      "        -2.7221e-01,  2.1142e-01,  2.9716e-01,  8.4069e-02, -2.6419e-01,\n",
      "        -7.8046e-02,  2.1797e-01,  7.2058e-02,  1.6754e-01,  6.8098e-02,\n",
      "        -3.2629e-01,  3.8487e-02, -4.0502e-01, -2.2460e-01, -3.2746e-01,\n",
      "        -2.1257e-01,  1.7493e-01, -1.9813e-02, -2.1947e-01, -3.3577e-01,\n",
      "         1.4499e-01, -8.1772e-02,  1.8306e-01,  9.3166e-02,  3.6577e-01,\n",
      "        -1.8522e-01], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name layer3.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.0534,  0.0394, -0.0593,  ...,  0.0331, -0.0660, -0.1512],\n",
      "        [-0.0707,  0.0280,  0.0912,  ...,  0.0649,  0.0359, -0.0095],\n",
      "        [ 0.0057, -0.0846,  0.0573,  ..., -0.0995,  0.0775,  0.1348],\n",
      "        ...,\n",
      "        [ 0.0561, -0.0423, -0.0047,  ...,  0.0799,  0.0327,  0.1121],\n",
      "        [ 0.0439, -0.1047,  0.0781,  ..., -0.0373,  0.0304, -0.0388],\n",
      "        [ 0.1122, -0.0937,  0.0110,  ..., -0.0770,  0.0039, -0.0645]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.2138,  0.1574, -0.2373,  ...,  0.1324, -0.2639, -0.6049],\n",
      "        [-0.2829,  0.1122,  0.3646,  ...,  0.2595,  0.1435, -0.0381],\n",
      "        [ 0.0226, -0.3384,  0.2291,  ..., -0.3981,  0.3100,  0.5393],\n",
      "        ...,\n",
      "        [ 0.2245, -0.1691, -0.0189,  ...,  0.3196,  0.1307,  0.4484],\n",
      "        [ 0.1756, -0.4190,  0.3125,  ..., -0.1493,  0.1214, -0.1554],\n",
      "        [ 0.4488, -0.3749,  0.0442,  ..., -0.3079,  0.0156, -0.2579]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer3.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0311,  0.0045, -0.0389,  0.0644, -0.0296, -0.1006, -0.0116, -0.0130,\n",
      "         0.1361, -0.0008,  0.0166, -0.0535,  0.1439, -0.0524,  0.1105, -0.0671,\n",
      "        -0.0602,  0.0411,  0.1518, -0.0128,  0.1021,  0.0380, -0.0618, -0.0268,\n",
      "         0.1222,  0.0266, -0.0341,  0.0175, -0.0549,  0.0562, -0.0313, -0.0062,\n",
      "         0.0635,  0.1021,  0.1544,  0.0332, -0.1124,  0.0177, -0.0699, -0.0127,\n",
      "        -0.0395,  0.0166, -0.0346,  0.0803,  0.0936,  0.0017,  0.0595, -0.0253,\n",
      "         0.0528,  0.1045, -0.1606,  0.0272,  0.0040, -0.0977, -0.0350, -0.0354,\n",
      "         0.0981, -0.0085, -0.0996, -0.0105], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.1243,  0.0182, -0.1558,  0.2575, -0.1185, -0.4024, -0.0465, -0.0521,\n",
      "         0.5445, -0.0032,  0.0662, -0.2139,  0.5755, -0.2096,  0.4419, -0.2682,\n",
      "        -0.2410,  0.1645,  0.6072, -0.0513,  0.4084,  0.1520, -0.2472, -0.1071,\n",
      "         0.4886,  0.1063, -0.1363,  0.0700, -0.2195,  0.2248, -0.1250, -0.0247,\n",
      "         0.2540,  0.4083,  0.6175,  0.1328, -0.4495,  0.0709, -0.2794, -0.0508,\n",
      "        -0.1581,  0.0663, -0.1384,  0.3213,  0.3742,  0.0067,  0.2382, -0.1013,\n",
      "         0.2114,  0.4180, -0.6422,  0.1089,  0.0159, -0.3908, -0.1400, -0.1415,\n",
      "         0.3922, -0.0339, -0.3983, -0.0422], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name output.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.1876,  0.1211, -0.1839, -0.2392, -0.1537,  0.0930,  0.1808, -0.2183,\n",
      "         -0.2477, -0.2111,  0.2171,  0.1873,  0.1251,  0.1397,  0.1093, -0.2261,\n",
      "         -0.2495, -0.2636, -0.2681,  0.1593, -0.0734, -0.0200, -0.1795, -0.1791,\n",
      "         -0.0630,  0.2294, -0.1460, -0.0683,  0.1325,  0.2676, -0.1196,  0.1995,\n",
      "          0.2875,  0.1687,  0.2431, -0.4945,  0.0042,  0.1023, -0.2416,  0.0180,\n",
      "         -0.1167, -0.2004,  0.1393,  0.2139, -0.2647, -0.1645, -0.1790,  0.0096,\n",
      "         -0.0730, -0.1452,  0.1573, -0.1535,  0.1173,  0.1330,  0.0780,  0.2460,\n",
      "          0.3555, -0.1045, -0.1032, -0.0840]], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.7504,  0.4845, -0.7358, -0.9569, -0.6148,  0.3719,  0.7232, -0.8732,\n",
      "         -0.9908, -0.8445,  0.8682,  0.7491,  0.5004,  0.5587,  0.4372, -0.9043,\n",
      "         -0.9979, -1.0542, -1.0723,  0.6371, -0.2937, -0.0801, -0.7181, -0.7164,\n",
      "         -0.2519,  0.9175, -0.5839, -0.2734,  0.5301,  1.0703, -0.4784,  0.7978,\n",
      "          1.1499,  0.6748,  0.9723, -1.9778,  0.0166,  0.4091, -0.9665,  0.0721,\n",
      "         -0.4668, -0.8014,  0.5574,  0.8556, -1.0587, -0.6582, -0.7159,  0.0384,\n",
      "         -0.2919, -0.5807,  0.6291, -0.6138,  0.4693,  0.5319,  0.3122,  0.9840,\n",
      "          1.4222, -0.4179, -0.4128, -0.3359]], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')\n",
      "name output.bias \n",
      "\n",
      "\n",
      " model tensor([0.2638], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([1.0551], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500], device='cuda:0')\n",
      "name layer1.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.2594, -0.3976,  0.1014,  ...,  0.1605,  0.3764,  0.2278],\n",
      "        [-0.0603, -0.2498,  0.4555,  ...,  0.0047,  0.0211, -0.5338],\n",
      "        [ 0.0142, -0.2424,  0.5220,  ...,  0.2355,  0.4628, -0.5232],\n",
      "        ...,\n",
      "        [-0.0353,  0.0278, -0.3114,  ..., -0.2814,  0.3428,  0.1711],\n",
      "        [-0.4060, -0.4858, -0.3467,  ...,  0.2642,  0.0740,  0.0224],\n",
      "        [ 0.3685,  0.1776,  0.1710,  ..., -0.0011, -0.2734, -0.0445]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 1.0376, -1.5904,  0.4056,  ...,  0.6420,  1.5056,  0.9111],\n",
      "        [-0.2412, -0.9991,  1.8220,  ...,  0.0190,  0.0842, -2.1352],\n",
      "        [ 0.0569, -0.9697,  2.0882,  ...,  0.9422,  1.8511, -2.0929],\n",
      "        ...,\n",
      "        [-0.1413,  0.1111, -1.2456,  ..., -1.1256,  1.3714,  0.6843],\n",
      "        [-1.6240, -1.9431, -1.3870,  ...,  1.0569,  0.2961,  0.0896],\n",
      "        [ 1.4741,  0.7104,  0.6841,  ..., -0.0044, -1.0934, -0.1782]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer1.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0533, -0.2976,  0.1683, -0.2796,  0.4783, -0.1859,  0.1970,  0.2235,\n",
      "         0.4476, -0.4746,  0.0582,  0.2373, -0.0943, -0.0593, -0.0383,  0.4056,\n",
      "         0.1094,  0.3961,  0.2945, -0.4114,  0.2467,  0.1748, -0.2457,  0.0433,\n",
      "        -0.2843, -0.4822, -0.0307,  0.0962,  0.3481,  0.2948, -0.1386, -0.2260,\n",
      "        -0.1408, -0.2667,  0.3383, -0.2663,  0.2134, -0.2520, -0.3442, -0.5159,\n",
      "        -0.3523,  0.0703,  0.1675, -0.4977,  0.0274, -0.5294, -0.4071, -0.2197,\n",
      "         0.1088,  0.3603, -0.0918,  0.0754,  0.4334,  0.3057,  0.3766, -0.1408,\n",
      "        -0.3021,  0.3022, -0.2236,  0.3569,  0.4050, -0.2624,  0.4538,  0.1725,\n",
      "         0.0586,  0.2513, -0.1856,  0.3054, -0.2262, -0.1141, -0.0513,  0.2058,\n",
      "        -0.3756,  0.2403, -0.2751, -0.3446,  0.2654, -0.0293, -0.1686,  0.3292,\n",
      "         0.4432, -0.4684, -0.2763, -0.3647,  0.4375, -0.1154,  0.1397,  0.4050,\n",
      "         0.3931, -0.1647, -0.1753, -0.3663, -0.4981, -0.1992,  0.2200,  0.1146,\n",
      "        -0.3251,  0.0533,  0.0160,  0.2042, -0.2496,  0.1141, -0.4078, -0.1424,\n",
      "         0.1085, -0.4270, -0.3692, -0.2484, -0.4747, -0.1498, -0.5340, -0.0314,\n",
      "        -0.1802, -0.0993, -0.4582,  0.2455, -0.2263, -0.0550, -0.1619, -0.1929,\n",
      "         0.0914,  0.1237, -0.0710,  0.4401, -0.2588,  0.4174,  0.3714,  0.2574,\n",
      "        -0.3506, -0.4329,  0.1630,  0.4667, -0.0287,  0.3322, -0.1821, -0.3801,\n",
      "        -0.2611,  0.4145, -0.0007, -0.3786, -0.3146, -0.0159,  0.0102,  0.2508,\n",
      "        -0.3237, -0.3670,  0.1127, -0.5219, -0.2609,  0.3361,  0.2909,  0.2630,\n",
      "         0.3016, -0.3917, -0.4872, -0.4266,  0.2443, -0.3212, -0.0918, -0.0804,\n",
      "        -0.2903, -0.5236, -0.1680,  0.3135,  0.0755, -0.4906,  0.2998, -0.4246,\n",
      "         0.0407, -0.1521,  0.2767, -0.3670, -0.3565,  0.2147,  0.2308,  0.3634,\n",
      "         0.0052,  0.0903,  0.3477,  0.1473,  0.0694, -0.0430, -0.2624, -0.0924,\n",
      "         0.3793,  0.0731, -0.5296,  0.2131,  0.3267,  0.2794, -0.3457,  0.3043,\n",
      "         0.0422,  0.0724, -0.3112,  0.0859, -0.2733, -0.2427,  0.1653, -0.1537,\n",
      "         0.0476, -0.2495, -0.4828, -0.1244, -0.1088,  0.0046, -0.0012,  0.4617,\n",
      "         0.2375, -0.3458,  0.1529, -0.3748, -0.0276, -0.2087,  0.1731, -0.3386,\n",
      "         0.1302, -0.1685,  0.3728,  0.0889, -0.3937,  0.2867,  0.3718, -0.3299,\n",
      "        -0.3796, -0.4960,  0.0468, -0.4306, -0.3099, -0.3600,  0.1805,  0.4462,\n",
      "         0.0164, -0.4883, -0.0203,  0.3129, -0.5222,  0.0441,  0.0137,  0.0102,\n",
      "        -0.1568, -0.5443, -0.0731, -0.1525,  0.2775, -0.1180,  0.4777, -0.3341,\n",
      "        -0.3536,  0.2311, -0.1433, -0.3046,  0.1661,  0.2469, -0.3509,  0.3217,\n",
      "        -0.2336,  0.4728,  0.3464, -0.2160, -0.3805, -0.4916,  0.0239,  0.0339,\n",
      "         0.3407,  0.2819, -0.0255, -0.4977,  0.3217,  0.2836,  0.3084,  0.3222,\n",
      "        -0.0258,  0.2586,  0.0075, -0.4786, -0.2350,  0.3078,  0.0185,  0.1093,\n",
      "         0.0401, -0.0789,  0.1429, -0.3439,  0.2971, -0.4580, -0.0578, -0.5280,\n",
      "         0.2342,  0.3148, -0.4135, -0.2279, -0.4449, -0.3578,  0.3702,  0.1336,\n",
      "        -0.5069, -0.3149, -0.5344,  0.4053, -0.1074,  0.3561, -0.1516,  0.0733,\n",
      "         0.2813,  0.2799,  0.2204,  0.2000, -0.3559, -0.1788, -0.1005,  0.1587,\n",
      "        -0.3056,  0.0480, -0.1572,  0.0988, -0.3203,  0.3752,  0.1441, -0.1858,\n",
      "         0.0496, -0.2569, -0.0618,  0.0264, -0.4264, -0.1943, -0.1492,  0.3311,\n",
      "         0.1900,  0.4722,  0.1155,  0.3057, -0.3209,  0.4003, -0.0986,  0.1345,\n",
      "         0.3535, -0.1337, -0.0899,  0.3471,  0.4302, -0.3008, -0.0366, -0.2108,\n",
      "        -0.1828,  0.4011, -0.5035,  0.2733,  0.0909, -0.0391,  0.3933, -0.0676,\n",
      "        -0.3030, -0.1426, -0.0465,  0.0018, -0.0830,  0.0181,  0.0739, -0.1898,\n",
      "        -0.3978,  0.1440,  0.2752, -0.1834,  0.1594,  0.0292, -0.3882, -0.3835,\n",
      "         0.3140,  0.1215, -0.2803, -0.2791, -0.1879,  0.4263,  0.2653, -0.4888,\n",
      "        -0.3841, -0.5022,  0.2064, -0.1732,  0.2672, -0.4353,  0.1857, -0.4265,\n",
      "        -0.2006,  0.2788, -0.3028,  0.4202,  0.2692,  0.4319,  0.3084, -0.4182,\n",
      "         0.2253, -0.3089,  0.0870,  0.0059, -0.3054, -0.0176, -0.4108, -0.0083,\n",
      "        -0.3409,  0.4043, -0.1234,  0.1365,  0.1504, -0.0527,  0.4243,  0.0080,\n",
      "         0.1225,  0.3391,  0.1453, -0.0215, -0.3462,  0.1187, -0.4027,  0.0875,\n",
      "         0.4066, -0.0149,  0.3713, -0.1068,  0.1035,  0.3800,  0.2460,  0.3619,\n",
      "        -0.4111, -0.1826,  0.2778, -0.3398,  0.3421,  0.4399, -0.3789,  0.3259,\n",
      "        -0.1909, -0.2622,  0.4355,  0.1426,  0.1120,  0.3445,  0.0265, -0.0767,\n",
      "        -0.1538, -0.4370,  0.3049,  0.2692, -0.0558, -0.0103, -0.2042,  0.3164,\n",
      "         0.3008, -0.0461,  0.1757,  0.2354, -0.4257, -0.3459,  0.0008,  0.0081,\n",
      "         0.3492,  0.2701, -0.0588,  0.3723,  0.2743,  0.3463, -0.2644, -0.2942,\n",
      "        -0.3763, -0.1899, -0.1459, -0.5280,  0.3025,  0.0344, -0.4106,  0.2667,\n",
      "        -0.0136, -0.0609, -0.2359,  0.0552,  0.2563,  0.1276,  0.0555, -0.3754,\n",
      "        -0.2732,  0.1979, -0.4710, -0.2199, -0.3255, -0.0013, -0.0319, -0.4547,\n",
      "         0.1165, -0.0975,  0.1514,  0.0644, -0.2438,  0.0285, -0.0318,  0.1264,\n",
      "        -0.4959, -0.1553, -0.0676, -0.1071, -0.2301, -0.4652, -0.3956,  0.1989,\n",
      "         0.3353,  0.3988, -0.2981, -0.4626,  0.4522, -0.2912,  0.1587,  0.1767],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.2131, -1.1906,  0.6731, -1.1182,  1.9130, -0.7437,  0.7880,  0.8939,\n",
      "         1.7904, -1.8985,  0.2328,  0.9491, -0.3773, -0.2372, -0.1531,  1.6225,\n",
      "         0.4378,  1.5845,  1.1779, -1.6454,  0.9866,  0.6994, -0.9830,  0.1732,\n",
      "        -1.1372, -1.9290, -0.1227,  0.3849,  1.3923,  1.1791, -0.5545, -0.9040,\n",
      "        -0.5633, -1.0668,  1.3533, -1.0651,  0.8536, -1.0079, -1.3769, -2.0637,\n",
      "        -1.4092,  0.2810,  0.6701, -1.9910,  0.1098, -2.1177, -1.6282, -0.8786,\n",
      "         0.4351,  1.4413, -0.3673,  0.3018,  1.7335,  1.2228,  1.5062, -0.5631,\n",
      "        -1.2085,  1.2088, -0.8945,  1.4276,  1.6199, -1.0497,  1.8152,  0.6902,\n",
      "         0.2343,  1.0054, -0.7423,  1.2216, -0.9049, -0.4566, -0.2051,  0.8231,\n",
      "        -1.5025,  0.9612, -1.1005, -1.3785,  1.0616, -0.1173, -0.6746,  1.3166,\n",
      "         1.7728, -1.8736, -1.1052, -1.4588,  1.7499, -0.4615,  0.5589,  1.6201,\n",
      "         1.5723, -0.6588, -0.7014, -1.4652, -1.9924, -0.7969,  0.8800,  0.4585,\n",
      "        -1.3004,  0.2133,  0.0640,  0.8167, -0.9985,  0.4563, -1.6313, -0.5697,\n",
      "         0.4339, -1.7081, -1.4767, -0.9935, -1.8990, -0.5992, -2.1360, -0.1256,\n",
      "        -0.7208, -0.3974, -1.8328,  0.9821, -0.9051, -0.2199, -0.6476, -0.7714,\n",
      "         0.3655,  0.4949, -0.2839,  1.7602, -1.0352,  1.6697,  1.4857,  1.0295,\n",
      "        -1.4025, -1.7317,  0.6519,  1.8667, -0.1150,  1.3287, -0.7285, -1.5205,\n",
      "        -1.0445,  1.6579, -0.0028, -1.5144, -1.2584, -0.0636,  0.0409,  1.0034,\n",
      "        -1.2948, -1.4682,  0.4508, -2.0877, -1.0438,  1.3445,  1.1635,  1.0521,\n",
      "         1.2066, -1.5669, -1.9486, -1.7063,  0.9772, -1.2846, -0.3671, -0.3215,\n",
      "        -1.1613, -2.0944, -0.6722,  1.2541,  0.3020, -1.9626,  1.1994, -1.6985,\n",
      "         0.1627, -0.6083,  1.1068, -1.4679, -1.4260,  0.8589,  0.9231,  1.4537,\n",
      "         0.0207,  0.3611,  1.3906,  0.5891,  0.2776, -0.1719, -1.0498, -0.3695,\n",
      "         1.5173,  0.2924, -2.1183,  0.8526,  1.3069,  1.1175, -1.3828,  1.2173,\n",
      "         0.1689,  0.2897, -1.2448,  0.3436, -1.0933, -0.9708,  0.6612, -0.6149,\n",
      "         0.1904, -0.9979, -1.9311, -0.4977, -0.4352,  0.0183, -0.0047,  1.8466,\n",
      "         0.9499, -1.3830,  0.6117, -1.4993, -0.1103, -0.8349,  0.6923, -1.3546,\n",
      "         0.5210, -0.6739,  1.4912,  0.3554, -1.5749,  1.1469,  1.4872, -1.3197,\n",
      "        -1.5184, -1.9840,  0.1870, -1.7225, -1.2398, -1.4401,  0.7221,  1.7849,\n",
      "         0.0654, -1.9533, -0.0813,  1.2516, -2.0887,  0.1764,  0.0549,  0.0409,\n",
      "        -0.6273, -2.1770, -0.2924, -0.6098,  1.1099, -0.4720,  1.9108, -1.3365,\n",
      "        -1.4145,  0.9243, -0.5732, -1.2183,  0.6646,  0.9877, -1.4038,  1.2869,\n",
      "        -0.9345,  1.8911,  1.3854, -0.8641, -1.5221, -1.9665,  0.0955,  0.1354,\n",
      "         1.3629,  1.1275, -0.1020, -1.9906,  1.2867,  1.1343,  1.2337,  1.2887,\n",
      "        -0.1033,  1.0344,  0.0301, -1.9145, -0.9401,  1.2311,  0.0739,  0.4373,\n",
      "         0.1606, -0.3156,  0.5717, -1.3757,  1.1886, -1.8321, -0.2312, -2.1120,\n",
      "         0.9370,  1.2593, -1.6539, -0.9116, -1.7795, -1.4310,  1.4807,  0.5343,\n",
      "        -2.0275, -1.2595, -2.1376,  1.6212, -0.4295,  1.4244, -0.6065,  0.2934,\n",
      "         1.1253,  1.1195,  0.8815,  0.7998, -1.4238, -0.7153, -0.4020,  0.6349,\n",
      "        -1.2225,  0.1919, -0.6289,  0.3952, -1.2812,  1.5006,  0.5765, -0.7431,\n",
      "         0.1983, -1.0275, -0.2470,  0.1057, -1.7058, -0.7773, -0.5967,  1.3246,\n",
      "         0.7600,  1.8889,  0.4619,  1.2226, -1.2838,  1.6011, -0.3944,  0.5381,\n",
      "         1.4140, -0.5347, -0.3597,  1.3885,  1.7208, -1.2032, -0.1463, -0.8433,\n",
      "        -0.7312,  1.6046, -2.0138,  1.0933,  0.3635, -0.1562,  1.5733, -0.2705,\n",
      "        -1.2121, -0.5703, -0.1859,  0.0073, -0.3320,  0.0725,  0.2955, -0.7593,\n",
      "        -1.5914,  0.5759,  1.1008, -0.7336,  0.6374,  0.1168, -1.5529, -1.5339,\n",
      "         1.2558,  0.4860, -1.1212, -1.1166, -0.7516,  1.7053,  1.0611, -1.9554,\n",
      "        -1.5363, -2.0087,  0.8257, -0.6929,  1.0686, -1.7412,  0.7426, -1.7062,\n",
      "        -0.8026,  1.1151, -1.2112,  1.6808,  1.0769,  1.7275,  1.2334, -1.6729,\n",
      "         0.9011, -1.2357,  0.3480,  0.0237, -1.2215, -0.0702, -1.6433, -0.0330,\n",
      "        -1.3638,  1.6171, -0.4937,  0.5458,  0.6017, -0.2107,  1.6971,  0.0319,\n",
      "         0.4902,  1.3563,  0.5812, -0.0860, -1.3850,  0.4749, -1.6107,  0.3498,\n",
      "         1.6265, -0.0594,  1.4850, -0.4271,  0.4141,  1.5199,  0.9841,  1.4477,\n",
      "        -1.6446, -0.7305,  1.1113, -1.3593,  1.3684,  1.7597, -1.5155,  1.3034,\n",
      "        -0.7636, -1.0487,  1.7421,  0.5703,  0.4478,  1.3780,  0.1061, -0.3068,\n",
      "        -0.6152, -1.7480,  1.2195,  1.0768, -0.2231, -0.0413, -0.8166,  1.2656,\n",
      "         1.2032, -0.1842,  0.7027,  0.9416, -1.7029, -1.3836,  0.0033,  0.0324,\n",
      "         1.3969,  1.0806, -0.2351,  1.4891,  1.0973,  1.3851, -1.0574, -1.1769,\n",
      "        -1.5053, -0.7595, -0.5838, -2.1122,  1.2101,  0.1377, -1.6425,  1.0670,\n",
      "        -0.0542, -0.2436, -0.9437,  0.2207,  1.0251,  0.5105,  0.2218, -1.5015,\n",
      "        -1.0929,  0.7918, -1.8841, -0.8797, -1.3020, -0.0050, -0.1278, -1.8188,\n",
      "         0.4661, -0.3899,  0.6054,  0.2576, -0.9753,  0.1138, -0.1273,  0.5054,\n",
      "        -1.9837, -0.6214, -0.2703, -0.4285, -0.9202, -1.8610, -1.5826,  0.7955,\n",
      "         1.3411,  1.5950, -1.1925, -1.8503,  1.8087, -1.1648,  0.6348,  0.7067],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
      "       device='cuda:0')\n",
      "name layer2.weight \n",
      "\n",
      "\n",
      " model tensor([[-0.0634, -0.0549,  0.0018,  ...,  0.0458, -0.0822, -0.0203],\n",
      "        [ 0.0536,  0.0359,  0.0015,  ..., -0.0115, -0.0132,  0.0060],\n",
      "        [-0.0662, -0.0451,  0.0247,  ...,  0.0570,  0.0432, -0.0235],\n",
      "        ...,\n",
      "        [ 0.0251, -0.0267, -0.0191,  ..., -0.0429, -0.0456,  0.0241],\n",
      "        [ 0.1043, -0.0145,  0.0097,  ..., -0.0637, -0.0198,  0.0129],\n",
      "        [ 0.0114,  0.0589,  0.0012,  ...,  0.0182,  0.0218, -0.0614]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[-0.2534, -0.2197,  0.0070,  ...,  0.1831, -0.3286, -0.0811],\n",
      "        [ 0.2145,  0.1437,  0.0058,  ..., -0.0459, -0.0527,  0.0238],\n",
      "        [-0.2646, -0.1803,  0.0989,  ...,  0.2280,  0.1728, -0.0941],\n",
      "        ...,\n",
      "        [ 0.1004, -0.1066, -0.0764,  ..., -0.1718, -0.1823,  0.0963],\n",
      "        [ 0.4172, -0.0581,  0.0387,  ..., -0.2550, -0.0793,  0.0516],\n",
      "        [ 0.0455,  0.2356,  0.0048,  ...,  0.0728,  0.0871, -0.2454]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer2.bias \n",
      "\n",
      "\n",
      " model tensor([ 6.6205e-03, -3.2377e-02, -4.5958e-02,  6.1730e-02,  5.1142e-02,\n",
      "        -1.7462e-02, -5.4124e-02, -1.1232e-02,  5.8617e-02, -6.6676e-02,\n",
      "         3.8840e-02, -4.0419e-02,  9.2222e-03, -6.6494e-02, -4.6496e-02,\n",
      "        -1.2460e-02, -6.5301e-04, -6.5983e-02,  2.2284e-02, -2.4886e-02,\n",
      "         4.2531e-02,  8.4915e-02,  5.9802e-02, -3.4652e-03, -2.8837e-02,\n",
      "         6.0173e-02, -6.3772e-02, -8.8647e-02, -3.2353e-02,  1.2800e-01,\n",
      "         7.2008e-02,  8.0851e-02, -4.0117e-02,  2.5901e-02,  3.5606e-02,\n",
      "        -7.7808e-02,  3.5788e-03, -2.1756e-02, -1.2306e-01,  4.8123e-02,\n",
      "         1.1054e-02, -7.2002e-02,  7.8845e-02,  7.1751e-03,  3.9806e-02,\n",
      "         2.8828e-02, -3.6909e-02,  5.8479e-02, -7.0202e-02,  5.6132e-02,\n",
      "        -3.1911e-02, -1.9451e-02,  1.2588e-02, -4.9605e-02, -2.6415e-02,\n",
      "         5.0884e-02,  4.3441e-02, -1.1917e-02, -3.6576e-02,  5.1420e-02,\n",
      "        -8.2422e-02,  7.8674e-02, -3.3395e-02, -1.1349e-02,  3.8636e-03,\n",
      "        -3.9502e-03,  4.0672e-02, -2.6127e-02,  2.6751e-02, -2.2839e-02,\n",
      "        -3.9352e-02,  2.5426e-02,  3.9454e-02, -4.3014e-02, -6.4135e-02,\n",
      "         2.4835e-02, -1.9392e-02,  4.7930e-02,  5.5934e-02,  2.8320e-02,\n",
      "        -8.6030e-02,  1.4140e-02,  2.4989e-02,  4.3506e-02, -4.9501e-02,\n",
      "         3.8060e-02, -2.3130e-02, -5.5456e-02,  7.4137e-02,  6.9622e-02,\n",
      "        -5.7255e-02, -1.2605e-02, -3.3073e-02, -5.0543e-02, -2.6859e-02,\n",
      "         2.7225e-02, -3.9992e-02, -1.5378e-01,  4.2800e-02, -1.3810e-04,\n",
      "         4.8996e-02, -7.1443e-02, -1.6780e-02, -1.1945e-02, -7.4407e-02,\n",
      "        -3.6141e-02, -1.2170e-02, -5.0433e-02, -7.2944e-02, -1.8268e-02,\n",
      "        -8.9378e-02,  7.2016e-03,  5.4560e-02, -5.7141e-02,  5.8410e-03,\n",
      "         7.8669e-02,  4.9474e-02, -2.1693e-02, -8.4660e-02,  5.1386e-02,\n",
      "         8.8519e-03,  4.5689e-02, -2.3721e-02,  4.9863e-02,  3.7764e-03,\n",
      "         4.1239e-02,  1.1756e-01,  2.4989e-02, -1.7260e-02, -7.1427e-02,\n",
      "        -7.9698e-02, -7.5401e-02,  7.6057e-02,  4.5944e-02,  1.5109e-02,\n",
      "        -4.0553e-02,  5.7381e-02,  2.7081e-02, -1.4482e-02, -7.1393e-02,\n",
      "        -1.1495e-02, -2.8827e-02, -5.9305e-02, -9.3016e-02, -8.8831e-02,\n",
      "         5.9434e-02, -4.1125e-03, -9.1109e-02, -9.7782e-02,  6.5756e-02,\n",
      "        -6.5883e-02, -6.4734e-02,  2.9286e-05, -7.8842e-02,  2.7385e-02,\n",
      "        -3.7701e-02,  3.7715e-02,  9.7803e-02, -5.0331e-02, -6.7174e-02,\n",
      "        -1.5403e-02, -1.3481e-02,  6.7517e-02,  6.1399e-02, -8.1103e-02,\n",
      "        -1.1418e-01,  7.5766e-02,  4.6393e-03,  6.1258e-02, -1.7248e-02,\n",
      "         6.2396e-02, -3.1449e-02,  2.7584e-02,  3.5604e-02, -7.7220e-02,\n",
      "        -6.0437e-02, -3.1787e-02,  4.3068e-02,  6.6608e-02,  5.1794e-02,\n",
      "         6.3146e-02, -8.5386e-02, -7.8354e-02,  4.2019e-02, -7.6459e-02,\n",
      "         6.8323e-02,  9.4791e-02, -6.3084e-02, -1.5255e-03,  1.0377e-02,\n",
      "         6.8170e-02, -5.7751e-02,  1.0723e-02,  7.7447e-02, -9.2818e-03,\n",
      "        -9.2200e-02, -5.3574e-02, -5.7750e-02, -1.9773e-02, -8.3680e-02,\n",
      "        -6.3854e-03, -3.3826e-02, -5.6497e-02, -2.6097e-02, -5.3209e-02,\n",
      "         1.5681e-03, -1.8333e-02,  5.8289e-02, -8.5990e-02,  6.7770e-02,\n",
      "        -1.0025e-01,  2.6034e-02,  3.0845e-02, -4.5089e-02, -2.0955e-03,\n",
      "         1.6112e-02,  5.3509e-02, -3.8658e-03,  9.3778e-02, -6.5697e-02,\n",
      "        -2.8214e-02,  3.9072e-02, -2.7483e-02,  7.7915e-02, -1.8103e-02,\n",
      "        -2.3917e-02, -5.3242e-03,  6.8477e-02,  7.3193e-02,  4.7106e-02,\n",
      "        -6.8053e-02,  5.2855e-02,  7.4291e-02,  2.1017e-02, -6.6047e-02,\n",
      "        -1.9512e-02,  5.4492e-02,  1.8015e-02,  4.1885e-02,  1.7024e-02,\n",
      "        -8.1572e-02,  9.6217e-03, -1.0126e-01, -5.6150e-02, -8.1864e-02,\n",
      "        -5.3142e-02,  4.3732e-02, -4.9532e-03, -5.4868e-02, -8.3943e-02,\n",
      "         3.6248e-02, -2.0443e-02,  4.5764e-02,  2.3292e-02,  9.1441e-02,\n",
      "        -4.6304e-02], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([ 2.6482e-02, -1.2951e-01, -1.8383e-01,  2.4692e-01,  2.0457e-01,\n",
      "        -6.9849e-02, -2.1649e-01, -4.4930e-02,  2.3447e-01, -2.6670e-01,\n",
      "         1.5536e-01, -1.6168e-01,  3.6889e-02, -2.6598e-01, -1.8598e-01,\n",
      "        -4.9841e-02, -2.6121e-03, -2.6393e-01,  8.9138e-02, -9.9545e-02,\n",
      "         1.7012e-01,  3.3966e-01,  2.3921e-01, -1.3861e-02, -1.1535e-01,\n",
      "         2.4069e-01, -2.5509e-01, -3.5459e-01, -1.2941e-01,  5.1200e-01,\n",
      "         2.8803e-01,  3.2340e-01, -1.6047e-01,  1.0360e-01,  1.4243e-01,\n",
      "        -3.1123e-01,  1.4315e-02, -8.7023e-02, -4.9225e-01,  1.9249e-01,\n",
      "         4.4216e-02, -2.8801e-01,  3.1538e-01,  2.8700e-02,  1.5922e-01,\n",
      "         1.1531e-01, -1.4764e-01,  2.3392e-01, -2.8081e-01,  2.2453e-01,\n",
      "        -1.2764e-01, -7.7804e-02,  5.0351e-02, -1.9842e-01, -1.0566e-01,\n",
      "         2.0354e-01,  1.7377e-01, -4.7669e-02, -1.4630e-01,  2.0568e-01,\n",
      "        -3.2969e-01,  3.1470e-01, -1.3358e-01, -4.5395e-02,  1.5454e-02,\n",
      "        -1.5801e-02,  1.6269e-01, -1.0451e-01,  1.0700e-01, -9.1358e-02,\n",
      "        -1.5741e-01,  1.0170e-01,  1.5782e-01, -1.7206e-01, -2.5654e-01,\n",
      "         9.9338e-02, -7.7569e-02,  1.9172e-01,  2.2374e-01,  1.1328e-01,\n",
      "        -3.4412e-01,  5.6559e-02,  9.9956e-02,  1.7403e-01, -1.9800e-01,\n",
      "         1.5224e-01, -9.2521e-02, -2.2183e-01,  2.9655e-01,  2.7849e-01,\n",
      "        -2.2902e-01, -5.0421e-02, -1.3229e-01, -2.0217e-01, -1.0744e-01,\n",
      "         1.0890e-01, -1.5997e-01, -6.1513e-01,  1.7120e-01, -5.5240e-04,\n",
      "         1.9598e-01, -2.8577e-01, -6.7118e-02, -4.7782e-02, -2.9763e-01,\n",
      "        -1.4456e-01, -4.8680e-02, -2.0173e-01, -2.9177e-01, -7.3074e-02,\n",
      "        -3.5751e-01,  2.8807e-02,  2.1824e-01, -2.2856e-01,  2.3364e-02,\n",
      "         3.1468e-01,  1.9790e-01, -8.6772e-02, -3.3864e-01,  2.0554e-01,\n",
      "         3.5408e-02,  1.8276e-01, -9.4883e-02,  1.9945e-01,  1.5106e-02,\n",
      "         1.6495e-01,  4.7023e-01,  9.9957e-02, -6.9039e-02, -2.8571e-01,\n",
      "        -3.1879e-01, -3.0160e-01,  3.0423e-01,  1.8378e-01,  6.0437e-02,\n",
      "        -1.6221e-01,  2.2952e-01,  1.0832e-01, -5.7927e-02, -2.8557e-01,\n",
      "        -4.5980e-02, -1.1531e-01, -2.3722e-01, -3.7206e-01, -3.5532e-01,\n",
      "         2.3774e-01, -1.6450e-02, -3.6444e-01, -3.9113e-01,  2.6302e-01,\n",
      "        -2.6353e-01, -2.5894e-01,  1.1714e-04, -3.1537e-01,  1.0954e-01,\n",
      "        -1.5080e-01,  1.5086e-01,  3.9121e-01, -2.0133e-01, -2.6870e-01,\n",
      "        -6.1614e-02, -5.3923e-02,  2.7007e-01,  2.4559e-01, -3.2441e-01,\n",
      "        -4.5673e-01,  3.0306e-01,  1.8557e-02,  2.4503e-01, -6.8993e-02,\n",
      "         2.4959e-01, -1.2579e-01,  1.1033e-01,  1.4242e-01, -3.0888e-01,\n",
      "        -2.4175e-01, -1.2715e-01,  1.7227e-01,  2.6643e-01,  2.0718e-01,\n",
      "         2.5258e-01, -3.4154e-01, -3.1342e-01,  1.6808e-01, -3.0583e-01,\n",
      "         2.7329e-01,  3.7916e-01, -2.5233e-01, -6.1019e-03,  4.1509e-02,\n",
      "         2.7268e-01, -2.3100e-01,  4.2892e-02,  3.0979e-01, -3.7127e-02,\n",
      "        -3.6880e-01, -2.1429e-01, -2.3100e-01, -7.9094e-02, -3.3472e-01,\n",
      "        -2.5542e-02, -1.3530e-01, -2.2599e-01, -1.0439e-01, -2.1284e-01,\n",
      "         6.2723e-03, -7.3330e-02,  2.3316e-01, -3.4396e-01,  2.7108e-01,\n",
      "        -4.0098e-01,  1.0413e-01,  1.2338e-01, -1.8036e-01, -8.3819e-03,\n",
      "         6.4446e-02,  2.1404e-01, -1.5463e-02,  3.7511e-01, -2.6279e-01,\n",
      "        -1.1286e-01,  1.5629e-01, -1.0993e-01,  3.1166e-01, -7.2410e-02,\n",
      "        -9.5668e-02, -2.1297e-02,  2.7391e-01,  2.9277e-01,  1.8842e-01,\n",
      "        -2.7221e-01,  2.1142e-01,  2.9716e-01,  8.4069e-02, -2.6419e-01,\n",
      "        -7.8046e-02,  2.1797e-01,  7.2058e-02,  1.6754e-01,  6.8098e-02,\n",
      "        -3.2629e-01,  3.8487e-02, -4.0502e-01, -2.2460e-01, -3.2746e-01,\n",
      "        -2.1257e-01,  1.7493e-01, -1.9813e-02, -2.1947e-01, -3.3577e-01,\n",
      "         1.4499e-01, -8.1772e-02,  1.8306e-01,  9.3166e-02,  3.6577e-01,\n",
      "        -1.8522e-01], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name layer3.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.0534,  0.0394, -0.0593,  ...,  0.0331, -0.0660, -0.1512],\n",
      "        [-0.0707,  0.0280,  0.0912,  ...,  0.0649,  0.0359, -0.0095],\n",
      "        [ 0.0057, -0.0846,  0.0573,  ..., -0.0995,  0.0775,  0.1348],\n",
      "        ...,\n",
      "        [ 0.0561, -0.0423, -0.0047,  ...,  0.0799,  0.0327,  0.1121],\n",
      "        [ 0.0439, -0.1047,  0.0781,  ..., -0.0373,  0.0304, -0.0388],\n",
      "        [ 0.1122, -0.0937,  0.0110,  ..., -0.0770,  0.0039, -0.0645]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.2138,  0.1574, -0.2373,  ...,  0.1324, -0.2639, -0.6049],\n",
      "        [-0.2829,  0.1122,  0.3646,  ...,  0.2595,  0.1435, -0.0381],\n",
      "        [ 0.0226, -0.3384,  0.2291,  ..., -0.3981,  0.3100,  0.5393],\n",
      "        ...,\n",
      "        [ 0.2245, -0.1691, -0.0189,  ...,  0.3196,  0.1307,  0.4484],\n",
      "        [ 0.1756, -0.4190,  0.3125,  ..., -0.1493,  0.1214, -0.1554],\n",
      "        [ 0.4488, -0.3749,  0.0442,  ..., -0.3079,  0.0156, -0.2579]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer3.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0311,  0.0045, -0.0389,  0.0644, -0.0296, -0.1006, -0.0116, -0.0130,\n",
      "         0.1361, -0.0008,  0.0166, -0.0535,  0.1439, -0.0524,  0.1105, -0.0671,\n",
      "        -0.0602,  0.0411,  0.1518, -0.0128,  0.1021,  0.0380, -0.0618, -0.0268,\n",
      "         0.1222,  0.0266, -0.0341,  0.0175, -0.0549,  0.0562, -0.0313, -0.0062,\n",
      "         0.0635,  0.1021,  0.1544,  0.0332, -0.1124,  0.0177, -0.0699, -0.0127,\n",
      "        -0.0395,  0.0166, -0.0346,  0.0803,  0.0936,  0.0017,  0.0595, -0.0253,\n",
      "         0.0528,  0.1045, -0.1606,  0.0272,  0.0040, -0.0977, -0.0350, -0.0354,\n",
      "         0.0981, -0.0085, -0.0996, -0.0105], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.1243,  0.0182, -0.1558,  0.2575, -0.1185, -0.4024, -0.0465, -0.0521,\n",
      "         0.5445, -0.0032,  0.0662, -0.2139,  0.5755, -0.2096,  0.4419, -0.2682,\n",
      "        -0.2410,  0.1645,  0.6072, -0.0513,  0.4084,  0.1520, -0.2472, -0.1071,\n",
      "         0.4886,  0.1063, -0.1363,  0.0700, -0.2195,  0.2248, -0.1250, -0.0247,\n",
      "         0.2540,  0.4083,  0.6175,  0.1328, -0.4495,  0.0709, -0.2794, -0.0508,\n",
      "        -0.1581,  0.0663, -0.1384,  0.3213,  0.3742,  0.0067,  0.2382, -0.1013,\n",
      "         0.2114,  0.4180, -0.6422,  0.1089,  0.0159, -0.3908, -0.1400, -0.1415,\n",
      "         0.3922, -0.0339, -0.3983, -0.0422], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name output.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.1876,  0.1211, -0.1839, -0.2392, -0.1537,  0.0930,  0.1808, -0.2183,\n",
      "         -0.2477, -0.2111,  0.2171,  0.1873,  0.1251,  0.1397,  0.1093, -0.2261,\n",
      "         -0.2495, -0.2636, -0.2681,  0.1593, -0.0734, -0.0200, -0.1795, -0.1791,\n",
      "         -0.0630,  0.2294, -0.1460, -0.0683,  0.1325,  0.2676, -0.1196,  0.1995,\n",
      "          0.2875,  0.1687,  0.2431, -0.4945,  0.0042,  0.1023, -0.2416,  0.0180,\n",
      "         -0.1167, -0.2004,  0.1393,  0.2139, -0.2647, -0.1645, -0.1790,  0.0096,\n",
      "         -0.0730, -0.1452,  0.1573, -0.1535,  0.1173,  0.1330,  0.0780,  0.2460,\n",
      "          0.3555, -0.1045, -0.1032, -0.0840]], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.7504,  0.4845, -0.7358, -0.9569, -0.6148,  0.3719,  0.7232, -0.8732,\n",
      "         -0.9908, -0.8445,  0.8682,  0.7491,  0.5004,  0.5587,  0.4372, -0.9043,\n",
      "         -0.9979, -1.0542, -1.0723,  0.6371, -0.2937, -0.0801, -0.7181, -0.7164,\n",
      "         -0.2519,  0.9175, -0.5839, -0.2734,  0.5301,  1.0703, -0.4784,  0.7978,\n",
      "          1.1499,  0.6748,  0.9723, -1.9778,  0.0166,  0.4091, -0.9665,  0.0721,\n",
      "         -0.4668, -0.8014,  0.5574,  0.8556, -1.0587, -0.6582, -0.7159,  0.0384,\n",
      "         -0.2919, -0.5807,  0.6291, -0.6138,  0.4693,  0.5319,  0.3122,  0.9840,\n",
      "          1.4222, -0.4179, -0.4128, -0.3359]], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')\n",
      "name output.bias \n",
      "\n",
      "\n",
      " model tensor([0.2638], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([1.0551], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500], device='cuda:0')\n",
      "name layer1.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.2594, -0.3976,  0.1014,  ...,  0.1605,  0.3764,  0.2278],\n",
      "        [-0.0603, -0.2498,  0.4555,  ...,  0.0047,  0.0211, -0.5338],\n",
      "        [ 0.0142, -0.2424,  0.5220,  ...,  0.2355,  0.4628, -0.5232],\n",
      "        ...,\n",
      "        [-0.0353,  0.0278, -0.3114,  ..., -0.2814,  0.3428,  0.1711],\n",
      "        [-0.4060, -0.4858, -0.3467,  ...,  0.2642,  0.0740,  0.0224],\n",
      "        [ 0.3685,  0.1776,  0.1710,  ..., -0.0011, -0.2734, -0.0445]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 1.0376, -1.5904,  0.4056,  ...,  0.6420,  1.5056,  0.9111],\n",
      "        [-0.2412, -0.9991,  1.8220,  ...,  0.0190,  0.0842, -2.1352],\n",
      "        [ 0.0569, -0.9697,  2.0882,  ...,  0.9422,  1.8511, -2.0929],\n",
      "        ...,\n",
      "        [-0.1413,  0.1111, -1.2456,  ..., -1.1256,  1.3714,  0.6843],\n",
      "        [-1.6240, -1.9431, -1.3870,  ...,  1.0569,  0.2961,  0.0896],\n",
      "        [ 1.4741,  0.7104,  0.6841,  ..., -0.0044, -1.0934, -0.1782]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer1.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0533, -0.2976,  0.1683, -0.2796,  0.4783, -0.1859,  0.1970,  0.2235,\n",
      "         0.4476, -0.4746,  0.0582,  0.2373, -0.0943, -0.0593, -0.0383,  0.4056,\n",
      "         0.1094,  0.3961,  0.2945, -0.4114,  0.2467,  0.1748, -0.2457,  0.0433,\n",
      "        -0.2843, -0.4822, -0.0307,  0.0962,  0.3481,  0.2948, -0.1386, -0.2260,\n",
      "        -0.1408, -0.2667,  0.3383, -0.2663,  0.2134, -0.2520, -0.3442, -0.5159,\n",
      "        -0.3523,  0.0703,  0.1675, -0.4977,  0.0274, -0.5294, -0.4071, -0.2197,\n",
      "         0.1088,  0.3603, -0.0918,  0.0754,  0.4334,  0.3057,  0.3766, -0.1408,\n",
      "        -0.3021,  0.3022, -0.2236,  0.3569,  0.4050, -0.2624,  0.4538,  0.1725,\n",
      "         0.0586,  0.2513, -0.1856,  0.3054, -0.2262, -0.1141, -0.0513,  0.2058,\n",
      "        -0.3756,  0.2403, -0.2751, -0.3446,  0.2654, -0.0293, -0.1686,  0.3292,\n",
      "         0.4432, -0.4684, -0.2763, -0.3647,  0.4375, -0.1154,  0.1397,  0.4050,\n",
      "         0.3931, -0.1647, -0.1753, -0.3663, -0.4981, -0.1992,  0.2200,  0.1146,\n",
      "        -0.3251,  0.0533,  0.0160,  0.2042, -0.2496,  0.1141, -0.4078, -0.1424,\n",
      "         0.1085, -0.4270, -0.3692, -0.2484, -0.4747, -0.1498, -0.5340, -0.0314,\n",
      "        -0.1802, -0.0993, -0.4582,  0.2455, -0.2263, -0.0550, -0.1619, -0.1929,\n",
      "         0.0914,  0.1237, -0.0710,  0.4401, -0.2588,  0.4174,  0.3714,  0.2574,\n",
      "        -0.3506, -0.4329,  0.1630,  0.4667, -0.0287,  0.3322, -0.1821, -0.3801,\n",
      "        -0.2611,  0.4145, -0.0007, -0.3786, -0.3146, -0.0159,  0.0102,  0.2508,\n",
      "        -0.3237, -0.3670,  0.1127, -0.5219, -0.2609,  0.3361,  0.2909,  0.2630,\n",
      "         0.3016, -0.3917, -0.4872, -0.4266,  0.2443, -0.3212, -0.0918, -0.0804,\n",
      "        -0.2903, -0.5236, -0.1680,  0.3135,  0.0755, -0.4906,  0.2998, -0.4246,\n",
      "         0.0407, -0.1521,  0.2767, -0.3670, -0.3565,  0.2147,  0.2308,  0.3634,\n",
      "         0.0052,  0.0903,  0.3477,  0.1473,  0.0694, -0.0430, -0.2624, -0.0924,\n",
      "         0.3793,  0.0731, -0.5296,  0.2131,  0.3267,  0.2794, -0.3457,  0.3043,\n",
      "         0.0422,  0.0724, -0.3112,  0.0859, -0.2733, -0.2427,  0.1653, -0.1537,\n",
      "         0.0476, -0.2495, -0.4828, -0.1244, -0.1088,  0.0046, -0.0012,  0.4617,\n",
      "         0.2375, -0.3458,  0.1529, -0.3748, -0.0276, -0.2087,  0.1731, -0.3386,\n",
      "         0.1302, -0.1685,  0.3728,  0.0889, -0.3937,  0.2867,  0.3718, -0.3299,\n",
      "        -0.3796, -0.4960,  0.0468, -0.4306, -0.3099, -0.3600,  0.1805,  0.4462,\n",
      "         0.0164, -0.4883, -0.0203,  0.3129, -0.5222,  0.0441,  0.0137,  0.0102,\n",
      "        -0.1568, -0.5443, -0.0731, -0.1525,  0.2775, -0.1180,  0.4777, -0.3341,\n",
      "        -0.3536,  0.2311, -0.1433, -0.3046,  0.1661,  0.2469, -0.3509,  0.3217,\n",
      "        -0.2336,  0.4728,  0.3464, -0.2160, -0.3805, -0.4916,  0.0239,  0.0339,\n",
      "         0.3407,  0.2819, -0.0255, -0.4977,  0.3217,  0.2836,  0.3084,  0.3222,\n",
      "        -0.0258,  0.2586,  0.0075, -0.4786, -0.2350,  0.3078,  0.0185,  0.1093,\n",
      "         0.0401, -0.0789,  0.1429, -0.3439,  0.2971, -0.4580, -0.0578, -0.5280,\n",
      "         0.2342,  0.3148, -0.4135, -0.2279, -0.4449, -0.3578,  0.3702,  0.1336,\n",
      "        -0.5069, -0.3149, -0.5344,  0.4053, -0.1074,  0.3561, -0.1516,  0.0733,\n",
      "         0.2813,  0.2799,  0.2204,  0.2000, -0.3559, -0.1788, -0.1005,  0.1587,\n",
      "        -0.3056,  0.0480, -0.1572,  0.0988, -0.3203,  0.3752,  0.1441, -0.1858,\n",
      "         0.0496, -0.2569, -0.0618,  0.0264, -0.4264, -0.1943, -0.1492,  0.3311,\n",
      "         0.1900,  0.4722,  0.1155,  0.3057, -0.3209,  0.4003, -0.0986,  0.1345,\n",
      "         0.3535, -0.1337, -0.0899,  0.3471,  0.4302, -0.3008, -0.0366, -0.2108,\n",
      "        -0.1828,  0.4011, -0.5035,  0.2733,  0.0909, -0.0391,  0.3933, -0.0676,\n",
      "        -0.3030, -0.1426, -0.0465,  0.0018, -0.0830,  0.0181,  0.0739, -0.1898,\n",
      "        -0.3978,  0.1440,  0.2752, -0.1834,  0.1594,  0.0292, -0.3882, -0.3835,\n",
      "         0.3140,  0.1215, -0.2803, -0.2791, -0.1879,  0.4263,  0.2653, -0.4888,\n",
      "        -0.3841, -0.5022,  0.2064, -0.1732,  0.2672, -0.4353,  0.1857, -0.4265,\n",
      "        -0.2006,  0.2788, -0.3028,  0.4202,  0.2692,  0.4319,  0.3084, -0.4182,\n",
      "         0.2253, -0.3089,  0.0870,  0.0059, -0.3054, -0.0176, -0.4108, -0.0083,\n",
      "        -0.3409,  0.4043, -0.1234,  0.1365,  0.1504, -0.0527,  0.4243,  0.0080,\n",
      "         0.1225,  0.3391,  0.1453, -0.0215, -0.3462,  0.1187, -0.4027,  0.0875,\n",
      "         0.4066, -0.0149,  0.3713, -0.1068,  0.1035,  0.3800,  0.2460,  0.3619,\n",
      "        -0.4111, -0.1826,  0.2778, -0.3398,  0.3421,  0.4399, -0.3789,  0.3259,\n",
      "        -0.1909, -0.2622,  0.4355,  0.1426,  0.1120,  0.3445,  0.0265, -0.0767,\n",
      "        -0.1538, -0.4370,  0.3049,  0.2692, -0.0558, -0.0103, -0.2042,  0.3164,\n",
      "         0.3008, -0.0461,  0.1757,  0.2354, -0.4257, -0.3459,  0.0008,  0.0081,\n",
      "         0.3492,  0.2701, -0.0588,  0.3723,  0.2743,  0.3463, -0.2644, -0.2942,\n",
      "        -0.3763, -0.1899, -0.1459, -0.5280,  0.3025,  0.0344, -0.4106,  0.2667,\n",
      "        -0.0136, -0.0609, -0.2359,  0.0552,  0.2563,  0.1276,  0.0555, -0.3754,\n",
      "        -0.2732,  0.1979, -0.4710, -0.2199, -0.3255, -0.0013, -0.0319, -0.4547,\n",
      "         0.1165, -0.0975,  0.1514,  0.0644, -0.2438,  0.0285, -0.0318,  0.1264,\n",
      "        -0.4959, -0.1553, -0.0676, -0.1071, -0.2301, -0.4652, -0.3956,  0.1989,\n",
      "         0.3353,  0.3988, -0.2981, -0.4626,  0.4522, -0.2912,  0.1587,  0.1767],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.2131, -1.1906,  0.6731, -1.1182,  1.9130, -0.7437,  0.7880,  0.8939,\n",
      "         1.7904, -1.8985,  0.2328,  0.9491, -0.3773, -0.2372, -0.1531,  1.6225,\n",
      "         0.4378,  1.5845,  1.1779, -1.6454,  0.9866,  0.6994, -0.9830,  0.1732,\n",
      "        -1.1372, -1.9290, -0.1227,  0.3849,  1.3923,  1.1791, -0.5545, -0.9040,\n",
      "        -0.5633, -1.0668,  1.3533, -1.0651,  0.8536, -1.0079, -1.3769, -2.0637,\n",
      "        -1.4092,  0.2810,  0.6701, -1.9910,  0.1098, -2.1177, -1.6282, -0.8786,\n",
      "         0.4351,  1.4413, -0.3673,  0.3018,  1.7335,  1.2228,  1.5062, -0.5631,\n",
      "        -1.2085,  1.2088, -0.8945,  1.4276,  1.6199, -1.0497,  1.8152,  0.6902,\n",
      "         0.2343,  1.0054, -0.7423,  1.2216, -0.9049, -0.4566, -0.2051,  0.8231,\n",
      "        -1.5025,  0.9612, -1.1005, -1.3785,  1.0616, -0.1173, -0.6746,  1.3166,\n",
      "         1.7728, -1.8736, -1.1052, -1.4588,  1.7499, -0.4615,  0.5589,  1.6201,\n",
      "         1.5723, -0.6588, -0.7014, -1.4652, -1.9924, -0.7969,  0.8800,  0.4585,\n",
      "        -1.3004,  0.2133,  0.0640,  0.8167, -0.9985,  0.4563, -1.6313, -0.5697,\n",
      "         0.4339, -1.7081, -1.4767, -0.9935, -1.8990, -0.5992, -2.1360, -0.1256,\n",
      "        -0.7208, -0.3974, -1.8328,  0.9821, -0.9051, -0.2199, -0.6476, -0.7714,\n",
      "         0.3655,  0.4949, -0.2839,  1.7602, -1.0352,  1.6697,  1.4857,  1.0295,\n",
      "        -1.4025, -1.7317,  0.6519,  1.8667, -0.1150,  1.3287, -0.7285, -1.5205,\n",
      "        -1.0445,  1.6579, -0.0028, -1.5144, -1.2584, -0.0636,  0.0409,  1.0034,\n",
      "        -1.2948, -1.4682,  0.4508, -2.0877, -1.0438,  1.3445,  1.1635,  1.0521,\n",
      "         1.2066, -1.5669, -1.9486, -1.7063,  0.9772, -1.2846, -0.3671, -0.3215,\n",
      "        -1.1613, -2.0944, -0.6722,  1.2541,  0.3020, -1.9626,  1.1994, -1.6985,\n",
      "         0.1627, -0.6083,  1.1068, -1.4679, -1.4260,  0.8589,  0.9231,  1.4537,\n",
      "         0.0207,  0.3611,  1.3906,  0.5891,  0.2776, -0.1719, -1.0498, -0.3695,\n",
      "         1.5173,  0.2924, -2.1183,  0.8526,  1.3069,  1.1175, -1.3828,  1.2173,\n",
      "         0.1689,  0.2897, -1.2448,  0.3436, -1.0933, -0.9708,  0.6612, -0.6149,\n",
      "         0.1904, -0.9979, -1.9311, -0.4977, -0.4352,  0.0183, -0.0047,  1.8466,\n",
      "         0.9499, -1.3830,  0.6117, -1.4993, -0.1103, -0.8349,  0.6923, -1.3546,\n",
      "         0.5210, -0.6739,  1.4912,  0.3554, -1.5749,  1.1469,  1.4872, -1.3197,\n",
      "        -1.5184, -1.9840,  0.1870, -1.7225, -1.2398, -1.4401,  0.7221,  1.7849,\n",
      "         0.0654, -1.9533, -0.0813,  1.2516, -2.0887,  0.1764,  0.0549,  0.0409,\n",
      "        -0.6273, -2.1770, -0.2924, -0.6098,  1.1099, -0.4720,  1.9108, -1.3365,\n",
      "        -1.4145,  0.9243, -0.5732, -1.2183,  0.6646,  0.9877, -1.4038,  1.2869,\n",
      "        -0.9345,  1.8911,  1.3854, -0.8641, -1.5221, -1.9665,  0.0955,  0.1354,\n",
      "         1.3629,  1.1275, -0.1020, -1.9906,  1.2867,  1.1343,  1.2337,  1.2887,\n",
      "        -0.1033,  1.0344,  0.0301, -1.9145, -0.9401,  1.2311,  0.0739,  0.4373,\n",
      "         0.1606, -0.3156,  0.5717, -1.3757,  1.1886, -1.8321, -0.2312, -2.1120,\n",
      "         0.9370,  1.2593, -1.6539, -0.9116, -1.7795, -1.4310,  1.4807,  0.5343,\n",
      "        -2.0275, -1.2595, -2.1376,  1.6212, -0.4295,  1.4244, -0.6065,  0.2934,\n",
      "         1.1253,  1.1195,  0.8815,  0.7998, -1.4238, -0.7153, -0.4020,  0.6349,\n",
      "        -1.2225,  0.1919, -0.6289,  0.3952, -1.2812,  1.5006,  0.5765, -0.7431,\n",
      "         0.1983, -1.0275, -0.2470,  0.1057, -1.7058, -0.7773, -0.5967,  1.3246,\n",
      "         0.7600,  1.8889,  0.4619,  1.2226, -1.2838,  1.6011, -0.3944,  0.5381,\n",
      "         1.4140, -0.5347, -0.3597,  1.3885,  1.7208, -1.2032, -0.1463, -0.8433,\n",
      "        -0.7312,  1.6046, -2.0138,  1.0933,  0.3635, -0.1562,  1.5733, -0.2705,\n",
      "        -1.2121, -0.5703, -0.1859,  0.0073, -0.3320,  0.0725,  0.2955, -0.7593,\n",
      "        -1.5914,  0.5759,  1.1008, -0.7336,  0.6374,  0.1168, -1.5529, -1.5339,\n",
      "         1.2558,  0.4860, -1.1212, -1.1166, -0.7516,  1.7053,  1.0611, -1.9554,\n",
      "        -1.5363, -2.0087,  0.8257, -0.6929,  1.0686, -1.7412,  0.7426, -1.7062,\n",
      "        -0.8026,  1.1151, -1.2112,  1.6808,  1.0769,  1.7275,  1.2334, -1.6729,\n",
      "         0.9011, -1.2357,  0.3480,  0.0237, -1.2215, -0.0702, -1.6433, -0.0330,\n",
      "        -1.3638,  1.6171, -0.4937,  0.5458,  0.6017, -0.2107,  1.6971,  0.0319,\n",
      "         0.4902,  1.3563,  0.5812, -0.0860, -1.3850,  0.4749, -1.6107,  0.3498,\n",
      "         1.6265, -0.0594,  1.4850, -0.4271,  0.4141,  1.5199,  0.9841,  1.4477,\n",
      "        -1.6446, -0.7305,  1.1113, -1.3593,  1.3684,  1.7597, -1.5155,  1.3034,\n",
      "        -0.7636, -1.0487,  1.7421,  0.5703,  0.4478,  1.3780,  0.1061, -0.3068,\n",
      "        -0.6152, -1.7480,  1.2195,  1.0768, -0.2231, -0.0413, -0.8166,  1.2656,\n",
      "         1.2032, -0.1842,  0.7027,  0.9416, -1.7029, -1.3836,  0.0033,  0.0324,\n",
      "         1.3969,  1.0806, -0.2351,  1.4891,  1.0973,  1.3851, -1.0574, -1.1769,\n",
      "        -1.5053, -0.7595, -0.5838, -2.1122,  1.2101,  0.1377, -1.6425,  1.0670,\n",
      "        -0.0542, -0.2436, -0.9437,  0.2207,  1.0251,  0.5105,  0.2218, -1.5015,\n",
      "        -1.0929,  0.7918, -1.8841, -0.8797, -1.3020, -0.0050, -0.1278, -1.8188,\n",
      "         0.4661, -0.3899,  0.6054,  0.2576, -0.9753,  0.1138, -0.1273,  0.5054,\n",
      "        -1.9837, -0.6214, -0.2703, -0.4285, -0.9202, -1.8610, -1.5826,  0.7955,\n",
      "         1.3411,  1.5950, -1.1925, -1.8503,  1.8087, -1.1648,  0.6348,  0.7067],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
      "       device='cuda:0')\n",
      "name layer2.weight \n",
      "\n",
      "\n",
      " model tensor([[-0.0634, -0.0549,  0.0018,  ...,  0.0458, -0.0822, -0.0203],\n",
      "        [ 0.0536,  0.0359,  0.0015,  ..., -0.0115, -0.0132,  0.0060],\n",
      "        [-0.0662, -0.0451,  0.0247,  ...,  0.0570,  0.0432, -0.0235],\n",
      "        ...,\n",
      "        [ 0.0251, -0.0267, -0.0191,  ..., -0.0429, -0.0456,  0.0241],\n",
      "        [ 0.1043, -0.0145,  0.0097,  ..., -0.0637, -0.0198,  0.0129],\n",
      "        [ 0.0114,  0.0589,  0.0012,  ...,  0.0182,  0.0218, -0.0614]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[-0.2534, -0.2197,  0.0070,  ...,  0.1831, -0.3286, -0.0811],\n",
      "        [ 0.2145,  0.1437,  0.0058,  ..., -0.0459, -0.0527,  0.0238],\n",
      "        [-0.2646, -0.1803,  0.0989,  ...,  0.2280,  0.1728, -0.0941],\n",
      "        ...,\n",
      "        [ 0.1004, -0.1066, -0.0764,  ..., -0.1718, -0.1823,  0.0963],\n",
      "        [ 0.4172, -0.0581,  0.0387,  ..., -0.2550, -0.0793,  0.0516],\n",
      "        [ 0.0455,  0.2356,  0.0048,  ...,  0.0728,  0.0871, -0.2454]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer2.bias \n",
      "\n",
      "\n",
      " model tensor([ 6.6205e-03, -3.2377e-02, -4.5958e-02,  6.1730e-02,  5.1142e-02,\n",
      "        -1.7462e-02, -5.4124e-02, -1.1232e-02,  5.8617e-02, -6.6676e-02,\n",
      "         3.8840e-02, -4.0419e-02,  9.2222e-03, -6.6494e-02, -4.6496e-02,\n",
      "        -1.2460e-02, -6.5301e-04, -6.5983e-02,  2.2284e-02, -2.4886e-02,\n",
      "         4.2531e-02,  8.4915e-02,  5.9802e-02, -3.4652e-03, -2.8837e-02,\n",
      "         6.0173e-02, -6.3772e-02, -8.8647e-02, -3.2353e-02,  1.2800e-01,\n",
      "         7.2008e-02,  8.0851e-02, -4.0117e-02,  2.5901e-02,  3.5606e-02,\n",
      "        -7.7808e-02,  3.5788e-03, -2.1756e-02, -1.2306e-01,  4.8123e-02,\n",
      "         1.1054e-02, -7.2002e-02,  7.8845e-02,  7.1751e-03,  3.9806e-02,\n",
      "         2.8828e-02, -3.6909e-02,  5.8479e-02, -7.0202e-02,  5.6132e-02,\n",
      "        -3.1911e-02, -1.9451e-02,  1.2588e-02, -4.9605e-02, -2.6415e-02,\n",
      "         5.0884e-02,  4.3441e-02, -1.1917e-02, -3.6576e-02,  5.1420e-02,\n",
      "        -8.2422e-02,  7.8674e-02, -3.3395e-02, -1.1349e-02,  3.8636e-03,\n",
      "        -3.9502e-03,  4.0672e-02, -2.6127e-02,  2.6751e-02, -2.2839e-02,\n",
      "        -3.9352e-02,  2.5426e-02,  3.9454e-02, -4.3014e-02, -6.4135e-02,\n",
      "         2.4835e-02, -1.9392e-02,  4.7930e-02,  5.5934e-02,  2.8320e-02,\n",
      "        -8.6030e-02,  1.4140e-02,  2.4989e-02,  4.3506e-02, -4.9501e-02,\n",
      "         3.8060e-02, -2.3130e-02, -5.5456e-02,  7.4137e-02,  6.9622e-02,\n",
      "        -5.7255e-02, -1.2605e-02, -3.3073e-02, -5.0543e-02, -2.6859e-02,\n",
      "         2.7225e-02, -3.9992e-02, -1.5378e-01,  4.2800e-02, -1.3810e-04,\n",
      "         4.8996e-02, -7.1443e-02, -1.6780e-02, -1.1945e-02, -7.4407e-02,\n",
      "        -3.6141e-02, -1.2170e-02, -5.0433e-02, -7.2944e-02, -1.8268e-02,\n",
      "        -8.9378e-02,  7.2016e-03,  5.4560e-02, -5.7141e-02,  5.8410e-03,\n",
      "         7.8669e-02,  4.9474e-02, -2.1693e-02, -8.4660e-02,  5.1386e-02,\n",
      "         8.8519e-03,  4.5689e-02, -2.3721e-02,  4.9863e-02,  3.7764e-03,\n",
      "         4.1239e-02,  1.1756e-01,  2.4989e-02, -1.7260e-02, -7.1427e-02,\n",
      "        -7.9698e-02, -7.5401e-02,  7.6057e-02,  4.5944e-02,  1.5109e-02,\n",
      "        -4.0553e-02,  5.7381e-02,  2.7081e-02, -1.4482e-02, -7.1393e-02,\n",
      "        -1.1495e-02, -2.8827e-02, -5.9305e-02, -9.3016e-02, -8.8831e-02,\n",
      "         5.9434e-02, -4.1125e-03, -9.1109e-02, -9.7782e-02,  6.5756e-02,\n",
      "        -6.5883e-02, -6.4734e-02,  2.9286e-05, -7.8842e-02,  2.7385e-02,\n",
      "        -3.7701e-02,  3.7715e-02,  9.7803e-02, -5.0331e-02, -6.7174e-02,\n",
      "        -1.5403e-02, -1.3481e-02,  6.7517e-02,  6.1399e-02, -8.1103e-02,\n",
      "        -1.1418e-01,  7.5766e-02,  4.6393e-03,  6.1258e-02, -1.7248e-02,\n",
      "         6.2396e-02, -3.1449e-02,  2.7584e-02,  3.5604e-02, -7.7220e-02,\n",
      "        -6.0437e-02, -3.1787e-02,  4.3068e-02,  6.6608e-02,  5.1794e-02,\n",
      "         6.3146e-02, -8.5386e-02, -7.8354e-02,  4.2019e-02, -7.6459e-02,\n",
      "         6.8323e-02,  9.4791e-02, -6.3084e-02, -1.5255e-03,  1.0377e-02,\n",
      "         6.8170e-02, -5.7751e-02,  1.0723e-02,  7.7447e-02, -9.2818e-03,\n",
      "        -9.2200e-02, -5.3574e-02, -5.7750e-02, -1.9773e-02, -8.3680e-02,\n",
      "        -6.3854e-03, -3.3826e-02, -5.6497e-02, -2.6097e-02, -5.3209e-02,\n",
      "         1.5681e-03, -1.8333e-02,  5.8289e-02, -8.5990e-02,  6.7770e-02,\n",
      "        -1.0025e-01,  2.6034e-02,  3.0845e-02, -4.5089e-02, -2.0955e-03,\n",
      "         1.6112e-02,  5.3509e-02, -3.8658e-03,  9.3778e-02, -6.5697e-02,\n",
      "        -2.8214e-02,  3.9072e-02, -2.7483e-02,  7.7915e-02, -1.8103e-02,\n",
      "        -2.3917e-02, -5.3242e-03,  6.8477e-02,  7.3193e-02,  4.7106e-02,\n",
      "        -6.8053e-02,  5.2855e-02,  7.4291e-02,  2.1017e-02, -6.6047e-02,\n",
      "        -1.9512e-02,  5.4492e-02,  1.8015e-02,  4.1885e-02,  1.7024e-02,\n",
      "        -8.1572e-02,  9.6217e-03, -1.0126e-01, -5.6150e-02, -8.1864e-02,\n",
      "        -5.3142e-02,  4.3732e-02, -4.9532e-03, -5.4868e-02, -8.3943e-02,\n",
      "         3.6248e-02, -2.0443e-02,  4.5764e-02,  2.3292e-02,  9.1441e-02,\n",
      "        -4.6304e-02], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([ 2.6482e-02, -1.2951e-01, -1.8383e-01,  2.4692e-01,  2.0457e-01,\n",
      "        -6.9849e-02, -2.1649e-01, -4.4930e-02,  2.3447e-01, -2.6670e-01,\n",
      "         1.5536e-01, -1.6168e-01,  3.6889e-02, -2.6598e-01, -1.8598e-01,\n",
      "        -4.9841e-02, -2.6121e-03, -2.6393e-01,  8.9138e-02, -9.9545e-02,\n",
      "         1.7012e-01,  3.3966e-01,  2.3921e-01, -1.3861e-02, -1.1535e-01,\n",
      "         2.4069e-01, -2.5509e-01, -3.5459e-01, -1.2941e-01,  5.1200e-01,\n",
      "         2.8803e-01,  3.2340e-01, -1.6047e-01,  1.0360e-01,  1.4243e-01,\n",
      "        -3.1123e-01,  1.4315e-02, -8.7023e-02, -4.9225e-01,  1.9249e-01,\n",
      "         4.4216e-02, -2.8801e-01,  3.1538e-01,  2.8700e-02,  1.5922e-01,\n",
      "         1.1531e-01, -1.4764e-01,  2.3392e-01, -2.8081e-01,  2.2453e-01,\n",
      "        -1.2764e-01, -7.7804e-02,  5.0351e-02, -1.9842e-01, -1.0566e-01,\n",
      "         2.0354e-01,  1.7377e-01, -4.7669e-02, -1.4630e-01,  2.0568e-01,\n",
      "        -3.2969e-01,  3.1470e-01, -1.3358e-01, -4.5395e-02,  1.5454e-02,\n",
      "        -1.5801e-02,  1.6269e-01, -1.0451e-01,  1.0700e-01, -9.1358e-02,\n",
      "        -1.5741e-01,  1.0170e-01,  1.5782e-01, -1.7206e-01, -2.5654e-01,\n",
      "         9.9338e-02, -7.7569e-02,  1.9172e-01,  2.2374e-01,  1.1328e-01,\n",
      "        -3.4412e-01,  5.6559e-02,  9.9956e-02,  1.7403e-01, -1.9800e-01,\n",
      "         1.5224e-01, -9.2521e-02, -2.2183e-01,  2.9655e-01,  2.7849e-01,\n",
      "        -2.2902e-01, -5.0421e-02, -1.3229e-01, -2.0217e-01, -1.0744e-01,\n",
      "         1.0890e-01, -1.5997e-01, -6.1513e-01,  1.7120e-01, -5.5240e-04,\n",
      "         1.9598e-01, -2.8577e-01, -6.7118e-02, -4.7782e-02, -2.9763e-01,\n",
      "        -1.4456e-01, -4.8680e-02, -2.0173e-01, -2.9177e-01, -7.3074e-02,\n",
      "        -3.5751e-01,  2.8807e-02,  2.1824e-01, -2.2856e-01,  2.3364e-02,\n",
      "         3.1468e-01,  1.9790e-01, -8.6772e-02, -3.3864e-01,  2.0554e-01,\n",
      "         3.5408e-02,  1.8276e-01, -9.4883e-02,  1.9945e-01,  1.5106e-02,\n",
      "         1.6495e-01,  4.7023e-01,  9.9957e-02, -6.9039e-02, -2.8571e-01,\n",
      "        -3.1879e-01, -3.0160e-01,  3.0423e-01,  1.8378e-01,  6.0437e-02,\n",
      "        -1.6221e-01,  2.2952e-01,  1.0832e-01, -5.7927e-02, -2.8557e-01,\n",
      "        -4.5980e-02, -1.1531e-01, -2.3722e-01, -3.7206e-01, -3.5532e-01,\n",
      "         2.3774e-01, -1.6450e-02, -3.6444e-01, -3.9113e-01,  2.6302e-01,\n",
      "        -2.6353e-01, -2.5894e-01,  1.1714e-04, -3.1537e-01,  1.0954e-01,\n",
      "        -1.5080e-01,  1.5086e-01,  3.9121e-01, -2.0133e-01, -2.6870e-01,\n",
      "        -6.1614e-02, -5.3923e-02,  2.7007e-01,  2.4559e-01, -3.2441e-01,\n",
      "        -4.5673e-01,  3.0306e-01,  1.8557e-02,  2.4503e-01, -6.8993e-02,\n",
      "         2.4959e-01, -1.2579e-01,  1.1033e-01,  1.4242e-01, -3.0888e-01,\n",
      "        -2.4175e-01, -1.2715e-01,  1.7227e-01,  2.6643e-01,  2.0718e-01,\n",
      "         2.5258e-01, -3.4154e-01, -3.1342e-01,  1.6808e-01, -3.0583e-01,\n",
      "         2.7329e-01,  3.7916e-01, -2.5233e-01, -6.1019e-03,  4.1509e-02,\n",
      "         2.7268e-01, -2.3100e-01,  4.2892e-02,  3.0979e-01, -3.7127e-02,\n",
      "        -3.6880e-01, -2.1429e-01, -2.3100e-01, -7.9094e-02, -3.3472e-01,\n",
      "        -2.5542e-02, -1.3530e-01, -2.2599e-01, -1.0439e-01, -2.1284e-01,\n",
      "         6.2723e-03, -7.3330e-02,  2.3316e-01, -3.4396e-01,  2.7108e-01,\n",
      "        -4.0098e-01,  1.0413e-01,  1.2338e-01, -1.8036e-01, -8.3819e-03,\n",
      "         6.4446e-02,  2.1404e-01, -1.5463e-02,  3.7511e-01, -2.6279e-01,\n",
      "        -1.1286e-01,  1.5629e-01, -1.0993e-01,  3.1166e-01, -7.2410e-02,\n",
      "        -9.5668e-02, -2.1297e-02,  2.7391e-01,  2.9277e-01,  1.8842e-01,\n",
      "        -2.7221e-01,  2.1142e-01,  2.9716e-01,  8.4069e-02, -2.6419e-01,\n",
      "        -7.8046e-02,  2.1797e-01,  7.2058e-02,  1.6754e-01,  6.8098e-02,\n",
      "        -3.2629e-01,  3.8487e-02, -4.0502e-01, -2.2460e-01, -3.2746e-01,\n",
      "        -2.1257e-01,  1.7493e-01, -1.9813e-02, -2.1947e-01, -3.3577e-01,\n",
      "         1.4499e-01, -8.1772e-02,  1.8306e-01,  9.3166e-02,  3.6577e-01,\n",
      "        -1.8522e-01], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name layer3.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.0534,  0.0394, -0.0593,  ...,  0.0331, -0.0660, -0.1512],\n",
      "        [-0.0707,  0.0280,  0.0912,  ...,  0.0649,  0.0359, -0.0095],\n",
      "        [ 0.0057, -0.0846,  0.0573,  ..., -0.0995,  0.0775,  0.1348],\n",
      "        ...,\n",
      "        [ 0.0561, -0.0423, -0.0047,  ...,  0.0799,  0.0327,  0.1121],\n",
      "        [ 0.0439, -0.1047,  0.0781,  ..., -0.0373,  0.0304, -0.0388],\n",
      "        [ 0.1122, -0.0937,  0.0110,  ..., -0.0770,  0.0039, -0.0645]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.2138,  0.1574, -0.2373,  ...,  0.1324, -0.2639, -0.6049],\n",
      "        [-0.2829,  0.1122,  0.3646,  ...,  0.2595,  0.1435, -0.0381],\n",
      "        [ 0.0226, -0.3384,  0.2291,  ..., -0.3981,  0.3100,  0.5393],\n",
      "        ...,\n",
      "        [ 0.2245, -0.1691, -0.0189,  ...,  0.3196,  0.1307,  0.4484],\n",
      "        [ 0.1756, -0.4190,  0.3125,  ..., -0.1493,  0.1214, -0.1554],\n",
      "        [ 0.4488, -0.3749,  0.0442,  ..., -0.3079,  0.0156, -0.2579]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer3.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0311,  0.0045, -0.0389,  0.0644, -0.0296, -0.1006, -0.0116, -0.0130,\n",
      "         0.1361, -0.0008,  0.0166, -0.0535,  0.1439, -0.0524,  0.1105, -0.0671,\n",
      "        -0.0602,  0.0411,  0.1518, -0.0128,  0.1021,  0.0380, -0.0618, -0.0268,\n",
      "         0.1222,  0.0266, -0.0341,  0.0175, -0.0549,  0.0562, -0.0313, -0.0062,\n",
      "         0.0635,  0.1021,  0.1544,  0.0332, -0.1124,  0.0177, -0.0699, -0.0127,\n",
      "        -0.0395,  0.0166, -0.0346,  0.0803,  0.0936,  0.0017,  0.0595, -0.0253,\n",
      "         0.0528,  0.1045, -0.1606,  0.0272,  0.0040, -0.0977, -0.0350, -0.0354,\n",
      "         0.0981, -0.0085, -0.0996, -0.0105], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.1243,  0.0182, -0.1558,  0.2575, -0.1185, -0.4024, -0.0465, -0.0521,\n",
      "         0.5445, -0.0032,  0.0662, -0.2139,  0.5755, -0.2096,  0.4419, -0.2682,\n",
      "        -0.2410,  0.1645,  0.6072, -0.0513,  0.4084,  0.1520, -0.2472, -0.1071,\n",
      "         0.4886,  0.1063, -0.1363,  0.0700, -0.2195,  0.2248, -0.1250, -0.0247,\n",
      "         0.2540,  0.4083,  0.6175,  0.1328, -0.4495,  0.0709, -0.2794, -0.0508,\n",
      "        -0.1581,  0.0663, -0.1384,  0.3213,  0.3742,  0.0067,  0.2382, -0.1013,\n",
      "         0.2114,  0.4180, -0.6422,  0.1089,  0.0159, -0.3908, -0.1400, -0.1415,\n",
      "         0.3922, -0.0339, -0.3983, -0.0422], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name output.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.1876,  0.1211, -0.1839, -0.2392, -0.1537,  0.0930,  0.1808, -0.2183,\n",
      "         -0.2477, -0.2111,  0.2171,  0.1873,  0.1251,  0.1397,  0.1093, -0.2261,\n",
      "         -0.2495, -0.2636, -0.2681,  0.1593, -0.0734, -0.0200, -0.1795, -0.1791,\n",
      "         -0.0630,  0.2294, -0.1460, -0.0683,  0.1325,  0.2676, -0.1196,  0.1995,\n",
      "          0.2875,  0.1687,  0.2431, -0.4945,  0.0042,  0.1023, -0.2416,  0.0180,\n",
      "         -0.1167, -0.2004,  0.1393,  0.2139, -0.2647, -0.1645, -0.1790,  0.0096,\n",
      "         -0.0730, -0.1452,  0.1573, -0.1535,  0.1173,  0.1330,  0.0780,  0.2460,\n",
      "          0.3555, -0.1045, -0.1032, -0.0840]], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.7504,  0.4845, -0.7358, -0.9569, -0.6148,  0.3719,  0.7232, -0.8732,\n",
      "         -0.9908, -0.8445,  0.8682,  0.7491,  0.5004,  0.5587,  0.4372, -0.9043,\n",
      "         -0.9979, -1.0542, -1.0723,  0.6371, -0.2937, -0.0801, -0.7181, -0.7164,\n",
      "         -0.2519,  0.9175, -0.5839, -0.2734,  0.5301,  1.0703, -0.4784,  0.7978,\n",
      "          1.1499,  0.6748,  0.9723, -1.9778,  0.0166,  0.4091, -0.9665,  0.0721,\n",
      "         -0.4668, -0.8014,  0.5574,  0.8556, -1.0587, -0.6582, -0.7159,  0.0384,\n",
      "         -0.2919, -0.5807,  0.6291, -0.6138,  0.4693,  0.5319,  0.3122,  0.9840,\n",
      "          1.4222, -0.4179, -0.4128, -0.3359]], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')\n",
      "name output.bias \n",
      "\n",
      "\n",
      " model tensor([0.2638], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([1.0551], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500], device='cuda:0')\n",
      "name layer1.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.2594, -0.3976,  0.1014,  ...,  0.1605,  0.3764,  0.2278],\n",
      "        [-0.0603, -0.2498,  0.4555,  ...,  0.0047,  0.0211, -0.5338],\n",
      "        [ 0.0142, -0.2424,  0.5220,  ...,  0.2355,  0.4628, -0.5232],\n",
      "        ...,\n",
      "        [-0.0353,  0.0278, -0.3114,  ..., -0.2814,  0.3428,  0.1711],\n",
      "        [-0.4060, -0.4858, -0.3467,  ...,  0.2642,  0.0740,  0.0224],\n",
      "        [ 0.3685,  0.1776,  0.1710,  ..., -0.0011, -0.2734, -0.0445]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 1.0376, -1.5904,  0.4056,  ...,  0.6420,  1.5056,  0.9111],\n",
      "        [-0.2412, -0.9991,  1.8220,  ...,  0.0190,  0.0842, -2.1352],\n",
      "        [ 0.0569, -0.9697,  2.0882,  ...,  0.9422,  1.8511, -2.0929],\n",
      "        ...,\n",
      "        [-0.1413,  0.1111, -1.2456,  ..., -1.1256,  1.3714,  0.6843],\n",
      "        [-1.6240, -1.9431, -1.3870,  ...,  1.0569,  0.2961,  0.0896],\n",
      "        [ 1.4741,  0.7104,  0.6841,  ..., -0.0044, -1.0934, -0.1782]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer1.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0533, -0.2976,  0.1683, -0.2796,  0.4783, -0.1859,  0.1970,  0.2235,\n",
      "         0.4476, -0.4746,  0.0582,  0.2373, -0.0943, -0.0593, -0.0383,  0.4056,\n",
      "         0.1094,  0.3961,  0.2945, -0.4114,  0.2467,  0.1748, -0.2457,  0.0433,\n",
      "        -0.2843, -0.4822, -0.0307,  0.0962,  0.3481,  0.2948, -0.1386, -0.2260,\n",
      "        -0.1408, -0.2667,  0.3383, -0.2663,  0.2134, -0.2520, -0.3442, -0.5159,\n",
      "        -0.3523,  0.0703,  0.1675, -0.4977,  0.0274, -0.5294, -0.4071, -0.2197,\n",
      "         0.1088,  0.3603, -0.0918,  0.0754,  0.4334,  0.3057,  0.3766, -0.1408,\n",
      "        -0.3021,  0.3022, -0.2236,  0.3569,  0.4050, -0.2624,  0.4538,  0.1725,\n",
      "         0.0586,  0.2513, -0.1856,  0.3054, -0.2262, -0.1141, -0.0513,  0.2058,\n",
      "        -0.3756,  0.2403, -0.2751, -0.3446,  0.2654, -0.0293, -0.1686,  0.3292,\n",
      "         0.4432, -0.4684, -0.2763, -0.3647,  0.4375, -0.1154,  0.1397,  0.4050,\n",
      "         0.3931, -0.1647, -0.1753, -0.3663, -0.4981, -0.1992,  0.2200,  0.1146,\n",
      "        -0.3251,  0.0533,  0.0160,  0.2042, -0.2496,  0.1141, -0.4078, -0.1424,\n",
      "         0.1085, -0.4270, -0.3692, -0.2484, -0.4747, -0.1498, -0.5340, -0.0314,\n",
      "        -0.1802, -0.0993, -0.4582,  0.2455, -0.2263, -0.0550, -0.1619, -0.1929,\n",
      "         0.0914,  0.1237, -0.0710,  0.4401, -0.2588,  0.4174,  0.3714,  0.2574,\n",
      "        -0.3506, -0.4329,  0.1630,  0.4667, -0.0287,  0.3322, -0.1821, -0.3801,\n",
      "        -0.2611,  0.4145, -0.0007, -0.3786, -0.3146, -0.0159,  0.0102,  0.2508,\n",
      "        -0.3237, -0.3670,  0.1127, -0.5219, -0.2609,  0.3361,  0.2909,  0.2630,\n",
      "         0.3016, -0.3917, -0.4872, -0.4266,  0.2443, -0.3212, -0.0918, -0.0804,\n",
      "        -0.2903, -0.5236, -0.1680,  0.3135,  0.0755, -0.4906,  0.2998, -0.4246,\n",
      "         0.0407, -0.1521,  0.2767, -0.3670, -0.3565,  0.2147,  0.2308,  0.3634,\n",
      "         0.0052,  0.0903,  0.3477,  0.1473,  0.0694, -0.0430, -0.2624, -0.0924,\n",
      "         0.3793,  0.0731, -0.5296,  0.2131,  0.3267,  0.2794, -0.3457,  0.3043,\n",
      "         0.0422,  0.0724, -0.3112,  0.0859, -0.2733, -0.2427,  0.1653, -0.1537,\n",
      "         0.0476, -0.2495, -0.4828, -0.1244, -0.1088,  0.0046, -0.0012,  0.4617,\n",
      "         0.2375, -0.3458,  0.1529, -0.3748, -0.0276, -0.2087,  0.1731, -0.3386,\n",
      "         0.1302, -0.1685,  0.3728,  0.0889, -0.3937,  0.2867,  0.3718, -0.3299,\n",
      "        -0.3796, -0.4960,  0.0468, -0.4306, -0.3099, -0.3600,  0.1805,  0.4462,\n",
      "         0.0164, -0.4883, -0.0203,  0.3129, -0.5222,  0.0441,  0.0137,  0.0102,\n",
      "        -0.1568, -0.5443, -0.0731, -0.1525,  0.2775, -0.1180,  0.4777, -0.3341,\n",
      "        -0.3536,  0.2311, -0.1433, -0.3046,  0.1661,  0.2469, -0.3509,  0.3217,\n",
      "        -0.2336,  0.4728,  0.3464, -0.2160, -0.3805, -0.4916,  0.0239,  0.0339,\n",
      "         0.3407,  0.2819, -0.0255, -0.4977,  0.3217,  0.2836,  0.3084,  0.3222,\n",
      "        -0.0258,  0.2586,  0.0075, -0.4786, -0.2350,  0.3078,  0.0185,  0.1093,\n",
      "         0.0401, -0.0789,  0.1429, -0.3439,  0.2971, -0.4580, -0.0578, -0.5280,\n",
      "         0.2342,  0.3148, -0.4135, -0.2279, -0.4449, -0.3578,  0.3702,  0.1336,\n",
      "        -0.5069, -0.3149, -0.5344,  0.4053, -0.1074,  0.3561, -0.1516,  0.0733,\n",
      "         0.2813,  0.2799,  0.2204,  0.2000, -0.3559, -0.1788, -0.1005,  0.1587,\n",
      "        -0.3056,  0.0480, -0.1572,  0.0988, -0.3203,  0.3752,  0.1441, -0.1858,\n",
      "         0.0496, -0.2569, -0.0618,  0.0264, -0.4264, -0.1943, -0.1492,  0.3311,\n",
      "         0.1900,  0.4722,  0.1155,  0.3057, -0.3209,  0.4003, -0.0986,  0.1345,\n",
      "         0.3535, -0.1337, -0.0899,  0.3471,  0.4302, -0.3008, -0.0366, -0.2108,\n",
      "        -0.1828,  0.4011, -0.5035,  0.2733,  0.0909, -0.0391,  0.3933, -0.0676,\n",
      "        -0.3030, -0.1426, -0.0465,  0.0018, -0.0830,  0.0181,  0.0739, -0.1898,\n",
      "        -0.3978,  0.1440,  0.2752, -0.1834,  0.1594,  0.0292, -0.3882, -0.3835,\n",
      "         0.3140,  0.1215, -0.2803, -0.2791, -0.1879,  0.4263,  0.2653, -0.4888,\n",
      "        -0.3841, -0.5022,  0.2064, -0.1732,  0.2672, -0.4353,  0.1857, -0.4265,\n",
      "        -0.2006,  0.2788, -0.3028,  0.4202,  0.2692,  0.4319,  0.3084, -0.4182,\n",
      "         0.2253, -0.3089,  0.0870,  0.0059, -0.3054, -0.0176, -0.4108, -0.0083,\n",
      "        -0.3409,  0.4043, -0.1234,  0.1365,  0.1504, -0.0527,  0.4243,  0.0080,\n",
      "         0.1225,  0.3391,  0.1453, -0.0215, -0.3462,  0.1187, -0.4027,  0.0875,\n",
      "         0.4066, -0.0149,  0.3713, -0.1068,  0.1035,  0.3800,  0.2460,  0.3619,\n",
      "        -0.4111, -0.1826,  0.2778, -0.3398,  0.3421,  0.4399, -0.3789,  0.3259,\n",
      "        -0.1909, -0.2622,  0.4355,  0.1426,  0.1120,  0.3445,  0.0265, -0.0767,\n",
      "        -0.1538, -0.4370,  0.3049,  0.2692, -0.0558, -0.0103, -0.2042,  0.3164,\n",
      "         0.3008, -0.0461,  0.1757,  0.2354, -0.4257, -0.3459,  0.0008,  0.0081,\n",
      "         0.3492,  0.2701, -0.0588,  0.3723,  0.2743,  0.3463, -0.2644, -0.2942,\n",
      "        -0.3763, -0.1899, -0.1459, -0.5280,  0.3025,  0.0344, -0.4106,  0.2667,\n",
      "        -0.0136, -0.0609, -0.2359,  0.0552,  0.2563,  0.1276,  0.0555, -0.3754,\n",
      "        -0.2732,  0.1979, -0.4710, -0.2199, -0.3255, -0.0013, -0.0319, -0.4547,\n",
      "         0.1165, -0.0975,  0.1514,  0.0644, -0.2438,  0.0285, -0.0318,  0.1264,\n",
      "        -0.4959, -0.1553, -0.0676, -0.1071, -0.2301, -0.4652, -0.3956,  0.1989,\n",
      "         0.3353,  0.3988, -0.2981, -0.4626,  0.4522, -0.2912,  0.1587,  0.1767],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.2131, -1.1906,  0.6731, -1.1182,  1.9130, -0.7437,  0.7880,  0.8939,\n",
      "         1.7904, -1.8985,  0.2328,  0.9491, -0.3773, -0.2372, -0.1531,  1.6225,\n",
      "         0.4378,  1.5845,  1.1779, -1.6454,  0.9866,  0.6994, -0.9830,  0.1732,\n",
      "        -1.1372, -1.9290, -0.1227,  0.3849,  1.3923,  1.1791, -0.5545, -0.9040,\n",
      "        -0.5633, -1.0668,  1.3533, -1.0651,  0.8536, -1.0079, -1.3769, -2.0637,\n",
      "        -1.4092,  0.2810,  0.6701, -1.9910,  0.1098, -2.1177, -1.6282, -0.8786,\n",
      "         0.4351,  1.4413, -0.3673,  0.3018,  1.7335,  1.2228,  1.5062, -0.5631,\n",
      "        -1.2085,  1.2088, -0.8945,  1.4276,  1.6199, -1.0497,  1.8152,  0.6902,\n",
      "         0.2343,  1.0054, -0.7423,  1.2216, -0.9049, -0.4566, -0.2051,  0.8231,\n",
      "        -1.5025,  0.9612, -1.1005, -1.3785,  1.0616, -0.1173, -0.6746,  1.3166,\n",
      "         1.7728, -1.8736, -1.1052, -1.4588,  1.7499, -0.4615,  0.5589,  1.6201,\n",
      "         1.5723, -0.6588, -0.7014, -1.4652, -1.9924, -0.7969,  0.8800,  0.4585,\n",
      "        -1.3004,  0.2133,  0.0640,  0.8167, -0.9985,  0.4563, -1.6313, -0.5697,\n",
      "         0.4339, -1.7081, -1.4767, -0.9935, -1.8990, -0.5992, -2.1360, -0.1256,\n",
      "        -0.7208, -0.3974, -1.8328,  0.9821, -0.9051, -0.2199, -0.6476, -0.7714,\n",
      "         0.3655,  0.4949, -0.2839,  1.7602, -1.0352,  1.6697,  1.4857,  1.0295,\n",
      "        -1.4025, -1.7317,  0.6519,  1.8667, -0.1150,  1.3287, -0.7285, -1.5205,\n",
      "        -1.0445,  1.6579, -0.0028, -1.5144, -1.2584, -0.0636,  0.0409,  1.0034,\n",
      "        -1.2948, -1.4682,  0.4508, -2.0877, -1.0438,  1.3445,  1.1635,  1.0521,\n",
      "         1.2066, -1.5669, -1.9486, -1.7063,  0.9772, -1.2846, -0.3671, -0.3215,\n",
      "        -1.1613, -2.0944, -0.6722,  1.2541,  0.3020, -1.9626,  1.1994, -1.6985,\n",
      "         0.1627, -0.6083,  1.1068, -1.4679, -1.4260,  0.8589,  0.9231,  1.4537,\n",
      "         0.0207,  0.3611,  1.3906,  0.5891,  0.2776, -0.1719, -1.0498, -0.3695,\n",
      "         1.5173,  0.2924, -2.1183,  0.8526,  1.3069,  1.1175, -1.3828,  1.2173,\n",
      "         0.1689,  0.2897, -1.2448,  0.3436, -1.0933, -0.9708,  0.6612, -0.6149,\n",
      "         0.1904, -0.9979, -1.9311, -0.4977, -0.4352,  0.0183, -0.0047,  1.8466,\n",
      "         0.9499, -1.3830,  0.6117, -1.4993, -0.1103, -0.8349,  0.6923, -1.3546,\n",
      "         0.5210, -0.6739,  1.4912,  0.3554, -1.5749,  1.1469,  1.4872, -1.3197,\n",
      "        -1.5184, -1.9840,  0.1870, -1.7225, -1.2398, -1.4401,  0.7221,  1.7849,\n",
      "         0.0654, -1.9533, -0.0813,  1.2516, -2.0887,  0.1764,  0.0549,  0.0409,\n",
      "        -0.6273, -2.1770, -0.2924, -0.6098,  1.1099, -0.4720,  1.9108, -1.3365,\n",
      "        -1.4145,  0.9243, -0.5732, -1.2183,  0.6646,  0.9877, -1.4038,  1.2869,\n",
      "        -0.9345,  1.8911,  1.3854, -0.8641, -1.5221, -1.9665,  0.0955,  0.1354,\n",
      "         1.3629,  1.1275, -0.1020, -1.9906,  1.2867,  1.1343,  1.2337,  1.2887,\n",
      "        -0.1033,  1.0344,  0.0301, -1.9145, -0.9401,  1.2311,  0.0739,  0.4373,\n",
      "         0.1606, -0.3156,  0.5717, -1.3757,  1.1886, -1.8321, -0.2312, -2.1120,\n",
      "         0.9370,  1.2593, -1.6539, -0.9116, -1.7795, -1.4310,  1.4807,  0.5343,\n",
      "        -2.0275, -1.2595, -2.1376,  1.6212, -0.4295,  1.4244, -0.6065,  0.2934,\n",
      "         1.1253,  1.1195,  0.8815,  0.7998, -1.4238, -0.7153, -0.4020,  0.6349,\n",
      "        -1.2225,  0.1919, -0.6289,  0.3952, -1.2812,  1.5006,  0.5765, -0.7431,\n",
      "         0.1983, -1.0275, -0.2470,  0.1057, -1.7058, -0.7773, -0.5967,  1.3246,\n",
      "         0.7600,  1.8889,  0.4619,  1.2226, -1.2838,  1.6011, -0.3944,  0.5381,\n",
      "         1.4140, -0.5347, -0.3597,  1.3885,  1.7208, -1.2032, -0.1463, -0.8433,\n",
      "        -0.7312,  1.6046, -2.0138,  1.0933,  0.3635, -0.1562,  1.5733, -0.2705,\n",
      "        -1.2121, -0.5703, -0.1859,  0.0073, -0.3320,  0.0725,  0.2955, -0.7593,\n",
      "        -1.5914,  0.5759,  1.1008, -0.7336,  0.6374,  0.1168, -1.5529, -1.5339,\n",
      "         1.2558,  0.4860, -1.1212, -1.1166, -0.7516,  1.7053,  1.0611, -1.9554,\n",
      "        -1.5363, -2.0087,  0.8257, -0.6929,  1.0686, -1.7412,  0.7426, -1.7062,\n",
      "        -0.8026,  1.1151, -1.2112,  1.6808,  1.0769,  1.7275,  1.2334, -1.6729,\n",
      "         0.9011, -1.2357,  0.3480,  0.0237, -1.2215, -0.0702, -1.6433, -0.0330,\n",
      "        -1.3638,  1.6171, -0.4937,  0.5458,  0.6017, -0.2107,  1.6971,  0.0319,\n",
      "         0.4902,  1.3563,  0.5812, -0.0860, -1.3850,  0.4749, -1.6107,  0.3498,\n",
      "         1.6265, -0.0594,  1.4850, -0.4271,  0.4141,  1.5199,  0.9841,  1.4477,\n",
      "        -1.6446, -0.7305,  1.1113, -1.3593,  1.3684,  1.7597, -1.5155,  1.3034,\n",
      "        -0.7636, -1.0487,  1.7421,  0.5703,  0.4478,  1.3780,  0.1061, -0.3068,\n",
      "        -0.6152, -1.7480,  1.2195,  1.0768, -0.2231, -0.0413, -0.8166,  1.2656,\n",
      "         1.2032, -0.1842,  0.7027,  0.9416, -1.7029, -1.3836,  0.0033,  0.0324,\n",
      "         1.3969,  1.0806, -0.2351,  1.4891,  1.0973,  1.3851, -1.0574, -1.1769,\n",
      "        -1.5053, -0.7595, -0.5838, -2.1122,  1.2101,  0.1377, -1.6425,  1.0670,\n",
      "        -0.0542, -0.2436, -0.9437,  0.2207,  1.0251,  0.5105,  0.2218, -1.5015,\n",
      "        -1.0929,  0.7918, -1.8841, -0.8797, -1.3020, -0.0050, -0.1278, -1.8188,\n",
      "         0.4661, -0.3899,  0.6054,  0.2576, -0.9753,  0.1138, -0.1273,  0.5054,\n",
      "        -1.9837, -0.6214, -0.2703, -0.4285, -0.9202, -1.8610, -1.5826,  0.7955,\n",
      "         1.3411,  1.5950, -1.1925, -1.8503,  1.8087, -1.1648,  0.6348,  0.7067],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
      "       device='cuda:0')\n",
      "name layer2.weight \n",
      "\n",
      "\n",
      " model tensor([[-0.0634, -0.0549,  0.0018,  ...,  0.0458, -0.0822, -0.0203],\n",
      "        [ 0.0536,  0.0359,  0.0015,  ..., -0.0115, -0.0132,  0.0060],\n",
      "        [-0.0662, -0.0451,  0.0247,  ...,  0.0570,  0.0432, -0.0235],\n",
      "        ...,\n",
      "        [ 0.0251, -0.0267, -0.0191,  ..., -0.0429, -0.0456,  0.0241],\n",
      "        [ 0.1043, -0.0145,  0.0097,  ..., -0.0637, -0.0198,  0.0129],\n",
      "        [ 0.0114,  0.0589,  0.0012,  ...,  0.0182,  0.0218, -0.0614]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[-0.2534, -0.2197,  0.0070,  ...,  0.1831, -0.3286, -0.0811],\n",
      "        [ 0.2145,  0.1437,  0.0058,  ..., -0.0459, -0.0527,  0.0238],\n",
      "        [-0.2646, -0.1803,  0.0989,  ...,  0.2280,  0.1728, -0.0941],\n",
      "        ...,\n",
      "        [ 0.1004, -0.1066, -0.0764,  ..., -0.1718, -0.1823,  0.0963],\n",
      "        [ 0.4172, -0.0581,  0.0387,  ..., -0.2550, -0.0793,  0.0516],\n",
      "        [ 0.0455,  0.2356,  0.0048,  ...,  0.0728,  0.0871, -0.2454]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer2.bias \n",
      "\n",
      "\n",
      " model tensor([ 6.6205e-03, -3.2377e-02, -4.5958e-02,  6.1730e-02,  5.1142e-02,\n",
      "        -1.7462e-02, -5.4124e-02, -1.1232e-02,  5.8617e-02, -6.6676e-02,\n",
      "         3.8840e-02, -4.0419e-02,  9.2222e-03, -6.6494e-02, -4.6496e-02,\n",
      "        -1.2460e-02, -6.5301e-04, -6.5983e-02,  2.2284e-02, -2.4886e-02,\n",
      "         4.2531e-02,  8.4915e-02,  5.9802e-02, -3.4652e-03, -2.8837e-02,\n",
      "         6.0173e-02, -6.3772e-02, -8.8647e-02, -3.2353e-02,  1.2800e-01,\n",
      "         7.2008e-02,  8.0851e-02, -4.0117e-02,  2.5901e-02,  3.5606e-02,\n",
      "        -7.7808e-02,  3.5788e-03, -2.1756e-02, -1.2306e-01,  4.8123e-02,\n",
      "         1.1054e-02, -7.2002e-02,  7.8845e-02,  7.1751e-03,  3.9806e-02,\n",
      "         2.8828e-02, -3.6909e-02,  5.8479e-02, -7.0202e-02,  5.6132e-02,\n",
      "        -3.1911e-02, -1.9451e-02,  1.2588e-02, -4.9605e-02, -2.6415e-02,\n",
      "         5.0884e-02,  4.3441e-02, -1.1917e-02, -3.6576e-02,  5.1420e-02,\n",
      "        -8.2422e-02,  7.8674e-02, -3.3395e-02, -1.1349e-02,  3.8636e-03,\n",
      "        -3.9502e-03,  4.0672e-02, -2.6127e-02,  2.6751e-02, -2.2839e-02,\n",
      "        -3.9352e-02,  2.5426e-02,  3.9454e-02, -4.3014e-02, -6.4135e-02,\n",
      "         2.4835e-02, -1.9392e-02,  4.7930e-02,  5.5934e-02,  2.8320e-02,\n",
      "        -8.6030e-02,  1.4140e-02,  2.4989e-02,  4.3506e-02, -4.9501e-02,\n",
      "         3.8060e-02, -2.3130e-02, -5.5456e-02,  7.4137e-02,  6.9622e-02,\n",
      "        -5.7255e-02, -1.2605e-02, -3.3073e-02, -5.0543e-02, -2.6859e-02,\n",
      "         2.7225e-02, -3.9992e-02, -1.5378e-01,  4.2800e-02, -1.3810e-04,\n",
      "         4.8996e-02, -7.1443e-02, -1.6780e-02, -1.1945e-02, -7.4407e-02,\n",
      "        -3.6141e-02, -1.2170e-02, -5.0433e-02, -7.2944e-02, -1.8268e-02,\n",
      "        -8.9378e-02,  7.2016e-03,  5.4560e-02, -5.7141e-02,  5.8410e-03,\n",
      "         7.8669e-02,  4.9474e-02, -2.1693e-02, -8.4660e-02,  5.1386e-02,\n",
      "         8.8519e-03,  4.5689e-02, -2.3721e-02,  4.9863e-02,  3.7764e-03,\n",
      "         4.1239e-02,  1.1756e-01,  2.4989e-02, -1.7260e-02, -7.1427e-02,\n",
      "        -7.9698e-02, -7.5401e-02,  7.6057e-02,  4.5944e-02,  1.5109e-02,\n",
      "        -4.0553e-02,  5.7381e-02,  2.7081e-02, -1.4482e-02, -7.1393e-02,\n",
      "        -1.1495e-02, -2.8827e-02, -5.9305e-02, -9.3016e-02, -8.8831e-02,\n",
      "         5.9434e-02, -4.1125e-03, -9.1109e-02, -9.7782e-02,  6.5756e-02,\n",
      "        -6.5883e-02, -6.4734e-02,  2.9286e-05, -7.8842e-02,  2.7385e-02,\n",
      "        -3.7701e-02,  3.7715e-02,  9.7803e-02, -5.0331e-02, -6.7174e-02,\n",
      "        -1.5403e-02, -1.3481e-02,  6.7517e-02,  6.1399e-02, -8.1103e-02,\n",
      "        -1.1418e-01,  7.5766e-02,  4.6393e-03,  6.1258e-02, -1.7248e-02,\n",
      "         6.2396e-02, -3.1449e-02,  2.7584e-02,  3.5604e-02, -7.7220e-02,\n",
      "        -6.0437e-02, -3.1787e-02,  4.3068e-02,  6.6608e-02,  5.1794e-02,\n",
      "         6.3146e-02, -8.5386e-02, -7.8354e-02,  4.2019e-02, -7.6459e-02,\n",
      "         6.8323e-02,  9.4791e-02, -6.3084e-02, -1.5255e-03,  1.0377e-02,\n",
      "         6.8170e-02, -5.7751e-02,  1.0723e-02,  7.7447e-02, -9.2818e-03,\n",
      "        -9.2200e-02, -5.3574e-02, -5.7750e-02, -1.9773e-02, -8.3680e-02,\n",
      "        -6.3854e-03, -3.3826e-02, -5.6497e-02, -2.6097e-02, -5.3209e-02,\n",
      "         1.5681e-03, -1.8333e-02,  5.8289e-02, -8.5990e-02,  6.7770e-02,\n",
      "        -1.0025e-01,  2.6034e-02,  3.0845e-02, -4.5089e-02, -2.0955e-03,\n",
      "         1.6112e-02,  5.3509e-02, -3.8658e-03,  9.3778e-02, -6.5697e-02,\n",
      "        -2.8214e-02,  3.9072e-02, -2.7483e-02,  7.7915e-02, -1.8103e-02,\n",
      "        -2.3917e-02, -5.3242e-03,  6.8477e-02,  7.3193e-02,  4.7106e-02,\n",
      "        -6.8053e-02,  5.2855e-02,  7.4291e-02,  2.1017e-02, -6.6047e-02,\n",
      "        -1.9512e-02,  5.4492e-02,  1.8015e-02,  4.1885e-02,  1.7024e-02,\n",
      "        -8.1572e-02,  9.6217e-03, -1.0126e-01, -5.6150e-02, -8.1864e-02,\n",
      "        -5.3142e-02,  4.3732e-02, -4.9532e-03, -5.4868e-02, -8.3943e-02,\n",
      "         3.6248e-02, -2.0443e-02,  4.5764e-02,  2.3292e-02,  9.1441e-02,\n",
      "        -4.6304e-02], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([ 2.6482e-02, -1.2951e-01, -1.8383e-01,  2.4692e-01,  2.0457e-01,\n",
      "        -6.9849e-02, -2.1649e-01, -4.4930e-02,  2.3447e-01, -2.6670e-01,\n",
      "         1.5536e-01, -1.6168e-01,  3.6889e-02, -2.6598e-01, -1.8598e-01,\n",
      "        -4.9841e-02, -2.6121e-03, -2.6393e-01,  8.9138e-02, -9.9545e-02,\n",
      "         1.7012e-01,  3.3966e-01,  2.3921e-01, -1.3861e-02, -1.1535e-01,\n",
      "         2.4069e-01, -2.5509e-01, -3.5459e-01, -1.2941e-01,  5.1200e-01,\n",
      "         2.8803e-01,  3.2340e-01, -1.6047e-01,  1.0360e-01,  1.4243e-01,\n",
      "        -3.1123e-01,  1.4315e-02, -8.7023e-02, -4.9225e-01,  1.9249e-01,\n",
      "         4.4216e-02, -2.8801e-01,  3.1538e-01,  2.8700e-02,  1.5922e-01,\n",
      "         1.1531e-01, -1.4764e-01,  2.3392e-01, -2.8081e-01,  2.2453e-01,\n",
      "        -1.2764e-01, -7.7804e-02,  5.0351e-02, -1.9842e-01, -1.0566e-01,\n",
      "         2.0354e-01,  1.7377e-01, -4.7669e-02, -1.4630e-01,  2.0568e-01,\n",
      "        -3.2969e-01,  3.1470e-01, -1.3358e-01, -4.5395e-02,  1.5454e-02,\n",
      "        -1.5801e-02,  1.6269e-01, -1.0451e-01,  1.0700e-01, -9.1358e-02,\n",
      "        -1.5741e-01,  1.0170e-01,  1.5782e-01, -1.7206e-01, -2.5654e-01,\n",
      "         9.9338e-02, -7.7569e-02,  1.9172e-01,  2.2374e-01,  1.1328e-01,\n",
      "        -3.4412e-01,  5.6559e-02,  9.9956e-02,  1.7403e-01, -1.9800e-01,\n",
      "         1.5224e-01, -9.2521e-02, -2.2183e-01,  2.9655e-01,  2.7849e-01,\n",
      "        -2.2902e-01, -5.0421e-02, -1.3229e-01, -2.0217e-01, -1.0744e-01,\n",
      "         1.0890e-01, -1.5997e-01, -6.1513e-01,  1.7120e-01, -5.5240e-04,\n",
      "         1.9598e-01, -2.8577e-01, -6.7118e-02, -4.7782e-02, -2.9763e-01,\n",
      "        -1.4456e-01, -4.8680e-02, -2.0173e-01, -2.9177e-01, -7.3074e-02,\n",
      "        -3.5751e-01,  2.8807e-02,  2.1824e-01, -2.2856e-01,  2.3364e-02,\n",
      "         3.1468e-01,  1.9790e-01, -8.6772e-02, -3.3864e-01,  2.0554e-01,\n",
      "         3.5408e-02,  1.8276e-01, -9.4883e-02,  1.9945e-01,  1.5106e-02,\n",
      "         1.6495e-01,  4.7023e-01,  9.9957e-02, -6.9039e-02, -2.8571e-01,\n",
      "        -3.1879e-01, -3.0160e-01,  3.0423e-01,  1.8378e-01,  6.0437e-02,\n",
      "        -1.6221e-01,  2.2952e-01,  1.0832e-01, -5.7927e-02, -2.8557e-01,\n",
      "        -4.5980e-02, -1.1531e-01, -2.3722e-01, -3.7206e-01, -3.5532e-01,\n",
      "         2.3774e-01, -1.6450e-02, -3.6444e-01, -3.9113e-01,  2.6302e-01,\n",
      "        -2.6353e-01, -2.5894e-01,  1.1714e-04, -3.1537e-01,  1.0954e-01,\n",
      "        -1.5080e-01,  1.5086e-01,  3.9121e-01, -2.0133e-01, -2.6870e-01,\n",
      "        -6.1614e-02, -5.3923e-02,  2.7007e-01,  2.4559e-01, -3.2441e-01,\n",
      "        -4.5673e-01,  3.0306e-01,  1.8557e-02,  2.4503e-01, -6.8993e-02,\n",
      "         2.4959e-01, -1.2579e-01,  1.1033e-01,  1.4242e-01, -3.0888e-01,\n",
      "        -2.4175e-01, -1.2715e-01,  1.7227e-01,  2.6643e-01,  2.0718e-01,\n",
      "         2.5258e-01, -3.4154e-01, -3.1342e-01,  1.6808e-01, -3.0583e-01,\n",
      "         2.7329e-01,  3.7916e-01, -2.5233e-01, -6.1019e-03,  4.1509e-02,\n",
      "         2.7268e-01, -2.3100e-01,  4.2892e-02,  3.0979e-01, -3.7127e-02,\n",
      "        -3.6880e-01, -2.1429e-01, -2.3100e-01, -7.9094e-02, -3.3472e-01,\n",
      "        -2.5542e-02, -1.3530e-01, -2.2599e-01, -1.0439e-01, -2.1284e-01,\n",
      "         6.2723e-03, -7.3330e-02,  2.3316e-01, -3.4396e-01,  2.7108e-01,\n",
      "        -4.0098e-01,  1.0413e-01,  1.2338e-01, -1.8036e-01, -8.3819e-03,\n",
      "         6.4446e-02,  2.1404e-01, -1.5463e-02,  3.7511e-01, -2.6279e-01,\n",
      "        -1.1286e-01,  1.5629e-01, -1.0993e-01,  3.1166e-01, -7.2410e-02,\n",
      "        -9.5668e-02, -2.1297e-02,  2.7391e-01,  2.9277e-01,  1.8842e-01,\n",
      "        -2.7221e-01,  2.1142e-01,  2.9716e-01,  8.4069e-02, -2.6419e-01,\n",
      "        -7.8046e-02,  2.1797e-01,  7.2058e-02,  1.6754e-01,  6.8098e-02,\n",
      "        -3.2629e-01,  3.8487e-02, -4.0502e-01, -2.2460e-01, -3.2746e-01,\n",
      "        -2.1257e-01,  1.7493e-01, -1.9813e-02, -2.1947e-01, -3.3577e-01,\n",
      "         1.4499e-01, -8.1772e-02,  1.8306e-01,  9.3166e-02,  3.6577e-01,\n",
      "        -1.8522e-01], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name layer3.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.0534,  0.0394, -0.0593,  ...,  0.0331, -0.0660, -0.1512],\n",
      "        [-0.0707,  0.0280,  0.0912,  ...,  0.0649,  0.0359, -0.0095],\n",
      "        [ 0.0057, -0.0846,  0.0573,  ..., -0.0995,  0.0775,  0.1348],\n",
      "        ...,\n",
      "        [ 0.0561, -0.0423, -0.0047,  ...,  0.0799,  0.0327,  0.1121],\n",
      "        [ 0.0439, -0.1047,  0.0781,  ..., -0.0373,  0.0304, -0.0388],\n",
      "        [ 0.1122, -0.0937,  0.0110,  ..., -0.0770,  0.0039, -0.0645]],\n",
      "       device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.2138,  0.1574, -0.2373,  ...,  0.1324, -0.2639, -0.6049],\n",
      "        [-0.2829,  0.1122,  0.3646,  ...,  0.2595,  0.1435, -0.0381],\n",
      "        [ 0.0226, -0.3384,  0.2291,  ..., -0.3981,  0.3100,  0.5393],\n",
      "        ...,\n",
      "        [ 0.2245, -0.1691, -0.0189,  ...,  0.3196,  0.1307,  0.4484],\n",
      "        [ 0.1756, -0.4190,  0.3125,  ..., -0.1493,  0.1214, -0.1554],\n",
      "        [ 0.4488, -0.3749,  0.0442,  ..., -0.3079,  0.0156, -0.2579]],\n",
      "       device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "       device='cuda:0')\n",
      "name layer3.bias \n",
      "\n",
      "\n",
      " model tensor([-0.0311,  0.0045, -0.0389,  0.0644, -0.0296, -0.1006, -0.0116, -0.0130,\n",
      "         0.1361, -0.0008,  0.0166, -0.0535,  0.1439, -0.0524,  0.1105, -0.0671,\n",
      "        -0.0602,  0.0411,  0.1518, -0.0128,  0.1021,  0.0380, -0.0618, -0.0268,\n",
      "         0.1222,  0.0266, -0.0341,  0.0175, -0.0549,  0.0562, -0.0313, -0.0062,\n",
      "         0.0635,  0.1021,  0.1544,  0.0332, -0.1124,  0.0177, -0.0699, -0.0127,\n",
      "        -0.0395,  0.0166, -0.0346,  0.0803,  0.0936,  0.0017,  0.0595, -0.0253,\n",
      "         0.0528,  0.1045, -0.1606,  0.0272,  0.0040, -0.0977, -0.0350, -0.0354,\n",
      "         0.0981, -0.0085, -0.0996, -0.0105], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([-0.1243,  0.0182, -0.1558,  0.2575, -0.1185, -0.4024, -0.0465, -0.0521,\n",
      "         0.5445, -0.0032,  0.0662, -0.2139,  0.5755, -0.2096,  0.4419, -0.2682,\n",
      "        -0.2410,  0.1645,  0.6072, -0.0513,  0.4084,  0.1520, -0.2472, -0.1071,\n",
      "         0.4886,  0.1063, -0.1363,  0.0700, -0.2195,  0.2248, -0.1250, -0.0247,\n",
      "         0.2540,  0.4083,  0.6175,  0.1328, -0.4495,  0.0709, -0.2794, -0.0508,\n",
      "        -0.1581,  0.0663, -0.1384,  0.3213,  0.3742,  0.0067,  0.2382, -0.1013,\n",
      "         0.2114,  0.4180, -0.6422,  0.1089,  0.0159, -0.3908, -0.1400, -0.1415,\n",
      "         0.3922, -0.0339, -0.3983, -0.0422], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')\n",
      "name output.weight \n",
      "\n",
      "\n",
      " model tensor([[ 0.1876,  0.1211, -0.1839, -0.2392, -0.1537,  0.0930,  0.1808, -0.2183,\n",
      "         -0.2477, -0.2111,  0.2171,  0.1873,  0.1251,  0.1397,  0.1093, -0.2261,\n",
      "         -0.2495, -0.2636, -0.2681,  0.1593, -0.0734, -0.0200, -0.1795, -0.1791,\n",
      "         -0.0630,  0.2294, -0.1460, -0.0683,  0.1325,  0.2676, -0.1196,  0.1995,\n",
      "          0.2875,  0.1687,  0.2431, -0.4945,  0.0042,  0.1023, -0.2416,  0.0180,\n",
      "         -0.1167, -0.2004,  0.1393,  0.2139, -0.2647, -0.1645, -0.1790,  0.0096,\n",
      "         -0.0730, -0.1452,  0.1573, -0.1535,  0.1173,  0.1330,  0.0780,  0.2460,\n",
      "          0.3555, -0.1045, -0.1032, -0.0840]], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([[ 0.7504,  0.4845, -0.7358, -0.9569, -0.6148,  0.3719,  0.7232, -0.8732,\n",
      "         -0.9908, -0.8445,  0.8682,  0.7491,  0.5004,  0.5587,  0.4372, -0.9043,\n",
      "         -0.9979, -1.0542, -1.0723,  0.6371, -0.2937, -0.0801, -0.7181, -0.7164,\n",
      "         -0.2519,  0.9175, -0.5839, -0.2734,  0.5301,  1.0703, -0.4784,  0.7978,\n",
      "          1.1499,  0.6748,  0.9723, -1.9778,  0.0166,  0.4091, -0.9665,  0.0721,\n",
      "         -0.4668, -0.8014,  0.5574,  0.8556, -1.0587, -0.6582, -0.7159,  0.0384,\n",
      "         -0.2919, -0.5807,  0.6291, -0.6138,  0.4693,  0.5319,  0.3122,  0.9840,\n",
      "          1.4222, -0.4179, -0.4128, -0.3359]], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')\n",
      "name output.bias \n",
      "\n",
      "\n",
      " model tensor([0.2638], device='cuda:0')\n",
      "\n",
      " sum_params[name] tensor([1.0551], device='cuda:0')\n",
      "\n",
      " model[name]  /= sum_params[name] tensor([0.2500], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FairFed \"FF\" and FedAVG \"FA\"\n",
    "\n",
    "algo=\"FF\" \n",
    "\n",
    "if algo==\"FA\":\n",
    "    print(\"Comment out part1 and part2 def_fairFed \")\n",
    "    \n",
    "else:\n",
    "    print(\"Algorithm is \", algo)\n",
    "    \n",
    "global_model_base_path = \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/models\"\n",
    "shutil.rmtree(global_model_base_path)\n",
    "os.mkdir(global_model_base_path)\n",
    "\n",
    "selected_clients = [0, 1, 2, 3]\n",
    "\n",
    "assigner = AssignModel(global_model_base_path, selected_clients,algo)\n",
    "\n",
    "temp_model = DeepNet()  # Replace with your model instantiation\n",
    "assigner.save_global_model(temp_model)\n",
    "assigner.save_client_models(temp_model)\n",
    "\n",
    "\n",
    "model = DeepNet()\n",
    "server = Serverbase(model)\n",
    "server.fedAlgo(num_rounds=1,local_epochs=1,learning_rate = 0.001, beta = 1,algo=algo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b8580-35d9-4889-9a91-9e6fe5c52a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77244b-4b61-4dee-b028-991dbaac5299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696f369-f476-4ff5-ba94-0f44bf35833e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6226f-dea7-4c5d-8e72-8938b57a11f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d090012-28e5-4b94-80ea-b4e348064e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9c6b0-6dc8-44be-971c-47462a1d4136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "047ff52d-72c5-4992-a4ae-a951fdd65c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of weights across all models: -12.481867059133947\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create instances of your DeepNet model\n",
    "model1 = DeepNet()\n",
    "model2 = DeepNet()\n",
    "model3 = DeepNet()\n",
    "\n",
    "# Combine the models into a list\n",
    "models = [model1, model2, model3]\n",
    "\n",
    "# Initialize a variable to store the sum of weights\n",
    "total_weights = 0\n",
    "\n",
    "# Loop through the models and sum their parameters\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        total_weights += param.sum().item()\n",
    "\n",
    "print(\"Sum of weights across all models:\", total_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c73ed-150f-4475-a231-d480907cf0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
