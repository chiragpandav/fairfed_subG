{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae9b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import glob\n",
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from folktables import ACSDataSource, ACSEmployment,ACSIncome\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "import torch, random, copy, os\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d23e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8550ad",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70374e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################## MODEL SETTING ########################\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "#########################################################\n",
    "\n",
    "# class LoadData(Dataset):\n",
    "#     def __init__(self, df, pred_var, sen_var):\n",
    "#         self.y = df[pred_var].values\n",
    "#         self.x = df.drop(pred_var, axis = 1).values\n",
    "#         self.sen = df[sen_var].values\n",
    "\n",
    "#         print(\"self.x\",self.x)\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         return torch.tensor(self.x[index]), torch.tensor(self.y[index]), torch.tensor(self.sen[index])\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.y.shape[0]\n",
    "\n",
    "\n",
    "class LoadData(Dataset):\n",
    "    def __init__(self, df, pred_var, sen_var):\n",
    "        self.y = df[pred_var].values.astype(np.float32)  # Convert to float32\n",
    "        self.x = df.drop(pred_var, axis=1).values.astype(np.float32)  # Convert to float32\n",
    "        self.sen = df[sen_var].values.astype(np.float32)  # Convert to float32\n",
    "\n",
    "        print(\"self.x\", self.x)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index]), torch.tensor(self.y[index]), torch.tensor(self.sen[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"\n",
    "    An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "        self.x = self.dataset.x[self.idxs]\n",
    "        self.y = self.dataset.y[self.idxs]\n",
    "        self.sen = self.dataset.sen[self.idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        feature, label, sensitive = self.dataset[self.idxs[item]]\n",
    "        return feature, label, sensitive\n",
    "        # return self.x[item], self.y[item], self.sen[item]\n",
    "    \n",
    "class logReg(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Logistic regression model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_classes, seed = 123):\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x.float())\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return probas.type(torch.FloatTensor), logits\n",
    "\n",
    "class mlp(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Logistic regression model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_classes, seed = 123):\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.linear1 = torch.nn.Linear(num_features, 4)\n",
    "        self.linear2 = torch.nn.Linear(4, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x.float())\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        probas = torch.sigmoid(out)\n",
    "        return probas.type(torch.FloatTensor), out\n",
    "\n",
    "def logit_compute(probas):\n",
    "    return torch.log(probas/(1-probas))\n",
    "    \n",
    "def riskDifference(n_yz, absolute = True):\n",
    "    \"\"\"\n",
    "    Given a dictionary of number of samples in different groups, compute the risk difference.\n",
    "    |P(Group1, pos) - P(Group2, pos)| = |N(Group1, pos)/N(Group1) - N(Group2, pos)/N(Group2)|\n",
    "    \"\"\"\n",
    "    n_z1 = max(n_yz[(1,1)] + n_yz[(0,1)], 1)\n",
    "    n_z0 = max(n_yz[(0,0)] + n_yz[(1,0)], 1)\n",
    "    if absolute:\n",
    "        return abs(n_yz[(1,1)]/n_z1 - n_yz[(1,0)]/n_z0)\n",
    "    else:\n",
    "        return n_yz[(1,1)]/n_z1 - n_yz[(1,0)]/n_z0\n",
    "\n",
    "def pRule(n_yz):\n",
    "    \"\"\"\n",
    "    Compute the p rule level.\n",
    "    min(P(Group1, pos)/P(Group2, pos), P(Group2, pos)/P(Group1, pos))\n",
    "    \"\"\"\n",
    "    return min(n_yz[(1,1)]/n_yz[(1,0)], n_yz[(1,0)]/n_yz[(1,1)])\n",
    "\n",
    "def DPDisparity(n_yz, each_z = False):\n",
    "    \"\"\"\n",
    "    Same metric as FairBatch. Compute the demographic disparity.\n",
    "    max(|P(pos | Group1) - P(pos)|, |P(pos | Group2) - P(pos)|)\n",
    "    \"\"\"\n",
    "    z_set = sorted(list(set([z for _, z in n_yz.keys()])))\n",
    "    p_y1_n, p_y1_d, n_z = 0, 0, []\n",
    "    for z in z_set:\n",
    "        p_y1_n += n_yz[(1,z)]\n",
    "        n_z.append(max(n_yz[(1,z)] + n_yz[(0,z)], 1))\n",
    "        for y in [0,1]:\n",
    "            p_y1_d += n_yz[(y,z)]\n",
    "    p_y1 = p_y1_n / p_y1_d\n",
    "\n",
    "    if not each_z:\n",
    "        return max([abs(n_yz[(1,z)]/n_z[z] - p_y1) for z in z_set])\n",
    "    else:\n",
    "        return [n_yz[(1,z)]/n_z[z] - p_y1 for z in z_set]\n",
    "\n",
    "def EODisparity(n_eyz, each_z = False):\n",
    "    \"\"\"\n",
    "    Equal opportunity disparity: max_z{|P(yhat=1|z=z,y=1)-P(yhat=1|y=1)|}\n",
    "\n",
    "    Parameter:\n",
    "    n_eyz: dictionary. #(yhat=e,y=y,z=z)\n",
    "    \"\"\"\n",
    "    z_set = list(set([z for _,_, z in n_eyz.keys()]))\n",
    "    if not each_z:\n",
    "        eod = 0\n",
    "        p11 = sum([n_eyz[(1,1,z)] for z in z_set]) / sum([n_eyz[(1,1,z)]+n_eyz[(0,1,z)] for z in z_set])\n",
    "        for z in z_set:\n",
    "            try:\n",
    "                eod_z = abs(n_eyz[(1,1,z)]/(n_eyz[(0,1,z)] + n_eyz[(1,1,z)]) - p11)\n",
    "            except ZeroDivisionError:\n",
    "                if n_eyz[(1,1,z)] == 0: \n",
    "                    eod_z = 0\n",
    "                else:\n",
    "                    eod_z = 1\n",
    "            if eod < eod_z:\n",
    "                eod = eod_z\n",
    "        return eod\n",
    "    else:\n",
    "        eod = []\n",
    "        p11 = sum([n_eyz[(1,1,z)] for z in z_set]) / sum([n_eyz[(1,1,z)]+n_eyz[(0,1,z)] for z in z_set])\n",
    "        for z in z_set:\n",
    "            try:\n",
    "                eod_z = n_eyz[(1,1,z)]/(n_eyz[(0,1,z)] + n_eyz[(1,1,z)]) - p11\n",
    "            except ZeroDivisionError:\n",
    "                if n_eyz[(1,1,z)] == 0: \n",
    "                    eod_z = 0\n",
    "                else:\n",
    "                    eod_z = 1\n",
    "            eod.append(eod_z)\n",
    "        return eod\n",
    "\n",
    "def RepresentationDisparity(loss_z):\n",
    "    return max(loss_z) - min(loss_z)\n",
    "\n",
    "def accVariance(acc_z):\n",
    "    return np.std(acc_z)\n",
    "\n",
    "# def mutual_information(n_yz, u = 0):\n",
    "#     # u = 0 : demographic parity \n",
    "\n",
    "def average_weights(w, clients_idx, idx_users):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    num_samples = 0\n",
    "    for i in range(1, len(w)):\n",
    "        num_samples += len(clients_idx[idx_users[i]])\n",
    "        for key in w_avg.keys():            \n",
    "            w_avg[key] += w[i][key] * len(clients_idx[idx_users[i]])\n",
    "        \n",
    "    for key in w_avg.keys(): \n",
    "        w_avg[key] = torch.div(w_avg[key], num_samples)\n",
    "    return w_avg\n",
    "\n",
    "def weighted_average_weights(w, nc, n):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for i in range(1, len(w)):\n",
    "        for key in w_avg.keys():            \n",
    "            w_avg[key] += w[i][key] * nc[i]\n",
    "        \n",
    "    for key in w_avg.keys(): \n",
    "        w_avg[key] = torch.div(w_avg[key], n)\n",
    "    return w_avg\n",
    "\n",
    "def loss_func(option, logits, targets, outputs, sensitive, larg = 1):\n",
    "    \"\"\"\n",
    "    Loss function. \n",
    "    \"\"\"\n",
    "\n",
    "    acc_loss = F.cross_entropy(logits, targets, reduction = 'sum')\n",
    "    fair_loss0 = torch.mul(sensitive - sensitive.type(torch.FloatTensor).mean(), logits.T[0] - torch.mean(logits.T[0]))\n",
    "    fair_loss0 = torch.mean(torch.mul(fair_loss0, fair_loss0)) \n",
    "    fair_loss1 = torch.mul(sensitive - sensitive.type(torch.FloatTensor).mean(), logits.T[1] - torch.mean(logits.T[1]))\n",
    "    fair_loss1 = torch.mean(torch.mul(fair_loss1, fair_loss1)) \n",
    "    fair_loss = fair_loss0 + fair_loss1\n",
    "\n",
    "    if option == 'local zafar':\n",
    "        return acc_loss + larg*fair_loss, acc_loss, larg*fair_loss\n",
    "    elif option == 'FB_inference':\n",
    "        # acc_loss = torch.sum(torch.nn.BCELoss(reduction = 'none')((outputs.T[1]+1)/2, torch.ones(logits.shape[0])))\n",
    "        acc_loss = F.cross_entropy(logits, torch.ones(logits.shape[0]).type(torch.LongTensor).to(DEVICE), reduction = 'sum')\n",
    "        return acc_loss, acc_loss, fair_loss\n",
    "    else:\n",
    "        return acc_loss, acc_loss, larg*fair_loss\n",
    "\n",
    "def eo_loss(logits, targets, sensitive, larg, mean_z1 = None, left = None, option = 'local fc'):\n",
    "    acc_loss = F.cross_entropy(logits, targets, reduction = 'sum')\n",
    "    y1_idx = torch.where(targets == 1)\n",
    "    if option == 'unconstrained':\n",
    "        return acc_loss\n",
    "    if left:\n",
    "        fair_loss = torch.mean(torch.mul(sensitive[y1_idx] - mean_z1, logits.T[0][y1_idx] - torch.mean(logits.T[0][y1_idx])))\n",
    "        return acc_loss - larg * fair_loss\n",
    "    elif left == False: \n",
    "        fair_loss = torch.mean(torch.mul(sensitive[y1_idx] - mean_z1, logits.T[0][y1_idx] - torch.mean(logits.T[0][y1_idx])))\n",
    "        return acc_loss + larg * fair_loss\n",
    "    else:\n",
    "        fair_loss0 = torch.mul(sensitive[y1_idx] - sensitive.type(torch.FloatTensor).mean(), logits.T[0][y1_idx] - torch.mean(logits.T[0][y1_idx]))\n",
    "        fair_loss0 = torch.mean(torch.mul(fair_loss0, fair_loss0)) \n",
    "        fair_loss1 = torch.mul(sensitive[y1_idx] - sensitive.type(torch.FloatTensor).mean(), logits.T[1][y1_idx] - torch.mean(logits.T[1][y1_idx]))\n",
    "        fair_loss1 = torch.mean(torch.mul(fair_loss1, fair_loss1)) \n",
    "        fair_loss = fair_loss0 + fair_loss1\n",
    "        return acc_loss + larg * fair_loss\n",
    "\n",
    "def zafar_loss(logits, targets, outputs, sensitive, larg, mean_z, left):\n",
    "    acc_loss = F.cross_entropy(logits, targets, reduction = 'sum')\n",
    "    fair_loss =  torch.mean(torch.mul(sensitive - mean_z, logits.T[0] - torch.mean(logits.T[0])))\n",
    "    if left:\n",
    "        return acc_loss - larg * fair_loss\n",
    "    else:\n",
    "        return acc_loss + larg * fair_loss\n",
    "\n",
    "def weighted_loss(logits, targets, weights, mean = True):\n",
    "    acc_loss = F.cross_entropy(logits, targets, reduction = 'none')\n",
    "    if mean:\n",
    "        weights_sum = weights.sum().item()\n",
    "        acc_loss = torch.sum(acc_loss * weights / weights_sum)\n",
    "    else:\n",
    "        acc_loss = torch.sum(acc_loss * weights)\n",
    "    return acc_loss\n",
    "    \n",
    "def al_loss(logits, targets, adv_logits, adv_targets):\n",
    "    acc_loss = F.cross_entropy(logits, targets, reduction = 'sum')\n",
    "    adv_loss = F.cross_entropy(adv_logits, adv_targets)\n",
    "    return acc_loss, adv_loss\n",
    "\n",
    "def mtl_loss(logits, targets, penalty, global_model, model):\n",
    "    penalty_term = torch.tensor(0., requires_grad=True).to(DEVICE)\n",
    "    for v, w in zip(model.parameters(), global_model.parameters()):\n",
    "        penalty_term = penalty_term + torch.norm(v-w) ** 2\n",
    "    # penalty_term = torch.nodem(v-global_weights, v-global_weights)\n",
    "    loss = F.cross_entropy(logits, targets, reduction = 'sum') + penalty /2 * penalty_term\n",
    "    return loss\n",
    "\n",
    "# copied from https://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays\n",
    "def cartesian(arrays, out=None):\n",
    "    \"\"\"\n",
    "    Generate a cartesian product of input arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : list of array-like\n",
    "        1-D arrays to form the cartesian product of.\n",
    "    out : ndarray\n",
    "        Array to place the cartesian product in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        2-D array of shape (M, len(arrays)) containing cartesian products\n",
    "        formed of input arrays.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n",
    "    array([[1, 4, 6],\n",
    "           [1, 4, 7],\n",
    "           [1, 5, 6],\n",
    "           [1, 5, 7],\n",
    "           [2, 4, 6],\n",
    "           [2, 4, 7],\n",
    "           [2, 5, 6],\n",
    "           [2, 5, 7],\n",
    "           [3, 4, 6],\n",
    "           [3, 4, 7],\n",
    "           [3, 5, 6],\n",
    "           [3, 5, 7]])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    arrays = [np.asarray(x) for x in arrays]\n",
    "    dtype = arrays[0].dtype\n",
    "\n",
    "    n = np.prod([x.size for x in arrays])\n",
    "    if out is None:\n",
    "        out = np.zeros([n, len(arrays)], dtype=dtype)\n",
    "\n",
    "    #m = n / arrays[0].size\n",
    "    m = int(n / arrays[0].size) \n",
    "    out[:,0] = np.repeat(arrays[0], m)\n",
    "    if arrays[1:]:\n",
    "        cartesian(arrays[1:], out=out[0:m, 1:])\n",
    "        for j in range(1, arrays[0].size):\n",
    "        #for j in xrange(1, arrays[0].size):\n",
    "            out[j*m:(j+1)*m, 1:] = out[0:m, 1:]\n",
    "    return out\n",
    "\n",
    "## Synthetic data generation ##\n",
    "########################\n",
    "####### setting ########\n",
    "########################\n",
    "X_DIST = {0:{\"mean\":(-2,-2), \"cov\":np.array([[10,1], [1,3]])}, \n",
    "     1:{\"mean\":(2,2), \"cov\":np.array([[5,1], [1,5]])}}\n",
    "\n",
    "def X_PRIME(x):\n",
    "    return (x[0]*np.cos(np.pi/4) - x[1]*np.sin(np.pi/4), \n",
    "            x[0]*np.sin(np.pi/4) + x[1]*np.cos(np.pi/4))\n",
    "\n",
    "def Z_MEAN(x):\n",
    "    \"\"\"\n",
    "    Given x, the probability of z = 1.\n",
    "    \"\"\"\n",
    "    x_transform = X_PRIME(x)\n",
    "    return multivariate_normal.pdf(x_transform, mean = X_DIST[1][\"mean\"], cov = X_DIST[1][\"cov\"])/(\n",
    "        multivariate_normal.pdf(x_transform, mean = X_DIST[1][\"mean\"], cov = X_DIST[1][\"cov\"]) + \n",
    "        multivariate_normal.pdf(x_transform, mean = X_DIST[0][\"mean\"], cov = X_DIST[0][\"cov\"])\n",
    "    )\n",
    "\n",
    "def dataSample(train_samples = 3000, test_samples = 500, \n",
    "                y_mean = 0.6, Z = 2):\n",
    "    num_samples = train_samples + test_samples\n",
    "    ys = np.random.binomial(n = 1, p = y_mean, size = num_samples)\n",
    "\n",
    "    xs, zs = [], []\n",
    "\n",
    "    if Z == 2:\n",
    "        for y in ys:\n",
    "            x = np.random.multivariate_normal(mean = X_DIST[y][\"mean\"], cov = X_DIST[y][\"cov\"], size = 1)[0]\n",
    "            z = np.random.binomial(n = 1, p = Z_MEAN(x), size = 1)[0]\n",
    "            xs.append(x)\n",
    "            zs.append(z)\n",
    "    elif Z == 3:\n",
    "        for y in ys:\n",
    "            x = np.random.multivariate_normal(mean = X_DIST[y][\"mean\"], cov = X_DIST[y][\"cov\"], size = 1)[0]\n",
    "            # Z = 3: 0.7 y = 1, 0.3 y = 1 + 0.3 y = 0, 0.7 y = 0\n",
    "            py1 = multivariate_normal.pdf(x, mean = X_DIST[1][\"mean\"], cov = X_DIST[1][\"cov\"])\n",
    "            py0 = multivariate_normal.pdf(x, mean = X_DIST[0][\"mean\"], cov = X_DIST[0][\"cov\"])\n",
    "            p = np.array([0.7 * py1, 0.3 * py1 + 0.3 * py0, 0.7 * py0]) / (py1+py0)\n",
    "            z = np.random.choice([0,1,2], size = 1, p = p)[0]\n",
    "            xs.append(x)\n",
    "            zs.append(z)\n",
    "\n",
    "    data = pd.DataFrame(zip(np.array(xs).T[0], np.array(xs).T[1], ys, zs), columns = [\"x1\", \"x2\", \"y\", \"z\"])\n",
    "    # data = data.sample(frac=1).reset_index(drop=True)\n",
    "    train_data = data[:train_samples]\n",
    "    test_data = data[train_samples:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def process_csv(dir_name, filename, label_name, favorable_class, sensitive_attributes, privileged_classes, categorical_attributes, continuous_attributes, features_to_keep, na_values = [], header = 'infer', columns = None):\n",
    "    \"\"\"\n",
    "    process the adult file: scale, one-hot encode\n",
    "    only support binary sensitive attributes -> [gender, race] -> 4 sensitive groups \n",
    "    \"\"\"\n",
    "    file_path = os.path.join(os.path.abspath(os.getcwd()), 'adult', 'adult.data')\n",
    "\n",
    "#     print(\"-----------print to fileName--------\", os.path.join('FedFB', dir_name, filename))\n",
    "\n",
    "#     df = pd.read_csv(os.path.join('FedFB', dir_name, filename), delimiter = ',', header = header, na_values = na_values)\n",
    "    \n",
    "    \n",
    "    print(\"-------------path-----------\", file_path)\n",
    "    df = pd.read_csv(file_path, delimiter = ',', header = header, na_values = na_values)\n",
    "    \n",
    "    if header == None: df.columns = columns\n",
    "    df = df[features_to_keep]\n",
    "\n",
    "    # apply one-hot encoding to convert the categorical attributes into vectors\n",
    "    df = pd.get_dummies(df, columns = categorical_attributes)\n",
    "\n",
    "    # normalize numerical attributes to the range within [0, 1]\n",
    "    def scale(vec):\n",
    "        minimum = min(vec)\n",
    "        maximum = max(vec)\n",
    "        return (vec-minimum)/(maximum-minimum)\n",
    "    \n",
    "    df[continuous_attributes] = df[continuous_attributes].apply(scale, axis = 0)\n",
    "    df.loc[df[label_name] != favorable_class, label_name] = 'SwapSwapSwap'\n",
    "    df.loc[df[label_name] == favorable_class, label_name] = 1\n",
    "    df.loc[df[label_name] == 'SwapSwapSwap', label_name] = 0\n",
    "    df[label_name] = df[label_name].astype('category').cat.codes\n",
    "    if len(sensitive_attributes) > 1:\n",
    "        if privileged_classes != None:\n",
    "            for i in range(len(sensitive_attributes)):\n",
    "                df.loc[df[sensitive_attributes[i]] != privileged_classes[i], sensitive_attributes[i]] = 0\n",
    "                df.loc[df[sensitive_attributes[i]] == privileged_classes[i], sensitive_attributes[i]] = 1\n",
    "        df['z'] = list(zip(*[df[c] for c in sensitive_attributes]))\n",
    "        df['z'] = df['z'].astype('category').cat.codes\n",
    "    else:\n",
    "        df['z'] = df[sensitive_attributes[0]].astype('category').cat.codes\n",
    "    df = df.drop(columns = sensitive_attributes)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7f234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f56621a-0fd7-4efa-a755-d5efc948f174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66347353-973e-4274-942c-f1b91fb39414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36aa0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "################## MODEL SETTING ########################\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "#########################################################\n",
    "\n",
    "class Server(object):\n",
    "    def __init__(self, model, dataset_info, seed = 123, num_workers = 4, ret = False, \n",
    "                train_prn = False, metric = \"Risk Difference\", select_round = False,\n",
    "                batch_size = 128, print_every = 1, fraction_clients = 1, Z = 2, prn = True, trial = False):\n",
    "        \"\"\"\n",
    "        Server execution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: torch.nn.Module object.\n",
    "\n",
    "        dataset_info: a list of three objects.\n",
    "            - train_dataset: Dataset object.\n",
    "            - test_dataset: Dataset object.\n",
    "            - clients_idx: a list of lists, with each sublist contains the indexs of the training samples in one client.\n",
    "                    the length of the list is the number of clients.\n",
    "\n",
    "        seed: random seed.\n",
    "\n",
    "        num_workers: number of workers.\n",
    "\n",
    "        ret: boolean value. If true, return the accuracy and fairness measure and print nothing; else print the log and return None.\n",
    "\n",
    "        train_prn: boolean value. If true, print the batch loss in local epochs.\n",
    "\n",
    "        metric: three options, \"Risk Difference\", \"pRule\", \"Demographic disparity\".\n",
    "\n",
    "        batch_size: a positive integer.\n",
    "\n",
    "        print_every: a positive integer. eg. print_every = 1 -> print the information of that global round every 1 round.\n",
    "\n",
    "        fraction_clients: float from 0 to 1. The fraction of clients chose to update the weights in each round.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "        if torch.cuda.device_count()>1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.ret = ret\n",
    "        self.prn = prn\n",
    "        self.train_prn = False if ret else train_prn\n",
    "\n",
    "        self.metric = metric\n",
    "        if metric == \"Risk Difference\":\n",
    "            self.disparity = riskDifference\n",
    "        elif metric == \"pRule\":\n",
    "            self.disparity = pRule\n",
    "        elif metric == \"Demographic disparity\":\n",
    "            self.disparity = DPDisparity\n",
    "        else:\n",
    "            warnings.warn(\"Warning message: metric \" + metric + \" is not supported! Use the default metric Demographic disparity. \")\n",
    "            self.disparity = DPDisparity\n",
    "            self.metric = \"Demographic disparity\"\n",
    "\n",
    "        self.batch_size = batch_size        \n",
    "        self.print_every = print_every\n",
    "        self.fraction_clients = fraction_clients\n",
    "\n",
    "        self.train_dataset, self.test_dataset, self.clients_idx = dataset_info        \n",
    "        self.num_clients = len(self.clients_idx)\n",
    "        self.Z = Z\n",
    "\n",
    "        self.trial = trial\n",
    "        self.select_round = select_round\n",
    "\n",
    "        self.trainloader, self.validloader = self.train_val(self.train_dataset, batch_size)\n",
    "    \n",
    "    def train_val(self, dataset, batch_size, idxs_train_full = None, split = False):\n",
    "        \"\"\"\n",
    "        Returns train, validation for a given local training dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.seed)\n",
    "        \n",
    "        # split indexes for train, validation (90, 10)\n",
    "        if idxs_train_full == None: idxs_train_full = np.arange(len(dataset))\n",
    "        idxs_train = idxs_train_full[:int(0.9*len(idxs_train_full))]\n",
    "        idxs_val = idxs_train_full[int(0.9*len(idxs_train_full)):]\n",
    "    \n",
    "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
    "                                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        if split:\n",
    "            validloader = {}\n",
    "            for sen in range(self.Z):\n",
    "                sen_idx = np.where(dataset.sen[idxs_val] == sen)[0]\n",
    "                validloader[sen] = DataLoader(DatasetSplit(dataset, idxs_val[sen_idx]),\n",
    "                                        batch_size=max(int(len(idxs_val)/10),10), shuffle=False)\n",
    "        else:\n",
    "            validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
    "                                     batch_size=max(int(len(idxs_val)/10),10), shuffle=False)\n",
    "        return trainloader, validloader\n",
    "\n",
    "\n",
    "    # only support binary sensitive attribute\n",
    "    # assign a higher weight to clients that have similar local fairness to the global fairness metric\n",
    "    def FairFed(self, num_rounds = 2, local_epochs = 2, learning_rate = 0.005, beta = 0.3, alpha = 0.1, optimizer = 'adam'):\n",
    "        # set seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        # Training\n",
    "        train_loss, train_accuracy = [], []\n",
    "        start_time = time.time()\n",
    "        weights = self.model.state_dict()\n",
    "\n",
    "        print(\"self.disparity---\",self.disparity)\n",
    "        \n",
    "        self.prn=True\n",
    "        self.trial=False\n",
    "        \n",
    "        lbd, m_yz = [None for _ in range(self.num_clients)], [None for _ in range(self.num_clients)]\n",
    "        \n",
    "        num_rounds=1 \n",
    "        print(num_rounds)\n",
    "            \n",
    "        for round_ in tqdm(range(num_rounds)):\n",
    "            \n",
    "            print(\"----in TQDM----\")\n",
    "            \n",
    "            local_weights, local_losses, nw = [], [], []\n",
    "            if self.prn: \n",
    "                print(f'\\n | Global Training Round : {round_+1} |\\n')\n",
    "\n",
    "            # get local fairness metric\n",
    "            list_acc = []\n",
    "            n_yz = {}\n",
    "            for y in [0,1]:\n",
    "                for z in range(self.Z):\n",
    "                    n_yz[(y,z)] = 0\n",
    "                    \n",
    "            self.model.eval()\n",
    "\n",
    "            for c in range(self.num_clients):\n",
    "                local_model = Client(dataset=self.train_dataset, idxs=self.clients_idx[c], \n",
    "                            batch_size = self.batch_size, option = \"unconstrained\", seed = self.seed, prn = self.train_prn, Z = self.Z)\n",
    "                acc, loss, n_yz_c, acc_loss, fair_loss, _ = local_model.inference(model = self.model, train = True) \n",
    "                list_acc.append(acc)\n",
    "\n",
    "                for yz in n_yz:\n",
    "                    n_yz[yz] += n_yz_c[yz]\n",
    "\n",
    "                nw.append(self.disparity(n_yz_c))\n",
    "                    \n",
    "                if self.prn: \n",
    "                    print(\"Client %d: accuracy loss: %.2f | fairness loss %.2f | %s = %.2f\" % (\n",
    "                            c+1, acc_loss, fair_loss, self.metric, self.disparity(n_yz_c)))\n",
    "\n",
    "            train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "            print(n_yz)\n",
    "\n",
    "            #update the client weights-Not sure. it used beta\n",
    "            for c in range(self.num_clients):\n",
    "                nw[c] = np.exp(-beta * abs(nw[c] - self.disparity(n_yz))) * len(self.clients_idx[c]) / len(self.train_dataset)\n",
    "\n",
    "            \n",
    "            # print global training loss after every 'i' rounds\n",
    "            if self.prn:\n",
    "                if (round_+1) % self.print_every == 0:\n",
    "                    print(f' \\nAvg Training Stats after {round_+1} global rounds:')\n",
    "                    print(\"Training loss: %.2f | Validation accuracy: %.2f%% | Validation %s: %.4f\" % (\n",
    "                        np.mean(np.array(train_loss)), \n",
    "                        100*train_accuracy[-1], self.metric, self.disparity(n_yz)))\n",
    "            \n",
    "            if self.trial:\n",
    "                with tune.checkpoint_dir(round_) as checkpoint_dir:\n",
    "                    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                    torch.save(self.model.state_dict(), path)\n",
    "                    \n",
    "                tune.report(loss = loss, accuracy = train_accuracy[-1], disp = self.disparity(n_yz), iteration = round_+1)\n",
    "                \n",
    "            torch.save(self.model.state_dict(), '/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/model.pth')\n",
    "            self.model.train()\n",
    "            \n",
    "            \n",
    "            for idx in range(self.num_clients):\n",
    "                local_model = Client(dataset=self.train_dataset, idxs=self.clients_idx[idx], \n",
    "                            batch_size = self.batch_size, option = \"FB-Variant1\", seed = self.seed, prn = self.train_prn, Z = self.Z)\n",
    "\n",
    "                w, loss = local_model.standard_update(\n",
    "                                model=copy.deepcopy(self.model), global_round=round_, \n",
    "                                    learning_rate = learning_rate, local_epochs = local_epochs, \n",
    "                                    optimizer = optimizer)\n",
    "                local_weights.append(copy.deepcopy(w))\n",
    "                local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "            # update global weights\n",
    "            weights = weighted_average_weights(local_weights, nw, sum(nw))\n",
    "            self.model.load_state_dict(weights)\n",
    "\n",
    "            loss_avg = sum(local_losses) / len(local_losses)\n",
    "            train_loss.append(loss_avg)\n",
    "\n",
    "        # Test inference after completion of training\n",
    "        test_acc, n_yz= self.test_inference()\n",
    "        rd = self.disparity(n_yz)\n",
    "\n",
    "        if self.prn:\n",
    "            print(f' \\n Results after {num_rounds} global rounds of training:')\n",
    "            print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "            print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "            # Compute fairness metric\n",
    "            print(\"|---- Test \"+ self.metric+\": {:.4f}\".format(rd))\n",
    "\n",
    "            print('\\n Total Run Time: {0:0.4f} sec'.format(time.time()-start_time))\n",
    "\n",
    "        if self.ret: return test_acc, rd, self.model\n",
    "\n",
    "\n",
    "\n",
    "    def inference(self, option = 'unconstrained', penalty = 100, model = None, validloader = None):\n",
    "        \"\"\" \n",
    "        Returns the inference accuracy, \n",
    "                                loss, \n",
    "                                N(sensitive group, pos), \n",
    "                                N(non-sensitive group, pos), \n",
    "                                N(sensitive group),\n",
    "                                N(non-sensitive group),\n",
    "                                acc_loss,\n",
    "                                fair_loss\n",
    "        \"\"\"\n",
    "\n",
    "        if model == None: model = self.model\n",
    "        if validloader == None: \n",
    "            validloader = self.validloader\n",
    "        model.eval()\n",
    "        loss, total, correct, fair_loss, acc_loss, num_batch = 0.0, 0.0, 0.0, 0.0, 0.0, 0\n",
    "        n_yz, loss_yz = {}, {}\n",
    "        for y in [0,1]:\n",
    "            for z in range(self.Z):\n",
    "                loss_yz[(y,z)] = 0\n",
    "                n_yz[(y,z)] = 0\n",
    "        \n",
    "        for _, (features, labels, sensitive) in enumerate(validloader):\n",
    "            features, labels = features.to(DEVICE), labels.type(torch.LongTensor).to(DEVICE)\n",
    "            sensitive = sensitive.type(torch.LongTensor).to(DEVICE)\n",
    "            \n",
    "            # Inference\n",
    "            outputs, logits = model(features)\n",
    "            outputs, logits = outputs.to(DEVICE), logits.to(DEVICE)\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1).to(DEVICE)\n",
    "            bool_correct = torch.eq(pred_labels, labels)\n",
    "            correct += torch.sum(bool_correct).item()\n",
    "            total += len(labels)\n",
    "            num_batch += 1\n",
    "\n",
    "            group_boolean_idx = {}\n",
    "            \n",
    "            for yz in n_yz:\n",
    "                group_boolean_idx[yz] = (labels == yz[0]) & (sensitive == yz[1])\n",
    "                n_yz[yz] += torch.sum((pred_labels == yz[0]) & (sensitive == yz[1])).item()     \n",
    "                \n",
    "                if option == \"FairBatch\":\n",
    "                # the objective function have no lagrangian term\n",
    "\n",
    "                    loss_yz_,_,_ = loss_func(\"FB_inference\", logits[group_boolean_idx[yz]].to(DEVICE), \n",
    "                                                    labels[group_boolean_idx[yz]].to(DEVICE), \n",
    "                                         outputs[group_boolean_idx[yz]].to(DEVICE), sensitive[group_boolean_idx[yz]].to(DEVICE), \n",
    "                                         penalty)\n",
    "                    loss_yz[yz] += loss_yz_\n",
    "            \n",
    "            batch_loss, batch_acc_loss, batch_fair_loss = loss_func(option, logits, \n",
    "                                                        labels, outputs, sensitive, penalty)\n",
    "            loss, acc_loss, fair_loss = (loss + batch_loss.item(), \n",
    "                                         acc_loss + batch_acc_loss.item(), \n",
    "                                         fair_loss + batch_fair_loss.item())\n",
    "        accuracy = correct/total\n",
    "        if option in [\"FairBatch\", \"FB-Variant1\"]:\n",
    "            return accuracy, loss, n_yz, acc_loss / num_batch, fair_loss / num_batch, loss_yz\n",
    "        else:\n",
    "            return accuracy, loss, n_yz, acc_loss / num_batch, fair_loss / num_batch, None\n",
    "\n",
    "    def test_inference(self, model = None, test_dataset = None):\n",
    "\n",
    "        \"\"\" \n",
    "        Returns the test accuracy and fairness level.\n",
    "        \"\"\"\n",
    "        # set seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        if model == None: model = self.model\n",
    "        if test_dataset == None: test_dataset = self.test_dataset\n",
    "\n",
    "        model.eval()\n",
    "        total, correct = 0.0, 0.0\n",
    "        n_yz = {}\n",
    "        for y in [0,1]:\n",
    "            for z in range(self.Z):\n",
    "                n_yz[(y,z)] = 0\n",
    "        \n",
    "        testloader = DataLoader(test_dataset, batch_size=self.batch_size,\n",
    "                                shuffle=False)\n",
    "\n",
    "        for _, (features, labels, sensitive) in enumerate(testloader):\n",
    "            features = features.to(DEVICE)\n",
    "            labels =  labels.to(DEVICE).type(torch.LongTensor)\n",
    "            # Inference\n",
    "            outputs, _ = model(features)\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            bool_correct = torch.eq(pred_labels, labels)\n",
    "            correct += torch.sum(bool_correct).item()\n",
    "            total += len(labels)\n",
    "            \n",
    "            for y,z in n_yz:\n",
    "                n_yz[(y,z)] += torch.sum((sensitive == z) & (pred_labels == y)).item()  \n",
    "\n",
    "        accuracy = correct/total\n",
    "\n",
    "        return accuracy, n_yz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1f1ce-58da-4e7a-80c2-5829408ceb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0333e-6ec2-468f-b584-dfe2b47b6947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b87bc75-3680-44b1-afc5-0f2c1db8d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Client(object):\n",
    "    def __init__(self, dataset, idxs, batch_size, option, seed = 0, prn = True, penalty = 500, Z = 2):\n",
    "        self.seed = seed \n",
    "        self.dataset = dataset\n",
    "        self.idxs = idxs\n",
    "        self.option = option\n",
    "        self.prn = prn\n",
    "        self.Z = Z\n",
    "        self.trainloader, self.validloader = self.train_val(dataset, list(idxs), batch_size)\n",
    "        self.penalty = penalty\n",
    "        self.disparity = riskDifference\n",
    "\n",
    "    def train_val(self, dataset, idxs, batch_size):\n",
    "        \"\"\"\n",
    "        Returns train, validation for a given local training dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.seed)\n",
    "        \n",
    "        # split indexes for train, validation (90, 10)\n",
    "        idxs_train = idxs[:int(0.9*len(idxs))]\n",
    "        idxs_val = idxs[int(0.9*len(idxs)):len(idxs)]\n",
    "\n",
    "        self.train_dataset = DatasetSplit(dataset, idxs_train)\n",
    "        self.test_dataset = DatasetSplit(dataset, idxs_val)\n",
    "\n",
    "        trainloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        validloader = DataLoader(self.test_dataset,\n",
    "                                     batch_size=max(int(len(idxs_val)/10),10), shuffle=False)\n",
    "        return trainloader, validloader\n",
    "\n",
    "    def standard_update(self, model, global_round, learning_rate, local_epochs, optimizer): \n",
    "        # Set mode to train model\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "\n",
    "        # set seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        # Set optimizer for the local updates\n",
    "        if optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                                        ) # momentum=0.5\n",
    "        elif optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                        weight_decay=1e-4)\n",
    "        for i in range(local_epochs):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (features, labels, sensitive) in enumerate(self.trainloader):\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE).type(torch.LongTensor)\n",
    "                sensitive = sensitive.to(DEVICE)\n",
    "                # we need to set the gradients to zero before starting to do backpropragation \n",
    "                # because PyTorch accumulates the gradients on subsequent backward passes. \n",
    "                # This is convenient while training RNNs\n",
    "                \n",
    "                probas, logits = model(features)\n",
    "                loss, _, _ = loss_func(self.option, logits.to(DEVICE), labels.to(DEVICE), probas, sensitive, self.penalty)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if self.prn and (100. * batch_idx / len(self.trainloader)) % 50 == 0:\n",
    "                    print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tBatch Loss: {:.6f}'.format(\n",
    "                        global_round + 1, i, batch_idx * len(features),\n",
    "                        len(self.trainloader.dataset),\n",
    "                        100. * batch_idx / len(self.trainloader), loss.item()))\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "        # weight, loss\n",
    "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "\n",
    "    def inference(self, model, train = False, bits = False, truem_yz = None):\n",
    "        \"\"\" \n",
    "        Returns the inference accuracy, \n",
    "                                loss, \n",
    "                                N(sensitive group, pos), \n",
    "                                N(non-sensitive group, pos), \n",
    "                                N(sensitive group),\n",
    "                                N(non-sensitive group),\n",
    "                                acc_loss,\n",
    "                                fair_loss\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()\n",
    "        loss, total, correct, fair_loss, acc_loss, num_batch = 0.0, 0.0, 0.0, 0.0, 0.0, 0\n",
    "        n_yz, loss_yz, m_yz, f_z = {}, {}, {}, {}\n",
    "\n",
    "        for y in [0,1]:\n",
    "            for z in range(self.Z):\n",
    "                loss_yz[(y,z)] = 0\n",
    "                n_yz[(y,z)] = 0\n",
    "                m_yz[(y,z)] = 0\n",
    "\n",
    "        dataset = self.validloader if not train else self.trainloader        \n",
    "        for _, (features, labels, sensitive) in enumerate(dataset):\n",
    "            features, labels = features.to(DEVICE), labels.type(torch.LongTensor).to(DEVICE)\n",
    "            sensitive = sensitive.type(torch.LongTensor).to(DEVICE)\n",
    "            \n",
    "            # Inference\n",
    "            outputs, logits = model(features)\n",
    "            outputs, logits = outputs.to(DEVICE), logits.to(DEVICE)\n",
    "\n",
    "            # Prediction\n",
    "            \n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1).to(DEVICE)\n",
    "            bool_correct = torch.eq(pred_labels, labels)\n",
    "            correct += torch.sum(bool_correct).item()\n",
    "            total += len(labels)\n",
    "            num_batch += 1\n",
    "\n",
    "            group_boolean_idx = {}\n",
    "            \n",
    "            for yz in n_yz:\n",
    "                group_boolean_idx[yz] = (labels == yz[0]) & (sensitive == yz[1])\n",
    "                n_yz[yz] += torch.sum((pred_labels == yz[0]) & (sensitive == yz[1])).item()     \n",
    "                m_yz[yz] += torch.sum((labels == yz[0]) & (sensitive == yz[1])).item()    \n",
    "                \n",
    "                if self.option in[\"FairBatch\", \"FB-Variant1\"]:\n",
    "                # the objective function have no lagrangian term\n",
    "\n",
    "                    loss_yz_,_,_ = loss_func(\"standard\", logits[group_boolean_idx[yz]].to(DEVICE), \n",
    "                                                    labels[group_boolean_idx[yz]].to(DEVICE), \n",
    "                                         outputs[group_boolean_idx[yz]].to(DEVICE), sensitive[group_boolean_idx[yz]].to(DEVICE), \n",
    "                                         self.penalty)\n",
    "                    loss_yz[yz] += loss_yz_\n",
    "            \n",
    "            batch_loss, batch_acc_loss, batch_fair_loss = loss_func(self.option, logits, \n",
    "                                                        labels, outputs, sensitive, self.penalty)\n",
    "            loss, acc_loss, fair_loss = (loss + batch_loss.item(), \n",
    "                                         acc_loss + batch_acc_loss.item(), \n",
    "                                         fair_loss + batch_fair_loss.item())\n",
    "        accuracy = correct/total\n",
    "        if self.option in [\"FairBatch\", \"FB-Variant1\"]:\n",
    "            for z in range(1, self.Z):\n",
    "                f_z[z] = - loss_yz[(0,0)]/(truem_yz[(0,0)] + truem_yz[(1,0)]) + loss_yz[(1,0)]/(truem_yz[(0,0)] + truem_yz[(1,0)]) + loss_yz[(0,z)]/(truem_yz[(0,z)] + truem_yz[(1,z)]) - loss_yz[(1,z)]/(truem_yz[(0,z)] + truem_yz[(1,z)]) \n",
    "            if bits: \n",
    "                bins = np.linspace(-2, 2, 2**bits // (self.Z - 1))\n",
    "                for z in range(1, self.Z):\n",
    "                    f_z[z] = bins[np.digitize(f_z[z].item(), bins)-1]\n",
    "            return accuracy, loss, n_yz, acc_loss / num_batch, fair_loss / num_batch, f_z\n",
    "        else:\n",
    "            return accuracy, loss, n_yz, acc_loss / num_batch, fair_loss / num_batch, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be1740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb5ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------path----------- /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/adult/adult.data\n",
      "-------------path----------- /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/adult/adult.data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Adult\n",
    "sensitive_attributes = ['sex']\n",
    "categorical_attributes = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country']\n",
    "continuous_attributes = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "features_to_keep = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "            'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss','hours-per-week', \n",
    "            'native-country', 'salary']\n",
    "label_name = 'salary'\n",
    "\n",
    "adult = process_csv('adult', 'adult.data', label_name, ' >50K', sensitive_attributes, [' Female'], categorical_attributes, continuous_attributes, features_to_keep, na_values = [], header = None, columns = features_to_keep)\n",
    "test = process_csv('adult', 'adult.test', label_name, ' >50K.', sensitive_attributes, [' Female'], categorical_attributes, continuous_attributes, features_to_keep, na_values = [], header = None, columns = features_to_keep) # the distribution is very different from training distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bffb45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x [[0.30136988 0.0443019  0.8        ... 0.         0.         1.        ]\n",
      " [0.4520548  0.0482376  0.8        ... 0.         0.         1.        ]\n",
      " [0.28767124 0.13811344 0.53333336 ... 0.         0.         1.        ]\n",
      " ...\n",
      " [0.56164384 0.09482688 0.53333336 ... 0.         0.         0.        ]\n",
      " [0.06849315 0.12849934 0.53333336 ... 0.         0.         1.        ]\n",
      " [0.47945204 0.18720338 0.53333336 ... 0.         0.         0.        ]]\n",
      "self.x [[0.30136988 0.0443019  0.8        ... 0.         0.         1.        ]\n",
      " [0.4520548  0.0482376  0.8        ... 0.         0.         1.        ]\n",
      " [0.28767124 0.13811344 0.53333336 ... 0.         0.         1.        ]\n",
      " ...\n",
      " [0.56164384 0.09482688 0.53333336 ... 0.         0.         0.        ]\n",
      " [0.06849315 0.12849934 0.53333336 ... 0.         0.         1.        ]\n",
      " [0.47945204 0.18720338 0.53333336 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "test['native-country_ Holand-Netherlands'] = 0\n",
    "test = test[adult.columns]\n",
    "\n",
    "np.random.seed(1)\n",
    "adult_private_idx = adult[adult['workclass_ Private'] == 1].index\n",
    "adult_others_idx = adult[adult['workclass_ Private'] == 0].index\n",
    "adult_mean_sensitive = adult['z'].mean()\n",
    "\n",
    "client1_idx = np.concatenate((np.random.choice(adult_private_idx, int(.8*len(adult_private_idx)), replace = False),\n",
    "                                np.random.choice(adult_others_idx, int(.2*len(adult_others_idx)), replace = False)))\n",
    "client2_idx = np.array(list(set(adult.index) - set(client1_idx)))\n",
    "adult_clients_idx = [client1_idx, client2_idx]\n",
    "\n",
    "adult_num_features = len(adult.columns)-1\n",
    "adult_test = LoadData(test, 'salary', 'z')\n",
    "adult_train = LoadData(adult, 'salary', 'z')\n",
    "torch.manual_seed(0)\n",
    "adult_info = [adult_train, adult_test, adult_clients_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b6a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "add874c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_dp(method, model, dataset, prn = True, seed = 123, trial = False, select_round = False, **kwargs):\n",
    "    # choose the model\n",
    "    if model == 'logistic regression':\n",
    "        arc = logReg\n",
    "    elif model == 'multilayer perceptron':\n",
    "        arc = mlp\n",
    "    else:\n",
    "        Warning('Does not support this model!')\n",
    "        exit(1)\n",
    "\n",
    "    # set up the dataset\n",
    "    \n",
    "    if dataset == 'synthetic':\n",
    "        Z, num_features, info = 2, 3, synthetic_info\n",
    "    elif dataset == 'adult':\n",
    "        Z, num_features, info = 2, adult_num_features, adult_info\n",
    "        print(Z,num_features, info)\n",
    "\n",
    "    else:\n",
    "        Warning('Does not support this dataset!')\n",
    "        exit(1)\n",
    "\n",
    "    # set up the server\n",
    "    server = Server(arc(num_features=num_features, num_classes=2, seed = seed), info, train_prn = False, seed = seed, Z = Z, ret = True, prn = prn, trial = trial, select_round = select_round)\n",
    "\n",
    "    # execute\n",
    "    if method == 'fedavg':\n",
    "        acc, dpdisp, classifier = server.FedAvg(**kwargs)\n",
    "        \n",
    "    elif method == 'fairfed':\n",
    "        print(server.FairFed(**kwargs))\n",
    "        acc, dpdisp, classifier = server.FairFed(**kwargs)\n",
    "        \n",
    "    else:\n",
    "        Warning('Does not support this method!')\n",
    "        exit(1)\n",
    "\n",
    "    if not trial: return {'accuracy': acc, 'DP Disp': dpdisp}\n",
    "\n",
    "def sim_dp(method, model, dataset, num_sim = 1, seed = 0, resources_per_trial = {'cpu':4}, **kwargs):\n",
    "    # choose the model\n",
    "    if model == 'logistic regression':\n",
    "        arc = logReg\n",
    "    elif model == 'multilayer perceptron':\n",
    "        arc = mlp\n",
    "    else:\n",
    "        Warning('Does not support this model!')\n",
    "        exit(1)\n",
    "\n",
    "    # set up the dataset\n",
    "    \n",
    "    if dataset == 'synthetic':\n",
    "        Z, num_features, info = 2, 3, synthetic_info\n",
    "    elif dataset == 'adult':\n",
    "        Z, num_features, info = 2, adult_num_features, adult_info\n",
    "\n",
    "    else:\n",
    "        Warning('Does not support this dataset!')\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "    if method == 'fairfed':\n",
    "        print('--------------------------------Hyperparameter selection--------------------------------')\n",
    "        print('--------------------------------Seed:' + str(seed) + '--------------------------------')\n",
    "        # config = {'lr': tune.grid_search([.001, .005,]),\n",
    "        #         'alpha': tune.grid_search([.0001, .001, .01]),\n",
    "        #         'beta': tune.grid_search([0.02, 1, 50])}\n",
    "\n",
    "        config = {'lr': 0.001,\n",
    "                'alpha': 0.01,\n",
    "                'beta': 0.02}\n",
    "        \n",
    "        def trainable(config): \n",
    "            print(\"--------------CHIRAG ---------------FAIRFED-----------\")\n",
    "            return run_dp(method = method, model = model, dataset = dataset, prn = True, trial = True, seed = seed, learning_rate = config['lr'], alpha = config['alpha'], beta = config['beta'], **kwargs)\n",
    "\n",
    "        a=trainable(config)\n",
    "        # best_trial = analysis.get_best_trial(\"disp\", \"min\", \"last\")\n",
    "        # params = best_trial.config\n",
    "\n",
    "        params={'lr': 0.001,\n",
    "                'alpha': 0.01,\n",
    "                'beta': 0.02}\n",
    "        \n",
    "        learning_rate, alpha, beta = params['lr'], params['alpha'], params['beta']\n",
    "\n",
    "        print('--------------------------------Start Simulations--------------------------------')\n",
    "        # # get test result of the trained model\n",
    "        server = Server(arc(num_features=num_features, num_classes=2, seed = seed), info, train_prn = False, seed = seed, Z = Z, ret = True, prn = False)\n",
    "        trained_model = copy.deepcopy(server.model)\n",
    "        \n",
    "        # best_checkpoint = analysis.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\")\n",
    "        # best_checkpoint_dir = best_checkpoint.to_directory(path=\"directory\")        \n",
    "        \n",
    "        # trained_model.load_state_dict(torch.load(os.path.join(best_checkpoint_dir, 'checkpoint')))\n",
    "        trained_model.load_state_dict(torch.load('/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/model.pth'))\n",
    "\n",
    "        \n",
    "        trained_model.to(DEVICE)\n",
    "        test_acc, n_yz = server.test_inference(trained_model)\n",
    "        df = pd.DataFrame([{'accuracy': test_acc, 'DP Disp': riskDifference(n_yz)}])\n",
    "\n",
    "        # use the same hyperparameters for other seeds\n",
    "        for seed in range(1, num_sim):\n",
    "            print('--------------------------------Seed:' + str(seed) + '--------------------------------')\n",
    "            result = run_dp(method = method, model = model, dataset = dataset, prn = False, seed = seed, learning_rate = learning_rate, alpha = alpha, beta = beta, **kwargs)\n",
    "            \n",
    "            # df = df.concat(pd.DataFrame([result]))\n",
    "            df = pd.concat([df, pd.DataFrame([result])])\n",
    "\n",
    "            \n",
    "        df = df.reset_index(drop = True)\n",
    "        acc_mean, dp_mean = df.mean()\n",
    "        acc_std, dp_std = df.std()\n",
    "        print(\"Result across %d simulations: \" % num_sim)\n",
    "        print(\"| Accuracy: %.4f(%.4f) | DP Disp: %.4f(%.4f)\" % (acc_mean, acc_std, dp_mean, dp_std))\n",
    "        # acc_mean, acc_std, dp_mean, dp_std=1,1,1,1\n",
    "        return acc_mean, acc_std, dp_mean, dp_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b52b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d2fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Hyperparameter selection--------------------------------\n",
      "--------------------------------Seed:0--------------------------------\n",
      "--------------CHIRAG ---------------FAIRFED-----------\n",
      "2 107 [<__main__.LoadData object at 0x7f40b1c473d0>, <__main__.LoadData object at 0x7f41c80cdcf0>, [array([ 6312,  3946, 12933, ...,  6277, 22680, 30766]), array([    0,     1,     7, ..., 32550, 32558, 32560])]]\n",
      "self.disparity--- <function riskDifference at 0x7f40b1c00b80>\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----in TQDM----\n",
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "Client 1: accuracy loss: 86.28 | fairness loss 0.32 | Risk Difference = 0.00\n",
      "Client 2: accuracy loss: 86.05 | fairness loss 0.50 | Risk Difference = 0.00\n",
      "{(0, 0): 9752, (0, 1): 19531, (1, 0): 6, (1, 1): 15}\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training loss: nan | Validation accuracy: 75.70% | Validation Risk Difference: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiragapandav/Downloads/Hiwi/gpu_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/chiragapandav/Downloads/Hiwi/gpu_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 1 global rounds of training:\n",
      "|---- Avg Train Accuracy: 75.70%\n",
      "|---- Test Accuracy: 100.00%\n",
      "|---- Test Risk Difference: 0.0000\n",
      "\n",
      " Total Run Time: 3.7271 sec\n",
      "(1.0, 0.0, mlp(\n",
      "  (linear1): Linear(in_features=107, out_features=4, bias=True)\n",
      "  (linear2): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "))\n",
      "self.disparity--- <function riskDifference at 0x7f40b1c00b80>\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----in TQDM----\n",
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "Client 1: accuracy loss: 51.28 | fairness loss 174.68 | Risk Difference = 0.00\n",
      "Client 2: accuracy loss: 58.91 | fairness loss 181.95 | Risk Difference = 0.00\n",
      "{(0, 0): 9758, (0, 1): 19546, (1, 0): 0, (1, 1): 0}\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training loss: nan | Validation accuracy: 75.78% | Validation Risk Difference: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 1 global rounds of training:\n",
      "|---- Avg Train Accuracy: 75.78%\n",
      "|---- Test Accuracy: 81.98%\n",
      "|---- Test Risk Difference: 0.1940\n",
      "\n",
      " Total Run Time: 3.5521 sec\n",
      "--------------------------------Start Simulations--------------------------------\n",
      "Result across 1 simulations: \n",
      "| Accuracy: 1.0000(nan) | DP Disp: 0.0000(nan)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, nan, 0.0, nan)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = '.'\n",
    "sys.path.insert(1, os.path.join(working_dir, 'FedFB'))\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(working_dir, 'FedFB')\n",
    "# from DP_run import *\n",
    "\n",
    "# RUN BASELINE FEDAVG on ADULT DATASET\n",
    "sim_dp('fairfed', 'multilayer perceptron', 'adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d971bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c4c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee5857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7e16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b61f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a42d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05a780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aeb53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc3c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
